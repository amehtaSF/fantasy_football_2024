{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, TypeVar\n",
    "from rl_env_marl import MARLDraftEnv, NUM_DRAFT_ROUNDS, NUM_MGRS, ACTION_SPACE_DIM\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from pprint import pprint\n",
    "import os\n",
    "from stable_baselines3.common.logger import configure\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def learning_rate_schedule(initial_lr=1e-4, final_lr=5e-5):\n",
    "    return lambda progress_remaining: progress_remaining * (initial_lr - final_lr) + final_lr\n",
    "\n",
    "learning_rate_schedule_fn = learning_rate_schedule(initial_lr=1e-4, final_lr=5e-5)\n",
    "\n",
    "\n",
    "ppo_params = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"learning_rate\": learning_rate_schedule_fn,  # Adaptive learning rate\n",
    "    \"n_steps\": 750,\n",
    "    \"batch_size\": 750,\n",
    "    \"n_epochs\": 30,\n",
    "    \"gamma\": 0.99,\n",
    "    \"gae_lambda\": 0.95,\n",
    "    \"clip_range\": 0.2,\n",
    "    \"clip_range_vf\": None,\n",
    "    \"normalize_advantage\": True,\n",
    "    \"ent_coef\": .03, # Entropy coefficient for the loss calculation\n",
    "    \"vf_coef\": 0.5,\n",
    "    \"max_grad_norm\": 0.7,\n",
    "    \"use_sde\": False,\n",
    "    \"sde_sample_freq\": -1,\n",
    "    \"rollout_buffer_class\": None,\n",
    "    \"rollout_buffer_kwargs\": None,\n",
    "    # \"target_kl\": 0.01,\n",
    "    # \"target_kl\": None, \n",
    "    \"stats_window_size\": 100,\n",
    "    \"policy_kwargs\": dict(net_arch=[dict(pi=[256, 256, 128], vf=[256, 256, 128])]),\n",
    "    \"verbose\": 1,\n",
    "    \"seed\": 69,\n",
    "    \"device\": \"auto\",\n",
    "    \"_init_setup_model\": True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env(MARLDraftEnv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "env = MARLDraftEnv()\n",
    "\n",
    "n_agents = NUM_MGRS\n",
    "model_path = \"logs/PPO_20240831-014945/mgr_{mgr}/best_model_mgr_{mgr}.zip\"\n",
    "models = [PPO.load(model_path.format(mgr=mgr), env=env) for mgr in range(n_agents)]\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    # Update the optimizer with the new learning rate\n",
    "    if model.policy.optimizer is not None:\n",
    "        for param_group in model.policy.optimizer.param_groups:\n",
    "            param_group['lr'] = 5e-5\n",
    "        \n",
    "\n",
    "# total_episodes = int(2e6)\n",
    "total_timesteps = int(NUM_MGRS*2e6)\n",
    "# n_episodes = 50  # Number of episodes per update\n",
    "\n",
    "# saving frequency\n",
    "n_episodes_info = 5e4\n",
    "n_episodes_model = 5e4 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_num: 0\n",
      "Saved best model for manager 0 with draft reward 0.08137824386358261\n",
      "Saved best model for manager 1 with draft reward 0.04086847975850105\n",
      "Saved best model for manager 2 with draft reward 0.03172146528959274\n",
      "Saved best model for manager 3 with draft reward 0.04558950662612915\n",
      "Saved best model for manager 4 with draft reward 0.04069160670042038\n",
      "Saved best model for manager 5 with draft reward 0.03393426910042763\n",
      "Saved best model for manager 6 with draft reward 0.05292202904820442\n",
      "Saved best model for manager 7 with draft reward 0.033870384097099304\n",
      "Saved best model for manager 8 with draft reward 0.03409319743514061\n",
      "Saved best model for manager 9 with draft reward 0.037043940275907516\n",
      "Saved best model for manager 10 with draft reward 0.03409679979085922\n",
      "Saved best model for manager 11 with draft reward 0.03785931318998337\n",
      "step_num: 9000\n",
      "Saved best model for manager 0 with draft reward 0.0883965790271759\n",
      "Saved best model for manager 1 with draft reward 0.04220890626311302\n",
      "Saved best model for manager 2 with draft reward 0.03601158782839775\n",
      "Saved best model for manager 5 with draft reward 0.039526622742414474\n",
      "Saved best model for manager 8 with draft reward 0.034312158823013306\n",
      "Saved best model for manager 10 with draft reward 0.034176621586084366\n",
      "step_num: 18000\n",
      "Saved best model for manager 6 with draft reward 0.054043013602495193\n",
      "Saved best model for manager 7 with draft reward 0.038520533591508865\n",
      "Saved best model for manager 8 with draft reward 0.035604868084192276\n",
      "step_num: 27000\n",
      "Saved best model for manager 2 with draft reward 0.03893246129155159\n",
      "Saved best model for manager 6 with draft reward 0.05422207713127136\n",
      "Saved best model for manager 8 with draft reward 0.037293221801519394\n",
      "Saved best model for manager 9 with draft reward 0.03868871182203293\n",
      "step_num: 36000\n",
      "Saved best model for manager 6 with draft reward 0.055460814386606216\n",
      "Saved best model for manager 10 with draft reward 0.03533627465367317\n",
      "Saved best model for manager 11 with draft reward 0.03941931575536728\n",
      "step_num: 45000\n",
      "Saved best model for manager 2 with draft reward 0.03925836831331253\n",
      "Saved best model for manager 6 with draft reward 0.05650865286588669\n",
      "Saved best model for manager 9 with draft reward 0.04022187367081642\n",
      "step_num: 54000\n",
      "Saved best model for manager 1 with draft reward 0.04226183518767357\n",
      "Saved best model for manager 10 with draft reward 0.03539178892970085\n",
      "step_num: 63000\n",
      "Saved best model for manager 6 with draft reward 0.056838419288396835\n",
      "step_num: 72000\n",
      "Saved best model for manager 10 with draft reward 0.03650026395916939\n",
      "step_num: 81000\n",
      "step_num: 90000\n",
      "Saved best model for manager 0 with draft reward 0.09510036557912827\n",
      "Saved best model for manager 8 with draft reward 0.038671720772981644\n",
      "step_num: 99000\n",
      "Saved best model for manager 8 with draft reward 0.03880619257688522\n",
      "step_num: 108000\n",
      "Saved best model for manager 2 with draft reward 0.04052656143903732\n",
      "step_num: 117000\n",
      "Saved best model for manager 8 with draft reward 0.03944629430770874\n",
      "step_num: 126000\n",
      "step_num: 135000\n",
      "Saved best model for manager 1 with draft reward 0.04229120537638664\n",
      "Saved best model for manager 3 with draft reward 0.04588470607995987\n",
      "step_num: 144000\n",
      "step_num: 153000\n",
      "step_num: 162000\n",
      "Saved best model for manager 9 with draft reward 0.04029747098684311\n",
      "step_num: 171000\n",
      "Saved best model for manager 0 with draft reward 0.09645461291074753\n",
      "step_num: 180000\n",
      "Saved best model for manager 1 with draft reward 0.0423140712082386\n",
      "step_num: 189000\n",
      "step_num: 198000\n",
      "Saved best model for manager 11 with draft reward 0.039676859974861145\n",
      "step_num: 207000\n",
      "Saved best model for manager 4 with draft reward 0.04151342436671257\n",
      "step_num: 216000\n",
      "step_num: 225000\n",
      "Saved best model for manager 8 with draft reward 0.04021710529923439\n",
      "step_num: 234000\n",
      "step_num: 243000\n",
      "Saved best model for manager 1 with draft reward 0.042443934828042984\n",
      "step_num: 252000\n",
      "step_num: 261000\n",
      "step_num: 270000\n",
      "step_num: 279000\n",
      "step_num: 288000\n",
      "step_num: 297000\n",
      "Saved best model for manager 3 with draft reward 0.04837651178240776\n",
      "step_num: 306000\n",
      "Saved best model for manager 4 with draft reward 0.04350666701793671\n",
      "step_num: 315000\n",
      "step_num: 324000\n",
      "step_num: 333000\n",
      "step_num: 342000\n",
      "Saved best model for manager 6 with draft reward 0.059536468237638474\n",
      "step_num: 351000\n",
      "Saved best model for manager 3 with draft reward 0.04871191456913948\n",
      "step_num: 360000\n",
      "step_num: 369000\n",
      "Saved best model for manager 2 with draft reward 0.04172719269990921\n",
      "step_num: 378000\n",
      "Saved best model for manager 1 with draft reward 0.0437646247446537\n",
      "Saved best model for manager 2 with draft reward 0.04189291223883629\n",
      "step_num: 387000\n",
      "step_num: 396000\n",
      "step_num: 405000\n",
      "step_num: 414000\n",
      "step_num: 423000\n",
      "step_num: 432000\n",
      "Saved best model for manager 11 with draft reward 0.04043811932206154\n",
      "step_num: 441000\n",
      "Saved best model for manager 11 with draft reward 0.04231433942914009\n",
      "step_num: 450000\n",
      "step_num: 459000\n",
      "step_num: 468000\n",
      "Saved best model for manager 8 with draft reward 0.04023834690451622\n",
      "step_num: 477000\n",
      "step_num: 486000\n",
      "Saved best model for manager 10 with draft reward 0.04038689658045769\n",
      "step_num: 495000\n",
      "step_num: 504000\n",
      "step_num: 513000\n",
      "step_num: 522000\n",
      "step_num: 531000\n",
      "step_num: 540000\n",
      "step_num: 549000\n",
      "step_num: 558000\n",
      "step_num: 567000\n",
      "step_num: 576000\n",
      "step_num: 585000\n",
      "step_num: 594000\n",
      "step_num: 603000\n",
      "Saved best model for manager 3 with draft reward 0.04919370636343956\n",
      "step_num: 612000\n",
      "step_num: 621000\n",
      "step_num: 630000\n",
      "step_num: 639000\n",
      "step_num: 648000\n",
      "step_num: 657000\n",
      "step_num: 666000\n",
      "step_num: 675000\n",
      "Saved best model for manager 4 with draft reward 0.04386879876255989\n",
      "Saved best model for manager 5 with draft reward 0.0401192344725132\n",
      "step_num: 684000\n",
      "step_num: 693000\n",
      "step_num: 702000\n",
      "step_num: 711000\n",
      "step_num: 720000\n",
      "step_num: 729000\n",
      "step_num: 738000\n",
      "step_num: 747000\n",
      "step_num: 756000\n",
      "step_num: 765000\n",
      "step_num: 774000\n",
      "step_num: 783000\n",
      "step_num: 792000\n",
      "step_num: 801000\n",
      "step_num: 810000\n",
      "step_num: 819000\n",
      "step_num: 828000\n",
      "step_num: 837000\n",
      "Saved best model for manager 3 with draft reward 0.05613164231181145\n",
      "step_num: 846000\n",
      "step_num: 855000\n",
      "step_num: 864000\n",
      "step_num: 873000\n",
      "step_num: 882000\n",
      "step_num: 891000\n",
      "step_num: 900000\n",
      "step_num: 909000\n",
      "step_num: 918000\n",
      "step_num: 927000\n",
      "step_num: 936000\n",
      "step_num: 945000\n",
      "Saved best model for manager 7 with draft reward 0.03869079425930977\n",
      "step_num: 954000\n",
      "step_num: 963000\n",
      "step_num: 972000\n",
      "step_num: 981000\n",
      "step_num: 990000\n",
      "step_num: 999000\n",
      "step_num: 1008000\n",
      "step_num: 1017000\n",
      "step_num: 1026000\n",
      "step_num: 1035000\n",
      "Saved best model for manager 7 with draft reward 0.038696665316820145\n",
      "step_num: 1044000\n",
      "step_num: 1053000\n",
      "Saved best model for manager 2 with draft reward 0.04350860416889191\n",
      "step_num: 1062000\n",
      "step_num: 1071000\n",
      "step_num: 1080000\n",
      "step_num: 1089000\n",
      "step_num: 1098000\n",
      "step_num: 1107000\n",
      "step_num: 1116000\n",
      "step_num: 1125000\n",
      "step_num: 1134000\n",
      "step_num: 1143000\n",
      "step_num: 1152000\n",
      "step_num: 1161000\n",
      "step_num: 1170000\n",
      "step_num: 1179000\n",
      "Saved best model for manager 0 with draft reward 0.09910464286804199\n",
      "step_num: 1188000\n",
      "step_num: 1197000\n",
      "step_num: 1206000\n",
      "step_num: 1215000\n",
      "step_num: 1224000\n",
      "step_num: 1233000\n",
      "step_num: 1242000\n",
      "step_num: 1251000\n",
      "step_num: 1260000\n",
      "step_num: 1269000\n",
      "step_num: 1278000\n",
      "step_num: 1287000\n",
      "step_num: 1296000\n",
      "step_num: 1305000\n",
      "step_num: 1314000\n",
      "step_num: 1323000\n",
      "step_num: 1332000\n",
      "step_num: 1341000\n",
      "Saved best model for manager 2 with draft reward 0.04496964439749718\n",
      "step_num: 1350000\n",
      "step_num: 1359000\n",
      "step_num: 1368000\n",
      "step_num: 1377000\n",
      "step_num: 1386000\n",
      "step_num: 1395000\n",
      "step_num: 1404000\n",
      "step_num: 1413000\n",
      "Saved best model for manager 7 with draft reward 0.03943362832069397\n",
      "step_num: 1422000\n",
      "step_num: 1431000\n",
      "step_num: 1440000\n",
      "step_num: 1449000\n",
      "step_num: 1458000\n",
      "step_num: 1467000\n",
      "step_num: 1476000\n",
      "step_num: 1485000\n",
      "Saved best model for manager 0 with draft reward 0.10200104117393494\n",
      "step_num: 1494000\n",
      "step_num: 1503000\n",
      "step_num: 1512000\n",
      "step_num: 1521000\n",
      "step_num: 1530000\n",
      "step_num: 1539000\n",
      "step_num: 1548000\n",
      "step_num: 1557000\n",
      "step_num: 1566000\n",
      "step_num: 1575000\n",
      "step_num: 1584000\n",
      "step_num: 1593000\n",
      "step_num: 1602000\n",
      "step_num: 1611000\n",
      "Saved best model for manager 7 with draft reward 0.03957255184650421\n",
      "step_num: 1620000\n",
      "Saved best model for manager 2 with draft reward 0.04511319473385811\n",
      "step_num: 1629000\n",
      "step_num: 1638000\n",
      "step_num: 1647000\n",
      "Saved best model for manager 8 with draft reward 0.040757760405540466\n",
      "step_num: 1656000\n",
      "step_num: 1665000\n",
      "step_num: 1674000\n",
      "step_num: 1683000\n",
      "step_num: 1692000\n",
      "step_num: 1701000\n",
      "step_num: 1710000\n",
      "Saved best model for manager 2 with draft reward 0.04612697288393974\n",
      "step_num: 1719000\n",
      "step_num: 1728000\n",
      "step_num: 1737000\n",
      "step_num: 1746000\n",
      "step_num: 1755000\n",
      "step_num: 1764000\n",
      "step_num: 1773000\n",
      "step_num: 1782000\n",
      "step_num: 1791000\n",
      "step_num: 1800000\n",
      "step_num: 1809000\n",
      "step_num: 1818000\n",
      "step_num: 1827000\n",
      "step_num: 1836000\n",
      "step_num: 1845000\n",
      "step_num: 1854000\n",
      "step_num: 1863000\n",
      "Saved best model for manager 4 with draft reward 0.045384481549263\n",
      "step_num: 1872000\n",
      "step_num: 1881000\n",
      "step_num: 1890000\n",
      "step_num: 1899000\n",
      "step_num: 1908000\n",
      "step_num: 1917000\n",
      "step_num: 1926000\n",
      "step_num: 1935000\n",
      "step_num: 1944000\n",
      "step_num: 1953000\n",
      "Saved best model for manager 8 with draft reward 0.04166101664304733\n",
      "step_num: 1962000\n",
      "step_num: 1971000\n",
      "Saved best model for manager 2 with draft reward 0.04794923961162567\n",
      "step_num: 1980000\n",
      "step_num: 1989000\n",
      "step_num: 1998000\n",
      "step_num: 2007000\n",
      "step_num: 2016000\n",
      "step_num: 2025000\n",
      "step_num: 2034000\n",
      "step_num: 2043000\n",
      "step_num: 2052000\n",
      "step_num: 2061000\n",
      "step_num: 2070000\n",
      "step_num: 2079000\n",
      "step_num: 2088000\n",
      "step_num: 2097000\n",
      "step_num: 2106000\n",
      "step_num: 2115000\n",
      "step_num: 2124000\n",
      "step_num: 2133000\n",
      "Saved best model for manager 0 with draft reward 0.10307183861732483\n",
      "step_num: 2142000\n",
      "step_num: 2151000\n",
      "step_num: 2160000\n",
      "step_num: 2169000\n",
      "step_num: 2178000\n",
      "step_num: 2187000\n",
      "step_num: 2196000\n",
      "step_num: 2205000\n",
      "step_num: 2214000\n",
      "step_num: 2223000\n",
      "step_num: 2232000\n",
      "step_num: 2241000\n",
      "step_num: 2250000\n",
      "step_num: 2259000\n",
      "step_num: 2268000\n",
      "step_num: 2277000\n",
      "step_num: 2286000\n",
      "step_num: 2295000\n",
      "step_num: 2304000\n",
      "step_num: 2313000\n",
      "step_num: 2322000\n",
      "step_num: 2331000\n",
      "step_num: 2340000\n",
      "step_num: 2349000\n",
      "step_num: 2358000\n",
      "step_num: 2367000\n",
      "step_num: 2376000\n",
      "step_num: 2385000\n",
      "step_num: 2394000\n",
      "step_num: 2403000\n",
      "step_num: 2412000\n",
      "step_num: 2421000\n",
      "step_num: 2430000\n",
      "step_num: 2439000\n",
      "step_num: 2448000\n",
      "step_num: 2457000\n",
      "step_num: 2466000\n",
      "step_num: 2475000\n",
      "step_num: 2484000\n",
      "step_num: 2493000\n",
      "step_num: 2502000\n",
      "step_num: 2511000\n",
      "step_num: 2520000\n",
      "step_num: 2529000\n",
      "Saved best model for manager 5 with draft reward 0.04061119630932808\n",
      "step_num: 2538000\n",
      "step_num: 2547000\n",
      "step_num: 2556000\n",
      "step_num: 2565000\n",
      "Saved best model for manager 5 with draft reward 0.04083366319537163\n",
      "step_num: 2574000\n",
      "step_num: 2583000\n",
      "step_num: 2592000\n",
      "step_num: 2601000\n",
      "step_num: 2610000\n",
      "Saved best model for manager 9 with draft reward 0.04329969733953476\n",
      "step_num: 2619000\n",
      "step_num: 2628000\n",
      "step_num: 2637000\n",
      "step_num: 2646000\n",
      "Saved best model for manager 1 with draft reward 0.044454026967287064\n",
      "step_num: 2655000\n",
      "step_num: 2664000\n",
      "step_num: 2673000\n",
      "step_num: 2682000\n",
      "step_num: 2691000\n",
      "step_num: 2700000\n",
      "step_num: 2709000\n",
      "step_num: 2718000\n",
      "Saved best model for manager 7 with draft reward 0.041484538465738297\n",
      "step_num: 2727000\n",
      "step_num: 2736000\n",
      "step_num: 2745000\n",
      "step_num: 2754000\n",
      "step_num: 2763000\n",
      "step_num: 2772000\n",
      "step_num: 2781000\n",
      "step_num: 2790000\n",
      "step_num: 2799000\n",
      "step_num: 2808000\n",
      "step_num: 2817000\n",
      "step_num: 2826000\n",
      "step_num: 2835000\n",
      "step_num: 2844000\n",
      "step_num: 2853000\n",
      "step_num: 2862000\n",
      "step_num: 2871000\n",
      "step_num: 2880000\n",
      "step_num: 2889000\n",
      "step_num: 2898000\n",
      "step_num: 2907000\n",
      "step_num: 2916000\n",
      "step_num: 2925000\n",
      "step_num: 2934000\n",
      "step_num: 2943000\n",
      "step_num: 2952000\n",
      "step_num: 2961000\n",
      "step_num: 2970000\n",
      "Saved best model for manager 6 with draft reward 0.05992110073566437\n",
      "step_num: 2979000\n",
      "step_num: 2988000\n",
      "step_num: 2997000\n",
      "step_num: 3006000\n",
      "step_num: 3015000\n",
      "Saved best model for manager 6 with draft reward 0.06015024334192276\n",
      "step_num: 3024000\n",
      "step_num: 3033000\n",
      "step_num: 3042000\n",
      "step_num: 3051000\n",
      "step_num: 3060000\n",
      "step_num: 3069000\n",
      "step_num: 3078000\n",
      "step_num: 3087000\n",
      "step_num: 3096000\n",
      "step_num: 3105000\n",
      "step_num: 3114000\n",
      "step_num: 3123000\n",
      "step_num: 3132000\n",
      "step_num: 3141000\n",
      "step_num: 3150000\n",
      "step_num: 3159000\n",
      "step_num: 3168000\n",
      "step_num: 3177000\n",
      "Saved best model for manager 5 with draft reward 0.04163677245378494\n",
      "step_num: 3186000\n",
      "step_num: 3195000\n",
      "step_num: 3204000\n",
      "step_num: 3213000\n",
      "step_num: 3222000\n",
      "step_num: 3231000\n",
      "step_num: 3240000\n",
      "step_num: 3249000\n",
      "step_num: 3258000\n",
      "step_num: 3267000\n",
      "step_num: 3276000\n",
      "step_num: 3285000\n",
      "step_num: 3294000\n",
      "step_num: 3303000\n",
      "step_num: 3312000\n",
      "step_num: 3321000\n",
      "step_num: 3330000\n",
      "step_num: 3339000\n",
      "step_num: 3348000\n",
      "step_num: 3357000\n",
      "step_num: 3366000\n",
      "step_num: 3375000\n",
      "step_num: 3384000\n",
      "step_num: 3393000\n",
      "step_num: 3402000\n",
      "step_num: 3411000\n",
      "step_num: 3420000\n",
      "step_num: 3429000\n",
      "Saved best model for manager 2 with draft reward 0.05037660896778107\n",
      "step_num: 3438000\n",
      "step_num: 3447000\n",
      "step_num: 3456000\n",
      "step_num: 3465000\n",
      "step_num: 3474000\n",
      "step_num: 3483000\n",
      "step_num: 3492000\n",
      "step_num: 3501000\n",
      "step_num: 3510000\n",
      "step_num: 3519000\n",
      "step_num: 3528000\n",
      "step_num: 3537000\n",
      "Saved best model for manager 11 with draft reward 0.04238659515976906\n",
      "step_num: 3546000\n",
      "step_num: 3555000\n",
      "step_num: 3564000\n",
      "step_num: 3573000\n",
      "step_num: 3582000\n",
      "step_num: 3591000\n",
      "step_num: 3600000\n",
      "step_num: 3609000\n",
      "step_num: 3618000\n",
      "step_num: 3627000\n",
      "step_num: 3636000\n",
      "step_num: 3645000\n",
      "step_num: 3654000\n",
      "step_num: 3663000\n",
      "step_num: 3672000\n",
      "step_num: 3681000\n",
      "step_num: 3690000\n",
      "step_num: 3699000\n",
      "step_num: 3708000\n",
      "step_num: 3717000\n",
      "step_num: 3726000\n",
      "step_num: 3735000\n",
      "step_num: 3744000\n",
      "step_num: 3753000\n",
      "step_num: 3762000\n",
      "step_num: 3771000\n",
      "step_num: 3780000\n",
      "step_num: 3789000\n",
      "step_num: 3798000\n",
      "step_num: 3807000\n",
      "Saved best model for manager 2 with draft reward 0.05341725051403046\n",
      "step_num: 3816000\n",
      "step_num: 3825000\n",
      "step_num: 3834000\n",
      "step_num: 3843000\n",
      "step_num: 3852000\n",
      "step_num: 3861000\n",
      "step_num: 3870000\n",
      "Saved best model for manager 1 with draft reward 0.044549159705638885\n",
      "step_num: 3879000\n",
      "step_num: 3888000\n",
      "step_num: 3897000\n",
      "step_num: 3906000\n",
      "step_num: 3915000\n",
      "step_num: 3924000\n",
      "step_num: 3933000\n",
      "step_num: 3942000\n",
      "step_num: 3951000\n",
      "step_num: 3960000\n",
      "step_num: 3969000\n",
      "step_num: 3978000\n",
      "step_num: 3987000\n",
      "step_num: 3996000\n",
      "step_num: 4005000\n",
      "step_num: 4014000\n",
      "step_num: 4023000\n",
      "step_num: 4032000\n",
      "step_num: 4041000\n",
      "step_num: 4050000\n",
      "step_num: 4059000\n",
      "step_num: 4068000\n",
      "step_num: 4077000\n",
      "step_num: 4086000\n",
      "step_num: 4095000\n",
      "step_num: 4104000\n",
      "step_num: 4113000\n",
      "step_num: 4122000\n",
      "step_num: 4131000\n",
      "step_num: 4140000\n",
      "step_num: 4149000\n",
      "step_num: 4158000\n",
      "step_num: 4167000\n",
      "step_num: 4176000\n",
      "step_num: 4185000\n",
      "step_num: 4194000\n",
      "step_num: 4203000\n",
      "step_num: 4212000\n",
      "step_num: 4221000\n",
      "step_num: 4230000\n",
      "step_num: 4239000\n",
      "step_num: 4248000\n",
      "step_num: 4257000\n",
      "step_num: 4266000\n",
      "Saved best model for manager 3 with draft reward 0.05701666325330734\n",
      "step_num: 4275000\n",
      "step_num: 4284000\n",
      "step_num: 4293000\n",
      "step_num: 4302000\n",
      "step_num: 4311000\n",
      "step_num: 4320000\n",
      "step_num: 4329000\n",
      "step_num: 4338000\n",
      "step_num: 4347000\n",
      "step_num: 4356000\n",
      "step_num: 4365000\n",
      "step_num: 4374000\n",
      "step_num: 4383000\n",
      "Saved best model for manager 1 with draft reward 0.04457556828856468\n",
      "step_num: 4392000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "# Generate a run_id based on the current datetime\n",
    "run_id = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "run_dir = f\"logs/PPO_{run_id}\"\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "# Configure TensorBoard logger for each manager\n",
    "# loggers = [configure(run_dir, [\"tensorboard\"]) for _ in range(NUM_MGRS)]\n",
    "logger = configure(run_dir, [\"tensorboard\"])\n",
    "\n",
    "# Initialize variables for tracking the best model\n",
    "best_rewards = [-float('inf')] * NUM_MGRS\n",
    "\n",
    "# Create subdirectories for each manager\n",
    "manager_dirs = [os.path.join(run_dir, f\"mgr_{i}\") for i in range(NUM_MGRS)]\n",
    "for manager_dir in manager_dirs:\n",
    "    os.makedirs(manager_dir, exist_ok=True)\n",
    "\n",
    "best_model_paths = [os.path.join(manager_dirs[i], f\"best_model_mgr_{i}.zip\") for i in range(NUM_MGRS)]\n",
    "\n",
    "# Storage for info data\n",
    "info_history = []\n",
    "\n",
    "# Assign the new logger to each model\n",
    "for idx, model in enumerate(models):\n",
    "    model.set_logger(logger)\n",
    "\n",
    "step_num = 0\n",
    "n_episodes = 0\n",
    "\n",
    "for model in models:\n",
    "    assert model.n_steps % NUM_DRAFT_ROUNDS == 0, \"n_steps must be divisible by the number of draft rounds\"\n",
    "    \n",
    "while step_num < total_timesteps:\n",
    "    print('step_num:', step_num)\n",
    "    # reset buffers\n",
    "    rollout_step_num = 0\n",
    "    for model in models:\n",
    "        model.rollout_buffer.reset()\n",
    "    \n",
    "    while rollout_step_num < models[0].n_steps*NUM_MGRS:\n",
    "        \n",
    "        env.reset()\n",
    "        sarstti = {i: [] for i in range(NUM_MGRS)}\n",
    "        mgr_values = {i: [] for i in range(NUM_MGRS)}\n",
    "        mgr_log_probs = {i: [] for i in range(NUM_MGRS)}\n",
    "        \n",
    "        # complete 1 draft\n",
    "        new_episode = 1\n",
    "        for _, row in env.draft.iterrows(): # each turn of draft\n",
    "            mgr = row['mgr']\n",
    "            model = models[mgr]\n",
    "            state = env.state\n",
    "            # print(np.mean(env.state))\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(model.device)\n",
    "                actions, values, log_probs = model.policy(state_tensor, deterministic=False)\n",
    "            action = actions.cpu().numpy() \n",
    "\n",
    "            # clip actions to avoid out of bounds error\n",
    "            # as we are sampling from an unbounded gaussian distribution\n",
    "            clipped_actions = np.clip(actions, 0, ACTION_SPACE_DIM)\n",
    "            action = clipped_actions.item()\n",
    "            next_state, _, terminated, truncated, info = env.step(action)\n",
    "            # print(np.mean(next_state))\n",
    "            step_num += 1\n",
    "            rollout_step_num += 1\n",
    "            \n",
    "            action = np.array(action).reshape(-1, 1)\n",
    "            sarstti[mgr].append((state, action, 0, next_state, terminated, truncated, info))\n",
    "            mgr_values[mgr].append(values)\n",
    "            mgr_log_probs[mgr].append(log_probs)\n",
    "        \n",
    "        n_episodes += 1\n",
    "        # compute rewards for the draft\n",
    "        for mgr in range(NUM_MGRS):\n",
    "            reward = env.calc_reward(mgr)\n",
    "            # print(f\"Reward for manager {mgr}: {reward}\")\n",
    "            state, action, _, next_state, terminated, truncated, info = sarstti[mgr][-1]\n",
    "            sarstti[mgr][-1] = (state, action, reward, next_state, terminated, truncated, info)\n",
    "        \n",
    "        # add every step of draft to rollout buffer for each manager\n",
    "        for mgr, model in enumerate(models):\n",
    "            for round in range(NUM_DRAFT_ROUNDS):\n",
    "                state, action, reward, next_state, terminated, truncated, info = sarstti[mgr][round]\n",
    "                model.rollout_buffer.add(\n",
    "                    state,\n",
    "                    action,\n",
    "                    reward,\n",
    "                    new_episode,\n",
    "                    mgr_values[mgr][round],\n",
    "                    mgr_log_probs[mgr][round]\n",
    "                )\n",
    "                new_episode = 0\n",
    "                \n",
    "        if n_episodes % n_episodes_model == 0:\n",
    "            for mgr, model in enumerate(models):\n",
    "                model_save_path = os.path.join(manager_dirs[mgr], f\"ppo_model_mgr_{mgr}_episode_{n_episodes}.zip\")\n",
    "                model.save(model_save_path)\n",
    "                print(f\"Saved model for manager {mgr} at episode {n_episodes} to {model_save_path}\")\n",
    "        \n",
    "        if n_episodes % n_episodes_info == 0:\n",
    "            for mgr in range(NUM_MGRS):\n",
    "                info_file_path = os.path.join(manager_dirs[mgr], f\"info_history_episode_{n_episodes}.json\")\n",
    "                with open(info_file_path, 'w') as f:\n",
    "                    json.dump(info_history, f, indent=4)\n",
    "                print(f\"Saved info history to {info_file_path}\")\n",
    "            # Clear info history after saving to avoid redundant data\n",
    "            info_history.clear()\n",
    "    \n",
    "\n",
    "                \n",
    "    for mgr, model in enumerate(models):\n",
    "        \n",
    "        rewards = model.rollout_buffer.rewards\n",
    "        mean_reward = rewards.mean()\n",
    "        \n",
    "        # Log metrics to TensorBoard for this specific manager\n",
    "        model.logger.record(f\"rewards/draft_reward_mgr_{mgr}\", mean_reward)\n",
    "        model.logger.dump(n_episodes)\n",
    "\n",
    "\n",
    "        # check if best reward and save model if so\n",
    "        if mean_reward > best_rewards[mgr]:\n",
    "            best_rewards[mgr] = mean_reward\n",
    "            model.save(best_model_paths[mgr])\n",
    "            print(f\"Saved best model for manager {mgr} with draft reward {mean_reward}\")    \n",
    "        \n",
    "        model.rollout_buffer.compute_returns_and_advantage(last_values=torch.zeros_like(mgr_values[mgr][-1]), dones=terminated) # might need to make dones an array, not sure\n",
    "        model.train()\n",
    "        \n",
    "        # Extract and log individual losses\n",
    "        pg_loss = np.mean(model.logger.name_to_value['train/policy_gradient_loss'])  # Policy gradient loss\n",
    "        value_loss = np.mean(model.logger.name_to_value['train/value_loss'])  # Value loss\n",
    "        entropy_loss = np.mean(model.logger.name_to_value['train/entropy_loss'])  # Entropy loss\n",
    "\n",
    "        # Calculate total loss\n",
    "        total_loss = pg_loss + model.vf_coef * value_loss + model.ent_coef * entropy_loss\n",
    "\n",
    "        # Log the individual and total losses\n",
    "        model.logger.record(f\"loss_total/total_loss_mgr_{mgr}\", total_loss)\n",
    "        model.logger.record(f\"loss_pg/pg_loss_mgr_{mgr}\", pg_loss)\n",
    "        model.logger.record(f\"loss_value/value_loss_mgr_{mgr}\", value_loss)\n",
    "        model.logger.record(f\"loss_entropy/entropy_loss_mgr_{mgr}\", entropy_loss)\n",
    "\n",
    "        # Dump logs to TensorBoard\n",
    "        model.logger.dump(n_episodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, TypeVar\n",
    "from rl_env_marl import MARLDraftEnv, NUM_DRAFT_ROUNDS, NUM_MGRS, ACTION_SPACE_DIM\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from pprint import pprint\n",
    "import os\n",
    "from stable_baselines3.common.logger import configure\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def learning_rate_schedule(initial_lr=1e-4, final_lr=5e-5):\n",
    "    return lambda progress_remaining: progress_remaining * (initial_lr - final_lr) + final_lr\n",
    "\n",
    "learning_rate_schedule_fn = learning_rate_schedule(initial_lr=1e-4, final_lr=5e-5)\n",
    "\n",
    "\n",
    "ppo_params = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"learning_rate\": learning_rate_schedule_fn,  # Adaptive learning rate\n",
    "    \"n_steps\": 750,\n",
    "    \"batch_size\": 750,\n",
    "    \"n_epochs\": 30,\n",
    "    \"gamma\": 0.99,\n",
    "    \"gae_lambda\": 0.95,\n",
    "    \"clip_range\": 0.2,\n",
    "    \"clip_range_vf\": None,\n",
    "    \"normalize_advantage\": True,\n",
    "    \"ent_coef\": .03, # Entropy coefficient for the loss calculation\n",
    "    \"vf_coef\": 0.5,\n",
    "    \"max_grad_norm\": 0.7,\n",
    "    \"use_sde\": False,\n",
    "    \"sde_sample_freq\": -1,\n",
    "    \"rollout_buffer_class\": None,\n",
    "    \"rollout_buffer_kwargs\": None,\n",
    "    # \"target_kl\": 0.01,\n",
    "    # \"target_kl\": None, # TODO: TRY THIS NEXT TEST BECAUSE KEEP GETTING EARLY STOPPING\n",
    "    \"stats_window_size\": 100,\n",
    "    \"policy_kwargs\": dict(net_arch=[dict(pi=[256, 256, 128], vf=[256, 256, 128])]),\n",
    "    \"verbose\": 1,\n",
    "    \"seed\": 69,\n",
    "    \"device\": \"auto\",\n",
    "    \"_init_setup_model\": True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env(MARLDraftEnv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "env = MARLDraftEnv()\n",
    "\n",
    "n_agents = NUM_MGRS\n",
    "models = [PPO.load(\"logs/PPO_20240827-143658/best_model.zip\", env=env) for _ in range(n_agents)]\n",
    "\n",
    "# total_episodes = int(2e6)\n",
    "total_timesteps = int(15*2e6)\n",
    "# n_episodes = 50  # Number of episodes per update\n",
    "\n",
    "# saving frequency\n",
    "n_episodes_info = 5e4\n",
    "n_episodes_model = 5e4 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "Saved best model for manager 0 with draft reward -0.5\n",
      "Saved best model for manager 1 with draft reward 6.567834659156002\n",
      "Saved best model for manager 2 with draft reward -0.5\n",
      "Saved best model for manager 3 with draft reward -0.5\n",
      "Saved best model for manager 4 with draft reward -0.5\n",
      "Saved best model for manager 5 with draft reward 5.433010682380735\n",
      "Saved best model for manager 6 with draft reward -0.5\n",
      "Saved best model for manager 7 with draft reward -0.5\n",
      "Saved best model for manager 8 with draft reward -0.5\n",
      "Saved best model for manager 9 with draft reward -0.5\n",
      "Saved best model for manager 10 with draft reward -0.5\n",
      "Saved best model for manager 11 with draft reward -0.5\n",
      "Saved best model for manager 7 with draft reward 5.438974035832456\n",
      "Saved best model for manager 10 with draft reward 6.501792532224149\n",
      "Saved best model for manager 6 with draft reward 6.568311359134838\n",
      "Saved best model for manager 11 with draft reward 5.718961034382634\n",
      "Saved best model for manager 1 with draft reward 6.842814173228183\n",
      "Saved best model for manager 9 with draft reward 5.576002833590087\n",
      "Saved best model for manager 2 with draft reward 6.82394795918282\n",
      "Saved best model for manager 11 with draft reward 5.765592714748655\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# complete 1 draft\u001b[39;00m\n\u001b[1;32m     54\u001b[0m new_episode \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m env\u001b[38;5;241m.\u001b[39mdraft\u001b[38;5;241m.\u001b[39miterrows(): \u001b[38;5;66;03m# each turn of draft\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmgr\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     57\u001b[0m     model \u001b[38;5;241m=\u001b[39m models[mgr]\n",
      "File \u001b[0;32m~/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/pandas/core/frame.py:1554\u001b[0m, in \u001b[0;36mDataFrame.iterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1552\u001b[0m using_cow \u001b[38;5;241m=\u001b[39m using_copy_on_write()\n\u001b[1;32m   1553\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues):\n\u001b[0;32m-> 1554\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m using_cow \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mis_single_block:\n\u001b[1;32m   1556\u001b[0m         s\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39madd_references(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/pandas/core/series.py:588\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    586\u001b[0m manager \u001b[38;5;241m=\u001b[39m _get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.data_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 588\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mSingleBlockManager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    590\u001b[0m     data \u001b[38;5;241m=\u001b[39m SingleArrayManager\u001b[38;5;241m.\u001b[39mfrom_array(data, index)\n",
      "File \u001b[0;32m~/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/pandas/core/internals/managers.py:1870\u001b[0m, in \u001b[0;36mSingleBlockManager.from_array\u001b[0;34m(cls, array, index, refs)\u001b[0m\n\u001b[1;32m   1863\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1864\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_array\u001b[39m(\n\u001b[1;32m   1865\u001b[0m     \u001b[38;5;28mcls\u001b[39m, array: ArrayLike, index: Index, refs: BlockValuesRefs \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SingleBlockManager:\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;124;03m    Constructor for if we have an array that is not yet a Block.\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1870\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_coerce_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1871\u001b[0m     bp \u001b[38;5;241m=\u001b[39m BlockPlacement(\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(index)))\n\u001b[1;32m   1872\u001b[0m     block \u001b[38;5;241m=\u001b[39m new_block(array, placement\u001b[38;5;241m=\u001b[39mbp, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, refs\u001b[38;5;241m=\u001b[39mrefs)\n",
      "File \u001b[0;32m~/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/pandas/core/internals/blocks.py:2645\u001b[0m, in \u001b[0;36mmaybe_coerce_values\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   2638\u001b[0m     \u001b[38;5;18m__slots__\u001b[39m \u001b[38;5;241m=\u001b[39m ()\n\u001b[1;32m   2641\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m   2642\u001b[0m \u001b[38;5;66;03m# Constructor Helpers\u001b[39;00m\n\u001b[0;32m-> 2645\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmaybe_coerce_values\u001b[39m(values: ArrayLike) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArrayLike:\n\u001b[1;32m   2646\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2647\u001b[0m \u001b[38;5;124;03m    Input validation for values passed to __init__. Ensure that\u001b[39;00m\n\u001b[1;32m   2648\u001b[0m \u001b[38;5;124;03m    any datetime64/timedelta64 dtypes are in nanoseconds.  Ensure\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2657\u001b[0m \u001b[38;5;124;03m    values : np.ndarray or ExtensionArray\u001b[39;00m\n\u001b[1;32m   2658\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2659\u001b[0m     \u001b[38;5;66;03m# Caller is responsible for ensuring NumpyExtensionArray is already extracted.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "# Generate a run_id based on the current datetime\n",
    "run_id = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "run_dir = f\"logs/PPO_{run_id}\"\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "# Configure TensorBoard logger for each manager\n",
    "# loggers = [configure(run_dir, [\"tensorboard\"]) for _ in range(NUM_MGRS)]\n",
    "logger = configure(run_dir, [\"tensorboard\"])\n",
    "\n",
    "# Initialize variables for tracking the best model\n",
    "best_rewards = [-float('inf')] * NUM_MGRS\n",
    "\n",
    "# Create subdirectories for each manager\n",
    "manager_dirs = [os.path.join(run_dir, f\"mgr_{i}\") for i in range(NUM_MGRS)]\n",
    "for manager_dir in manager_dirs:\n",
    "    os.makedirs(manager_dir, exist_ok=True)\n",
    "\n",
    "best_model_paths = [os.path.join(manager_dirs[i], f\"best_model_mgr_{i}.zip\") for i in range(NUM_MGRS)]\n",
    "\n",
    "# Storage for info data\n",
    "info_history = []\n",
    "\n",
    "# Assign the new logger to each model\n",
    "for idx, model in enumerate(models):\n",
    "    model.set_logger(logger)\n",
    "\n",
    "step_num = 0\n",
    "n_episodes = 0\n",
    "\n",
    "for model in models:\n",
    "    assert model.n_steps % NUM_DRAFT_ROUNDS == 0, \"n_steps must be divisible by the number of draft rounds\"\n",
    "    \n",
    "while step_num < total_timesteps:\n",
    "    print('step_num:', step_num)\n",
    "    # reset buffers\n",
    "    rollout_step_num = 0\n",
    "    for model in models:\n",
    "        model.rollout_buffer.reset()\n",
    "    \n",
    "    while rollout_step_num < models[0].n_steps*NUM_MGRS:\n",
    "        \n",
    "        env.reset()\n",
    "        sarstti = {i: [] for i in range(NUM_MGRS)}\n",
    "        mgr_values = {i: [] for i in range(NUM_MGRS)}\n",
    "        mgr_log_probs = {i: [] for i in range(NUM_MGRS)}\n",
    "        \n",
    "        # complete 1 draft\n",
    "        new_episode = 1\n",
    "        for _, row in env.draft.iterrows(): # each turn of draft\n",
    "            mgr = row['mgr']\n",
    "            model = models[mgr]\n",
    "            state = env.state\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(model.device)\n",
    "                actions, values, log_probs = model.policy(state_tensor, deterministic=False)\n",
    "            action = actions.cpu().numpy() \n",
    "\n",
    "            # clip actions to avoid out of bounds error\n",
    "            # as we are sampling from an unbounded gaussian distribution\n",
    "            clipped_actions = np.clip(actions, 0, ACTION_SPACE_DIM)\n",
    "            action = clipped_actions.item()\n",
    "            next_state, _, terminated, truncated, info = env.step(action)\n",
    "            step_num += 1\n",
    "            rollout_step_num += 1\n",
    "            \n",
    "            action = np.array(action).reshape(-1, 1)\n",
    "            sarstti[mgr].append((state, action, 0, next_state, terminated, truncated, info))\n",
    "            mgr_values[mgr].append(values)\n",
    "            mgr_log_probs[mgr].append(log_probs)\n",
    "        \n",
    "        n_episodes += 1\n",
    "        # compute rewards for the draft\n",
    "        for mgr in range(NUM_MGRS):\n",
    "            reward = env.calc_reward(mgr)\n",
    "            state, action, _, next_state, terminated, truncated, info = sarstti[mgr][-1]\n",
    "            sarstti[mgr][-1] = (state, action, reward, next_state, terminated, truncated, info)\n",
    "\n",
    "            # Log metrics to TensorBoard for this specific manager\n",
    "            model.logger.record(f\"train/draft_reward_mgr_{mgr}\", reward)\n",
    "            model.logger.dump(n_episodes)\n",
    "        \n",
    "            # check if best reward and save model if so\n",
    "            if reward > best_rewards[mgr]:\n",
    "                best_rewards[mgr] = reward\n",
    "                model.save(best_model_paths[mgr])\n",
    "                print(f\"Saved best model for manager {mgr} with draft reward {reward}\")\n",
    "        \n",
    "        # add every step of draft to rollout buffer for each manager\n",
    "        for i, model in enumerate(models):\n",
    "            for round in range(NUM_DRAFT_ROUNDS):\n",
    "                state, action, reward, next_state, terminated, truncated, info = sarstti[mgr][round]\n",
    "                # print(\"values shape\", values[mgr][round].shape)\n",
    "                # print(\"log_probs shape\", log_probs[mgr][round].shape)\n",
    "                model.rollout_buffer.add(\n",
    "                    state,\n",
    "                    action,\n",
    "                    reward,\n",
    "                    new_episode,\n",
    "                    mgr_values[mgr][round],\n",
    "                    mgr_log_probs[mgr][round]\n",
    "                )\n",
    "                new_episode = 0\n",
    "                \n",
    "        if n_episodes % n_episodes_model == 0:\n",
    "            for mgr, model in enumerate(models):\n",
    "                model_save_path = os.path.join(manager_dirs[mgr], f\"ppo_model_mgr_{mgr}_episode_{n_episodes}.zip\")\n",
    "                model.save(model_save_path)\n",
    "                print(f\"Saved model for manager {mgr} at episode {n_episodes} to {model_save_path}\")\n",
    "        \n",
    "        if n_episodes % n_episodes_info == 0:\n",
    "            for mgr in range(NUM_MGRS):\n",
    "                info_file_path = os.path.join(manager_dirs[mgr], f\"info_history_episode_{n_episodes}.json\")\n",
    "                with open(info_file_path, 'w') as f:\n",
    "                    json.dump(info_history, f, indent=4)\n",
    "                print(f\"Saved info history to {info_file_path}\")\n",
    "            # Clear info history after saving to avoid redundant data\n",
    "            info_history.clear()\n",
    "            \n",
    "                \n",
    "    for mgr, model in enumerate(models):\n",
    "        model.rollout_buffer.compute_returns_and_advantage(last_values=torch.zeros_like(mgr_values[mgr][-1]), dones=terminated) # might need to make dones an array, not sure\n",
    "        model.train()\n",
    "\n",
    "            \n",
    "            \n",
    "    \n",
    "# for episode_num in range(total_episodes): # each draft\n",
    "    \n",
    "#     env.reset()\n",
    "#     sarstti = {i: [] for i in range(NUM_MGRS)}\n",
    "    \n",
    "#     for _, row in env.draft.iterrows(): # each turn of draft\n",
    "#         mgr = row['mgr']\n",
    "#         model = models[mgr]\n",
    "#         state = env.state\n",
    "#         action, _ = model.predict(env.state, deterministic=False)\n",
    "#         next_state, _, terminated, truncated, info = env.step(action.item())\n",
    "#         sarstti[mgr].append((state, action, 0, next_state, terminated, truncated, info))\n",
    "#         step_num += 1\n",
    "        \n",
    "#         # Append info to the history for saving\n",
    "#         info_history.append(info)\n",
    "    \n",
    "#     # calculate rewards\n",
    "#     for mgr in range(NUM_MGRS):\n",
    "#         state, action, _, next_state, _, truncated, info = sarstti[mgr][-1]\n",
    "#         reward = env.calc_reward(mgr)\n",
    "#         terminated = True\n",
    "#         sarstti[mgr][-1] = (state, action, reward, next_state, terminated, truncated, info)\n",
    "\n",
    "        \n",
    "        \n",
    "#     for mgr, model in enumerate(models):  # for each manager\n",
    "#         # print(f\"Training model for manager {mgr}\")\n",
    "        \n",
    "#         episode_start = 1\n",
    "#         for state, action, reward, next_state, terminated, truncated, info in sarstti[mgr]:\n",
    "#             # Convert the state (observation) from numpy to torch tensor\n",
    "#             # print(f\"state dim: {state.shape}\")\n",
    "#             # print(f\"action dim: {action.shape}\")\n",
    "#             with torch.no_grad():\n",
    "#                 state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(model.device)\n",
    "#                 action_tensor = torch.tensor(action, dtype=torch.float32).unsqueeze(0).to(model.device)\n",
    "                \n",
    "#                 # Calculate the value of the state and log probability of the action\n",
    "#                 value = model.policy.predict_values(state_tensor)\n",
    "#                 log_prob = model.policy.evaluate_actions(state_tensor, action_tensor)[1]\n",
    "\n",
    "           \n",
    "#             # Add experience to the rollout buffer\n",
    "#             model.rollout_buffer.add(\n",
    "#                 state,\n",
    "#                 action,\n",
    "#                 reward,\n",
    "#                 episode_start,\n",
    "#                 value,\n",
    "#                 log_prob\n",
    "#             )\n",
    "#             episode_start = 0\n",
    "#         dones = [x[4] for x in sarstti[mgr]]\n",
    "#         model.rollout_buffer.compute_returns_and_advantage(last_values=torch.zeros_like(value), dones=terminated)\n",
    "#         # ------\n",
    "#         # Only train if the buffer is full\n",
    "#         if model.rollout_buffer.full:\n",
    "#             model.train()\n",
    "            \n",
    "#             # Track draft rewards (or any other performance metric)\n",
    "#             draft_reward = sum([x[2] for x in sarstti[mgr]])  # Sum of rewards for the manager\n",
    "            \n",
    "#             # Save the best model if the performance improves\n",
    "#             if draft_reward > best_rewards[mgr]:\n",
    "#                 best_rewards[mgr] = draft_reward\n",
    "#                 model.save(best_model_paths[mgr])\n",
    "#                 print(f\"Saved best model for manager {mgr} with draft reward {draft_reward}\")\n",
    "            \n",
    "            # Log metrics to TensorBoard for this specific manager\n",
    "            # model.logger.record(f\"train/draft_reward_mgr_{mgr}\", draft_reward)\n",
    "            # model.logger.dump(episode_num)\n",
    "        # ----------\n",
    "        \n",
    "        # model.train()\n",
    "        \n",
    "        # # Track draft rewards (or any other performance metric)\n",
    "        # draft_reward = sum([x[2] for x in sarstti[mgr]])  # Sum of rewards for the manager\n",
    "        \n",
    "        # # Save the best model if the performance improves\n",
    "        # if draft_reward > best_rewards[mgr]:\n",
    "        #     best_rewards[mgr] = draft_reward\n",
    "        #     model.save(best_model_paths[mgr])\n",
    "        #     print(f\"Saved best model for manager {mgr} with draft reward {draft_reward}\")\n",
    "        \n",
    "        # # Log metrics to TensorBoard for this specific manager\n",
    "        # model.logger.record(f\"train/draft_reward_mgr_{mgr}\", draft_reward)\n",
    "        # model.logger.dump(episode_num)\n",
    "    \n",
    "    # Save info to a JSON file every n_episodes_info\n",
    "    # if episode_num % n_episodes_info == 0:\n",
    "    #     for mgr in range(NUM_MGRS):\n",
    "    #         info_file_path = os.path.join(manager_dirs[mgr], f\"info_history_episode_{episode_num}.json\")\n",
    "    #         with open(info_file_path, 'w') as f:\n",
    "    #             json.dump(info_history, f, indent=4)\n",
    "    #         print(f\"Saved info history to {info_file_path}\")\n",
    "    #     # Clear info history after saving to avoid redundant data\n",
    "    #     info_history.clear()\n",
    "\n",
    "    # Save the model every n_episodes_model\n",
    "    # if episode_num % n_episodes_model == 0:\n",
    "    #     for mgr, model in enumerate(models):\n",
    "    #         model_save_path = os.path.join(manager_dirs[mgr], f\"ppo_model_mgr_{mgr}_episode_{episode_num}.zip\")\n",
    "    #         model.save(model_save_path)\n",
    "    #         print(f\"Saved model for manager {mgr} at episode {episode_num} to {model_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

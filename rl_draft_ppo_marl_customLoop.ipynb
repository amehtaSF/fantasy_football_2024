{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, TypeVar\n",
    "from rl_env_marl import MARLDraftEnv, NUM_DRAFT_ROUNDS, NUM_MGRS, ACTION_SPACE_DIM\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from pprint import pprint\n",
    "import os\n",
    "from stable_baselines3.common.logger import configure\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def learning_rate_schedule(initial_lr=1e-4, final_lr=5e-5):\n",
    "    return lambda progress_remaining: progress_remaining * (initial_lr - final_lr) + final_lr\n",
    "\n",
    "learning_rate_schedule_fn = learning_rate_schedule(initial_lr=1e-4, final_lr=5e-5)\n",
    "\n",
    "\n",
    "ppo_params = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"learning_rate\": learning_rate_schedule_fn,  # Adaptive learning rate\n",
    "    \"n_steps\": 750,\n",
    "    \"batch_size\": 750,\n",
    "    \"n_epochs\": 30,\n",
    "    \"gamma\": 0.99,\n",
    "    \"gae_lambda\": 0.95,\n",
    "    \"clip_range\": 0.2,\n",
    "    \"clip_range_vf\": None,\n",
    "    \"normalize_advantage\": True,\n",
    "    \"ent_coef\": .03, # Entropy coefficient for the loss calculation\n",
    "    \"vf_coef\": 0.5,\n",
    "    \"max_grad_norm\": 0.7,\n",
    "    \"use_sde\": False,\n",
    "    \"sde_sample_freq\": -1,\n",
    "    \"rollout_buffer_class\": None,\n",
    "    \"rollout_buffer_kwargs\": None,\n",
    "    # \"target_kl\": 0.01,\n",
    "    # \"target_kl\": None, \n",
    "    \"stats_window_size\": 100,\n",
    "    \"policy_kwargs\": dict(net_arch=[dict(pi=[256, 256, 128], vf=[256, 256, 128])]),\n",
    "    \"verbose\": 1,\n",
    "    \"seed\": 69,\n",
    "    \"device\": \"auto\",\n",
    "    \"_init_setup_model\": True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env(MARLDraftEnv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "env = MARLDraftEnv()\n",
    "\n",
    "n_agents = NUM_MGRS\n",
    "model_path = \"logs/PPO_20240831-014945/mgr_{mgr}/best_model_mgr_{mgr}.zip\"\n",
    "models = [PPO.load(model_path.format(mgr=mgr), env=env) for mgr in range(n_agents)]\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    # Update the optimizer with the new learning rate\n",
    "    if model.policy.optimizer is not None:\n",
    "        for param_group in model.policy.optimizer.param_groups:\n",
    "            param_group['lr'] = 5e-5\n",
    "        \n",
    "\n",
    "# total_episodes = int(2e6)\n",
    "total_timesteps = int(NUM_MGRS*2e6)\n",
    "# n_episodes = 50  # Number of episodes per update\n",
    "\n",
    "# saving frequency\n",
    "n_episodes_info = 5e4\n",
    "n_episodes_model = 5e4 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_num: 0\n",
      "Saved best model for manager 0 with draft reward 0.07414621859788895\n",
      "Saved best model for manager 1 with draft reward 0.037895169109106064\n",
      "Saved best model for manager 2 with draft reward 0.04009617492556572\n",
      "Saved best model for manager 3 with draft reward 0.03857256844639778\n",
      "Saved best model for manager 4 with draft reward 0.03775728866457939\n",
      "Saved best model for manager 5 with draft reward 0.03534065932035446\n",
      "Saved best model for manager 6 with draft reward 0.05398833751678467\n",
      "Saved best model for manager 7 with draft reward 0.0336853452026844\n",
      "Saved best model for manager 8 with draft reward 0.03560575097799301\n",
      "Saved best model for manager 9 with draft reward 0.04129059240221977\n",
      "Saved best model for manager 10 with draft reward 0.03209613636136055\n",
      "Saved best model for manager 11 with draft reward 0.04082512483000755\n",
      "step_num: 9000\n",
      "Saved best model for manager 0 with draft reward 0.08529936522245407\n",
      "Saved best model for manager 3 with draft reward 0.044147055596113205\n",
      "Saved best model for manager 5 with draft reward 0.039841730147600174\n",
      "Saved best model for manager 7 with draft reward 0.03554655984044075\n",
      "Saved best model for manager 8 with draft reward 0.03849320858716965\n",
      "step_num: 18000\n",
      "Saved best model for manager 0 with draft reward 0.08949728310108185\n",
      "Saved best model for manager 1 with draft reward 0.04242061823606491\n",
      "step_num: 27000\n",
      "Saved best model for manager 3 with draft reward 0.045907389372587204\n",
      "Saved best model for manager 7 with draft reward 0.036559782922267914\n",
      "step_num: 36000\n",
      "Saved best model for manager 4 with draft reward 0.0421915166079998\n",
      "Saved best model for manager 10 with draft reward 0.032190680503845215\n",
      "step_num: 45000\n",
      "Saved best model for manager 4 with draft reward 0.04258967936038971\n",
      "step_num: 54000\n",
      "Saved best model for manager 6 with draft reward 0.05835740268230438\n",
      "Saved best model for manager 10 with draft reward 0.03686061501502991\n",
      "step_num: 63000\n",
      "step_num: 72000\n",
      "step_num: 81000\n",
      "Saved best model for manager 0 with draft reward 0.09382067620754242\n",
      "step_num: 90000\n",
      "step_num: 99000\n",
      "step_num: 108000\n",
      "Saved best model for manager 7 with draft reward 0.03841342031955719\n",
      "step_num: 117000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "# Generate a run_id based on the current datetime\n",
    "run_id = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "run_dir = f\"logs/PPO_{run_id}\"\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "# Configure TensorBoard logger for each manager\n",
    "# loggers = [configure(run_dir, [\"tensorboard\"]) for _ in range(NUM_MGRS)]\n",
    "logger = configure(run_dir, [\"tensorboard\"])\n",
    "\n",
    "# Initialize variables for tracking the best model\n",
    "best_rewards = [-float('inf')] * NUM_MGRS\n",
    "\n",
    "# Create subdirectories for each manager\n",
    "manager_dirs = [os.path.join(run_dir, f\"mgr_{i}\") for i in range(NUM_MGRS)]\n",
    "for manager_dir in manager_dirs:\n",
    "    os.makedirs(manager_dir, exist_ok=True)\n",
    "\n",
    "best_model_paths = [os.path.join(manager_dirs[i], f\"best_model_mgr_{i}.zip\") for i in range(NUM_MGRS)]\n",
    "\n",
    "# Storage for info data\n",
    "info_history = []\n",
    "\n",
    "# Assign the new logger to each model\n",
    "for idx, model in enumerate(models):\n",
    "    model.set_logger(logger)\n",
    "\n",
    "step_num = 0\n",
    "n_episodes = 0\n",
    "\n",
    "for model in models:\n",
    "    assert model.n_steps % NUM_DRAFT_ROUNDS == 0, \"n_steps must be divisible by the number of draft rounds\"\n",
    "    \n",
    "while step_num < total_timesteps:\n",
    "    print('step_num:', step_num)\n",
    "    # reset buffers\n",
    "    rollout_step_num = 0\n",
    "    for model in models:\n",
    "        model.rollout_buffer.reset()\n",
    "    \n",
    "    while rollout_step_num < models[0].n_steps*NUM_MGRS:\n",
    "        \n",
    "        env.reset()\n",
    "        sarstti = {i: [] for i in range(NUM_MGRS)}\n",
    "        mgr_values = {i: [] for i in range(NUM_MGRS)}\n",
    "        mgr_log_probs = {i: [] for i in range(NUM_MGRS)}\n",
    "        \n",
    "        # complete 1 draft\n",
    "        new_episode = 1\n",
    "        for _, row in env.draft.iterrows(): # each turn of draft\n",
    "            mgr = row['mgr']\n",
    "            model = models[mgr]\n",
    "            state = env.state\n",
    "            # print(np.mean(env.state))\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(model.device)\n",
    "                actions, values, log_probs = model.policy(state_tensor, deterministic=False)\n",
    "            action = actions.cpu().numpy() \n",
    "\n",
    "            # clip actions to avoid out of bounds error\n",
    "            # as we are sampling from an unbounded gaussian distribution\n",
    "            clipped_actions = np.clip(actions, 0, ACTION_SPACE_DIM)\n",
    "            action = clipped_actions.item()\n",
    "            next_state, _, terminated, truncated, info = env.step(action)\n",
    "            # print(np.mean(next_state))\n",
    "            step_num += 1\n",
    "            rollout_step_num += 1\n",
    "            \n",
    "            action = np.array(action).reshape(-1, 1)\n",
    "            sarstti[mgr].append((state, action, 0, next_state, terminated, truncated, info))\n",
    "            mgr_values[mgr].append(values)\n",
    "            mgr_log_probs[mgr].append(log_probs)\n",
    "        \n",
    "        n_episodes += 1\n",
    "        # compute rewards for the draft\n",
    "        for mgr in range(NUM_MGRS):\n",
    "            reward = env.calc_reward(mgr)\n",
    "            # print(f\"Reward for manager {mgr}: {reward}\")\n",
    "            state, action, _, next_state, terminated, truncated, info = sarstti[mgr][-1]\n",
    "            sarstti[mgr][-1] = (state, action, reward, next_state, terminated, truncated, info)\n",
    "        \n",
    "        # add every step of draft to rollout buffer for each manager\n",
    "        for mgr, model in enumerate(models):\n",
    "            for round in range(NUM_DRAFT_ROUNDS):\n",
    "                state, action, reward, next_state, terminated, truncated, info = sarstti[mgr][round]\n",
    "                model.rollout_buffer.add(\n",
    "                    state,\n",
    "                    action,\n",
    "                    reward,\n",
    "                    new_episode,\n",
    "                    mgr_values[mgr][round],\n",
    "                    mgr_log_probs[mgr][round]\n",
    "                )\n",
    "                new_episode = 0\n",
    "                \n",
    "        if n_episodes % n_episodes_model == 0:\n",
    "            for mgr, model in enumerate(models):\n",
    "                model_save_path = os.path.join(manager_dirs[mgr], f\"ppo_model_mgr_{mgr}_episode_{n_episodes}.zip\")\n",
    "                model.save(model_save_path)\n",
    "                print(f\"Saved model for manager {mgr} at episode {n_episodes} to {model_save_path}\")\n",
    "        \n",
    "        if n_episodes % n_episodes_info == 0:\n",
    "            for mgr in range(NUM_MGRS):\n",
    "                info_file_path = os.path.join(manager_dirs[mgr], f\"info_history_episode_{n_episodes}.json\")\n",
    "                with open(info_file_path, 'w') as f:\n",
    "                    json.dump(info_history, f, indent=4)\n",
    "                print(f\"Saved info history to {info_file_path}\")\n",
    "            # Clear info history after saving to avoid redundant data\n",
    "            info_history.clear()\n",
    "    \n",
    "\n",
    "                \n",
    "    for mgr, model in enumerate(models):\n",
    "        \n",
    "        rewards = model.rollout_buffer.rewards\n",
    "        mean_reward = rewards.mean()\n",
    "        \n",
    "        # Log metrics to TensorBoard for this specific manager\n",
    "        model.logger.record(f\"rewards/draft_reward_mgr_{mgr}\", mean_reward)\n",
    "        model.logger.dump(n_episodes)\n",
    "\n",
    "\n",
    "        # check if best reward and save model if so\n",
    "        if mean_reward > best_rewards[mgr]:\n",
    "            best_rewards[mgr] = mean_reward\n",
    "            model.save(best_model_paths[mgr])\n",
    "            print(f\"Saved best model for manager {mgr} with draft reward {mean_reward}\")    \n",
    "        \n",
    "        model.rollout_buffer.compute_returns_and_advantage(last_values=torch.zeros_like(mgr_values[mgr][-1]), dones=terminated) # might need to make dones an array, not sure\n",
    "        model.train()\n",
    "        \n",
    "        # Extract and log individual losses\n",
    "        pg_loss = np.mean(model.logger.name_to_value['train/policy_gradient_loss'])  # Policy gradient loss\n",
    "        value_loss = np.mean(model.logger.name_to_value['train/value_loss'])  # Value loss\n",
    "        entropy_loss = np.mean(model.logger.name_to_value['train/entropy_loss'])  # Entropy loss\n",
    "\n",
    "        # Calculate total loss\n",
    "        total_loss = pg_loss + model.vf_coef * value_loss + model.ent_coef * entropy_loss\n",
    "\n",
    "        # Log the individual and total losses\n",
    "        model.logger.record(f\"loss_total/total_loss_mgr_{mgr}\", total_loss)\n",
    "        model.logger.record(f\"loss_pg/pg_loss_mgr_{mgr}\", pg_loss)\n",
    "        model.logger.record(f\"loss_value/value_loss_mgr_{mgr}\", value_loss)\n",
    "        model.logger.record(f\"loss_entropy/entropy_loss_mgr_{mgr}\", entropy_loss)\n",
    "\n",
    "        # Dump logs to TensorBoard\n",
    "        model.logger.dump(n_episodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

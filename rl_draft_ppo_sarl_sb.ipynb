{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from rl_env_sarl import SARLDraftEnv\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in order to test how robust this is against managers with different ideas of player values, I should have the stochastic choice select a random projection source.\n",
    "\n",
    "\n",
    "model: PPO_20240827-124619\n",
    "- pretty good model - best so far - rewarded for first place in starters\n",
    "- I need to try turning off the KL divergence limit bc it's early stopping a lot which could prevent it from finding new strategies. \n",
    "- I also think the temperature on the random choices from other players might have been a bit too high so i will turn it down.\n",
    "\n",
    "model: logs/PPO_20240827-143658/best_model.zip\n",
    "- works very well\n",
    "\n",
    "- based on this article, https://fantasyfootballanalytics.net/2015/07/accuracy-of-fantasy-football-projections-interactive-scatterplot-in-r.html, DEF projections are not very reliable. maybe i should override rec to draft early\n",
    "    - i also know i'll need backup players of injured players\n",
    "    \n",
    "TODO:\n",
    "- need to rescrape and update projections\n",
    "- need to estimate conditional variance of each player from previous year and put a variance penalty on reward\n",
    "- make a method that offers me next best choice \n",
    "- in order to understand reliability of projections, need to look at correlation between projections and performance in previous years\n",
    "- look at whether the non penalized (def/k) algorithm shows worse performance of defense by the opponents\n",
    "- for draft interface, I need to immediately pull up plots with mean, proj std, and hist std for top few players in each position. also include bye week for bye week based decisions.k\n",
    "\n",
    "\n",
    "- prevent multiple def or k\n",
    "- give points for flex past starters\n",
    "- think about bye weeks - add to plotting\n",
    "\n",
    "\n",
    "- bots are choosing def and kicker too early. i put in a temp fix, but make it better\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class learning_rate_schedule:\n",
    "#     def __init__(self, initial_lr=1e-4, final_lr=5e-5):\n",
    "#         self.initial_lr = initial_lr\n",
    "#         self.final_lr = final_lr\n",
    "    \n",
    "#     def __str__(self):\n",
    "#         return f\"lr_{self.initial_lr}to{self.final_lr}\"\n",
    "\n",
    "#     def __call__(self, progress_remaining):\n",
    "#         return progress_remaining * (self.initial_lr - self.final_lr) + self.final_lr\n",
    "\n",
    "# class adaptive_ent_coef:\n",
    "#     def __init__(self, initial_ent_coef=0.03, final_ent_coef=0.01):\n",
    "#         self.initial_ent_coef = initial_ent_coef\n",
    "#         self.final_ent_coef = final_ent_coef\n",
    "        \n",
    "#     def __str__(self):\n",
    "#         return f\"ent_{self.initial_ent_coef}to{self.final_ent_coef}\"\n",
    "    \n",
    "#     def __call__(self, progress_remaining):\n",
    "#         return progress_remaining * (self.initial_ent_coef - self.final_ent_coef) + self.final_ent_coef\n",
    "    \n",
    "def learning_rate_schedule(initial_lr=1e-4, final_lr=5e-5):\n",
    "    return lambda progress_remaining: progress_remaining * (initial_lr - final_lr) + final_lr\n",
    "\n",
    "learning_rate_schedule_fn = learning_rate_schedule(initial_lr=1e-4, final_lr=5e-5)\n",
    "\n",
    "\n",
    "ppo_params = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"learning_rate\": learning_rate_schedule_fn,  # Adaptive learning rate\n",
    "    \"n_steps\": 750,\n",
    "    \"batch_size\": 750,\n",
    "    \"n_epochs\": 30,\n",
    "    \"gamma\": 0.99,\n",
    "    \"gae_lambda\": 0.95,\n",
    "    \"clip_range\": 0.2,\n",
    "    \"clip_range_vf\": None,\n",
    "    \"normalize_advantage\": True,\n",
    "    \"ent_coef\": .03, # Entropy coefficient for the loss calculation\n",
    "    \"vf_coef\": 0.5,\n",
    "    \"max_grad_norm\": 0.7,\n",
    "    \"use_sde\": False,\n",
    "    \"sde_sample_freq\": -1,\n",
    "    \"rollout_buffer_class\": None,\n",
    "    \"rollout_buffer_kwargs\": None,\n",
    "    # \"target_kl\": 0.01,\n",
    "    # \"target_kl\": None, # TODO: TRY THIS NEXT TEST BECAUSE KEEP GETTING EARLY STOPPING\n",
    "    \"stats_window_size\": 100,\n",
    "    \"policy_kwargs\": dict(net_arch=[dict(pi=[256, 256, 128], vf=[256, 256, 128])]),\n",
    "    \"verbose\": 1,\n",
    "    \"seed\": 69,\n",
    "    \"device\": \"auto\",\n",
    "    \"_init_setup_model\": True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env(SARLDraftEnv(stochastic_temp=.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = SARLDraftEnv()\n",
    "# model = DQN(env=env, **dqn_params)\n",
    "# model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/PPO_20240830-021915/\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward: -0.5151620507240295. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "-------------------------------------------------------------------\n",
      "| mean_reward              | -0.51516205                          |\n",
      "| params/                  |                                      |\n",
      "|    _init_setup_model     | True                                 |\n",
      "|    batch_size            | 750                                  |\n",
      "|    clip_range            | 0.2                                  |\n",
      "|    clip_range_vf         | None                                 |\n",
      "|    device                | auto                                 |\n",
      "|    ent_coef              | 0.03                                 |\n",
      "|    gae_lambda            | 0.95                                 |\n",
      "|    gamma                 | 0.99                                 |\n",
      "|    learning_rate         | <function learning_rate_schedule.... |\n",
      "|    max_grad_norm         | 0.7                                  |\n",
      "|    n_epochs              | 30                                   |\n",
      "|    n_steps               | 750                                  |\n",
      "|    normalize_advantage   | True                                 |\n",
      "|    policy                | MlpPolicy                            |\n",
      "|    policy_kwargs         | {'net_arch': [{'pi': [256, 256, 1... |\n",
      "|    rollout_buffer_class  | None                                 |\n",
      "|    rollout_buffer_kwargs | None                                 |\n",
      "|    sde_sample_freq       | -1                                   |\n",
      "|    seed                  | 69                                   |\n",
      "|    stats_window_size     | 100                                  |\n",
      "|    use_sde               | False                                |\n",
      "|    verbose               | 1                                    |\n",
      "|    vf_coef               | 0.5                                  |\n",
      "| rollout/                 |                                      |\n",
      "|    ep_len_mean           | 15                                   |\n",
      "|    ep_rew_mean           | -0.481                               |\n",
      "| time/                    |                                      |\n",
      "|    fps                   | 14                                   |\n",
      "|    iterations            | 1                                    |\n",
      "|    time_elapsed          | 51                                   |\n",
      "|    total_timesteps       | 750                                  |\n",
      "-------------------------------------------------------------------\n",
      "New best mean reward: -0.4788861572742462. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "-----------------------------------------\n",
      "| mean_reward             | -0.47888616 |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | -0.49       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 102         |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009011115 |\n",
      "|    clip_fraction        | 0.018       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.79       |\n",
      "|    explained_variance   | -5.05       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0612     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00877    |\n",
      "|    value_loss           | 0.0464      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | -0.49857408 |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | -0.504      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 152         |\n",
      "|    total_timesteps      | 2250        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014770046 |\n",
      "|    clip_fraction        | 0.0644      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.78       |\n",
      "|    explained_variance   | -0.267      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0635     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    value_loss           | 0.0258      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | -0.51058924 |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | -0.478      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 203         |\n",
      "|    total_timesteps      | 3000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013857949 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.76       |\n",
      "|    explained_variance   | -1.38       |\n",
      "|    learning_rate        | 9.99e-05    |\n",
      "|    loss                 | -0.0784     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.017      |\n",
      "|    value_loss           | 0.00562     |\n",
      "-----------------------------------------\n",
      "New best mean reward: -0.44486063718795776. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "-----------------------------------------\n",
      "| mean_reward             | -0.44486064 |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | -0.44       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 253         |\n",
      "|    total_timesteps      | 3750        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010517847 |\n",
      "|    clip_fraction        | 0.057       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.76       |\n",
      "|    explained_variance   | 0.0991      |\n",
      "|    learning_rate        | 9.99e-05    |\n",
      "|    loss                 | -0.0622     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00958    |\n",
      "|    value_loss           | 0.0204      |\n",
      "-----------------------------------------\n",
      "New best mean reward: -0.4348757565021515. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "-----------------------------------------\n",
      "| mean_reward             | -0.43487576 |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | -0.44       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 304         |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017452871 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.74       |\n",
      "|    explained_variance   | 0.0984      |\n",
      "|    learning_rate        | 9.99e-05    |\n",
      "|    loss                 | -0.0724     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0169     |\n",
      "|    value_loss           | 0.0206      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=5000, episode_reward=-0.55 +/- 0.00\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | -0.547      |\n",
      "| mean_reward             | -0.44621027 |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016182184 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.69       |\n",
      "|    explained_variance   | -0.283      |\n",
      "|    learning_rate        | 9.99e-05    |\n",
      "|    loss                 | -0.0727     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0154     |\n",
      "|    value_loss           | 0.0146      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | -0.406   |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 360      |\n",
      "|    total_timesteps | 5250     |\n",
      "---------------------------------\n",
      "New best mean reward: -0.36119818687438965. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "-----------------------------------------\n",
      "| mean_reward             | -0.3611982  |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | -0.384      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 411         |\n",
      "|    total_timesteps      | 6000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010740951 |\n",
      "|    clip_fraction        | 0.0386      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.64       |\n",
      "|    explained_variance   | -0.0283     |\n",
      "|    learning_rate        | 9.99e-05    |\n",
      "|    loss                 | -0.0578     |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    value_loss           | 0.0327      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | -0.38725722 |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | -0.377      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 462         |\n",
      "|    total_timesteps      | 6750        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015787069 |\n",
      "|    clip_fraction        | 0.0536      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.0627     |\n",
      "|    learning_rate        | 9.99e-05    |\n",
      "|    loss                 | -0.0647     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    value_loss           | 0.0161      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | -0.36871734 |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | -0.378      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 513         |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010954394 |\n",
      "|    clip_fraction        | 0.0684      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.0141      |\n",
      "|    learning_rate        | 9.98e-05    |\n",
      "|    loss                 | -0.0465     |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0128     |\n",
      "|    value_loss           | 0.0521      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| mean_reward             | -0.40322846  |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15           |\n",
      "|    ep_rew_mean          | -0.365       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 564          |\n",
      "|    total_timesteps      | 8250         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024713736 |\n",
      "|    clip_fraction        | 0.0141       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.54        |\n",
      "|    explained_variance   | -0.524       |\n",
      "|    learning_rate        | 9.98e-05     |\n",
      "|    loss                 | -0.0517      |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.00638     |\n",
      "|    value_loss           | 0.0197       |\n",
      "------------------------------------------\n",
      "New best mean reward: -0.30604302883148193. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "-----------------------------------------\n",
      "| mean_reward             | -0.30604303 |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | -0.321      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 614         |\n",
      "|    total_timesteps      | 9000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.01484526  |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.0808      |\n",
      "|    learning_rate        | 9.98e-05    |\n",
      "|    loss                 | -0.0496     |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    value_loss           | 0.0435      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | -0.33911464 |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | -0.344      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 665         |\n",
      "|    total_timesteps      | 9750        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.01384552  |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | -0.0172     |\n",
      "|    learning_rate        | 9.98e-05    |\n",
      "|    loss                 | -0.0464     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    value_loss           | 0.0576      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=-0.56 +/- 0.01\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | -0.562      |\n",
      "| mean_reward             | -0.3687863  |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016595716 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | -0.029      |\n",
      "|    learning_rate        | 9.98e-05    |\n",
      "|    loss                 | -0.0655     |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    value_loss           | 0.0288      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | -0.318   |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 720      |\n",
      "|    total_timesteps | 10500    |\n",
      "---------------------------------\n",
      "New best mean reward: -0.24833303689956665. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "-----------------------------------------\n",
      "| mean_reward             | -0.24833304 |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | -0.27       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 769         |\n",
      "|    total_timesteps      | 11250       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.01254159  |\n",
      "|    clip_fraction        | 0.0696      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | -0.081      |\n",
      "|    learning_rate        | 9.97e-05    |\n",
      "|    loss                 | -0.0385     |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    value_loss           | 0.0776      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | -0.29132456 |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | -0.273      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 819         |\n",
      "|    total_timesteps      | 12000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010449104 |\n",
      "|    clip_fraction        | 0.076       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.0169      |\n",
      "|    learning_rate        | 9.97e-05    |\n",
      "|    loss                 | -0.0458     |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 0.0631      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| mean_reward             | -0.27369845  |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15           |\n",
      "|    ep_rew_mean          | -0.246       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 869          |\n",
      "|    total_timesteps      | 12750        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062454534 |\n",
      "|    clip_fraction        | 0.0408       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.57        |\n",
      "|    explained_variance   | -0.0289      |\n",
      "|    learning_rate        | 9.97e-05     |\n",
      "|    loss                 | -0.0252      |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.0104      |\n",
      "|    value_loss           | 0.0948       |\n",
      "------------------------------------------\n",
      "New best mean reward: -0.22070972621440887. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "-----------------------------------------\n",
      "| mean_reward             | -0.22070973 |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | -0.233      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 919         |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009666934 |\n",
      "|    clip_fraction        | 0.0442      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | -0.156      |\n",
      "|    learning_rate        | 9.97e-05    |\n",
      "|    loss                 | -0.027      |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    value_loss           | 0.101       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | -0.23041448 |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | -0.279      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 969         |\n",
      "|    total_timesteps      | 14250       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007746291 |\n",
      "|    clip_fraction        | 0.0277      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | -0.0667     |\n",
      "|    learning_rate        | 9.97e-05    |\n",
      "|    loss                 | -0.0344     |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.00977    |\n",
      "|    value_loss           | 0.072       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=15000, episode_reward=-0.48 +/- 0.01\n",
      "Episode length: 15.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 15           |\n",
      "|    mean_reward          | -0.476       |\n",
      "| mean_reward             | -0.3113859   |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 15000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0117043555 |\n",
      "|    clip_fraction        | 0.129        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.57        |\n",
      "|    explained_variance   | 0.0672       |\n",
      "|    learning_rate        | 9.96e-05     |\n",
      "|    loss                 | -0.0521      |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.0173      |\n",
      "|    value_loss           | 0.0573       |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15       |\n",
      "|    ep_rew_mean     | -0.293   |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 1023     |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | -0.29067323 |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | -0.252      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 1073        |\n",
      "|    total_timesteps      | 15750       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007883333 |\n",
      "|    clip_fraction        | 0.0434      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | -0.252      |\n",
      "|    learning_rate        | 9.96e-05    |\n",
      "|    loss                 | -0.0451     |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    value_loss           | 0.0603      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | -0.227693   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | -0.244      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 1122        |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004735774 |\n",
      "|    clip_fraction        | 0.0332      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | -0.169      |\n",
      "|    learning_rate        | 9.96e-05    |\n",
      "|    loss                 | -0.0391     |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    value_loss           | 0.0712      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| mean_reward             | -0.25891322  |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15           |\n",
      "|    ep_rew_mean          | -0.251       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 1172         |\n",
      "|    total_timesteps      | 17250        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0086206915 |\n",
      "|    clip_fraction        | 0.0491       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.52        |\n",
      "|    explained_variance   | 0.000153     |\n",
      "|    learning_rate        | 9.96e-05     |\n",
      "|    loss                 | -0.0506      |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.0155      |\n",
      "|    value_loss           | 0.0574       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| mean_reward             | -0.24246712  |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15           |\n",
      "|    ep_rew_mean          | -0.253       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 1221         |\n",
      "|    total_timesteps      | 18000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056255506 |\n",
      "|    clip_fraction        | 0.032        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.5         |\n",
      "|    explained_variance   | -0.0234      |\n",
      "|    learning_rate        | 9.96e-05     |\n",
      "|    loss                 | -0.0349      |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | -0.0143      |\n",
      "|    value_loss           | 0.0877       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | -0.2648213  |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | -0.234      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 1271        |\n",
      "|    total_timesteps      | 18750       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009779877 |\n",
      "|    clip_fraction        | 0.0433      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | -0.194      |\n",
      "|    learning_rate        | 9.96e-05    |\n",
      "|    loss                 | -0.0457     |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    value_loss           | 0.0646      |\n",
      "-----------------------------------------\n",
      "New best mean reward: -0.2038188874721527. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "------------------------------------------\n",
      "| mean_reward             | -0.20381889  |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15           |\n",
      "|    ep_rew_mean          | -0.187       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 1321         |\n",
      "|    total_timesteps      | 19500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066520963 |\n",
      "|    clip_fraction        | 0.0671       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.51        |\n",
      "|    explained_variance   | 0.0521       |\n",
      "|    learning_rate        | 9.95e-05     |\n",
      "|    loss                 | -0.0354      |\n",
      "|    n_updates            | 750          |\n",
      "|    policy_gradient_loss | -0.0155      |\n",
      "|    value_loss           | 0.086        |\n",
      "------------------------------------------\n",
      "New best mean reward: -0.17017501592636108. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=20000, episode_reward=-0.65 +/- 0.04\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | -0.648      |\n",
      "| mean_reward             | -0.17017502 |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011376864 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | -0.0819     |\n",
      "|    learning_rate        | 9.95e-05    |\n",
      "|    loss                 | -0.0348     |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0212     |\n",
      "|    value_loss           | 0.104       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | -0.244   |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 1375     |\n",
      "|    total_timesteps | 20250    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | -0.2828802  |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | -0.196      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 1425        |\n",
      "|    total_timesteps      | 21000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005408388 |\n",
      "|    clip_fraction        | 0.0244      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | -0.321      |\n",
      "|    learning_rate        | 9.95e-05    |\n",
      "|    loss                 | -0.0509     |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.014      |\n",
      "|    value_loss           | 0.057       |\n",
      "-----------------------------------------\n",
      "New best mean reward: -0.09432831406593323. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "------------------------------------------\n",
      "| mean_reward             | -0.094328314 |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15           |\n",
      "|    ep_rew_mean          | -0.12        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 1474         |\n",
      "|    total_timesteps      | 21750        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.010259752  |\n",
      "|    clip_fraction        | 0.0993       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.52        |\n",
      "|    explained_variance   | -0.094       |\n",
      "|    learning_rate        | 9.95e-05     |\n",
      "|    loss                 | -0.0287      |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.0194      |\n",
      "|    value_loss           | 0.12         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | -0.1817903  |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | -0.136      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 1524        |\n",
      "|    total_timesteps      | 22500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013036245 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | -0.109      |\n",
      "|    learning_rate        | 9.95e-05    |\n",
      "|    loss                 | -0.0476     |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0243     |\n",
      "|    value_loss           | 0.0881      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | -0.1032304  |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | -0.163      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 1573        |\n",
      "|    total_timesteps      | 23250       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013805613 |\n",
      "|    clip_fraction        | 0.0994      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | -0.0245     |\n",
      "|    learning_rate        | 9.94e-05    |\n",
      "|    loss                 | -0.0379     |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    value_loss           | 0.089       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | -0.18153204 |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | -0.113      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 1623        |\n",
      "|    total_timesteps      | 24000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015179114 |\n",
      "|    clip_fraction        | 0.0691      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | -0.29       |\n",
      "|    learning_rate        | 9.94e-05    |\n",
      "|    loss                 | -0.0457     |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.019      |\n",
      "|    value_loss           | 0.0817      |\n",
      "-----------------------------------------\n",
      "New best mean reward: -0.010472836904227734. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "------------------------------------------\n",
      "| mean_reward             | -0.010472837 |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15           |\n",
      "|    ep_rew_mean          | -0.022       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 1672         |\n",
      "|    total_timesteps      | 24750        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.011501658  |\n",
      "|    clip_fraction        | 0.0818       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | -0.00456     |\n",
      "|    learning_rate        | 9.94e-05     |\n",
      "|    loss                 | -0.0294      |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.0213      |\n",
      "|    value_loss           | 0.119        |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=25000, episode_reward=0.80 +/- 0.37\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 0.803       |\n",
      "| mean_reward             | -0.03658697 |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012159216 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.0595      |\n",
      "|    learning_rate        | 9.94e-05    |\n",
      "|    loss                 | -0.0278     |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0225     |\n",
      "|    value_loss           | 0.113       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | -0.037   |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 1727     |\n",
      "|    total_timesteps | 25500    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | -0.0357987  |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 0.00322     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 1777        |\n",
      "|    total_timesteps      | 26250       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013610498 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.0185      |\n",
      "|    learning_rate        | 9.94e-05    |\n",
      "|    loss                 | -0.019      |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0243     |\n",
      "|    value_loss           | 0.139       |\n",
      "-----------------------------------------\n",
      "New best mean reward: 0.053242236375808716. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "-----------------------------------------\n",
      "| mean_reward             | 0.053242236 |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 0.0937      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 1826        |\n",
      "|    total_timesteps      | 27000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010902284 |\n",
      "|    clip_fraction        | 0.0743      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | -0.0647     |\n",
      "|    learning_rate        | 9.93e-05    |\n",
      "|    loss                 | -0.0344     |\n",
      "|    n_updates            | 1050        |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    value_loss           | 0.115       |\n",
      "-----------------------------------------\n",
      "New best mean reward: 0.13585315644741058. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "-----------------------------------------\n",
      "| mean_reward             | 0.13585316  |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 0.199       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 1876        |\n",
      "|    total_timesteps      | 27750       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011230957 |\n",
      "|    clip_fraction        | 0.0932      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.00105     |\n",
      "|    learning_rate        | 9.93e-05    |\n",
      "|    loss                 | -0.0245     |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n",
      "New best mean reward: 0.26513248682022095. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "-----------------------------------------\n",
      "| mean_reward             | 0.2651325   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 0.358       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 1926        |\n",
      "|    total_timesteps      | 28500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012903028 |\n",
      "|    clip_fraction        | 0.0786      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | -0.0442     |\n",
      "|    learning_rate        | 9.93e-05    |\n",
      "|    loss                 | -0.0169     |\n",
      "|    n_updates            | 1110        |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    value_loss           | 0.157       |\n",
      "-----------------------------------------\n",
      "New best mean reward: 0.4453781843185425. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "-----------------------------------------\n",
      "| mean_reward             | 0.44537818  |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 0.373       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 1976        |\n",
      "|    total_timesteps      | 29250       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013745523 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | -0.0563     |\n",
      "|    learning_rate        | 9.93e-05    |\n",
      "|    loss                 | -0.00787    |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0216     |\n",
      "|    value_loss           | 0.169       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=30000, episode_reward=0.58 +/- 0.61\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 0.585       |\n",
      "| mean_reward             | 0.3070122   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010027328 |\n",
      "|    clip_fraction        | 0.0892      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | -0.0692     |\n",
      "|    learning_rate        | 9.93e-05    |\n",
      "|    loss                 | -0.00822    |\n",
      "|    n_updates            | 1170        |\n",
      "|    policy_gradient_loss | -0.0245     |\n",
      "|    value_loss           | 0.171       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15       |\n",
      "|    ep_rew_mean     | 0.355    |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 2030     |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 0.42536902  |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 0.342       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 2080        |\n",
      "|    total_timesteps      | 30750       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017127039 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.0701      |\n",
      "|    learning_rate        | 9.93e-05    |\n",
      "|    loss                 | -0.0113     |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    value_loss           | 0.167       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 0.29997456  |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 0.364       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 2130        |\n",
      "|    total_timesteps      | 31500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013209493 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | -0.12       |\n",
      "|    learning_rate        | 9.92e-05    |\n",
      "|    loss                 | 0.0109      |\n",
      "|    n_updates            | 1230        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    value_loss           | 0.237       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 0.40733692  |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 0.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 2180        |\n",
      "|    total_timesteps      | 32250       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012565814 |\n",
      "|    clip_fraction        | 0.0712      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | -0.0323     |\n",
      "|    learning_rate        | 9.92e-05    |\n",
      "|    loss                 | 0.000695    |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.023      |\n",
      "|    value_loss           | 0.186       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 0.39212245  |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 0.399       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 2229        |\n",
      "|    total_timesteps      | 33000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011050569 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.0641      |\n",
      "|    learning_rate        | 9.92e-05    |\n",
      "|    loss                 | -0.0104     |\n",
      "|    n_updates            | 1290        |\n",
      "|    policy_gradient_loss | -0.0248     |\n",
      "|    value_loss           | 0.162       |\n",
      "-----------------------------------------\n",
      "New best mean reward: 0.4456127882003784. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "-----------------------------------------\n",
      "| mean_reward             | 0.4456128   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 0.453       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 2279        |\n",
      "|    total_timesteps      | 33750       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013265663 |\n",
      "|    clip_fraction        | 0.0876      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.0803      |\n",
      "|    learning_rate        | 9.92e-05    |\n",
      "|    loss                 | -0.0228     |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    value_loss           | 0.134       |\n",
      "-----------------------------------------\n",
      "New best mean reward: 0.48520252108573914. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "-----------------------------------------\n",
      "| mean_reward             | 0.48520252  |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 0.524       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 2329        |\n",
      "|    total_timesteps      | 34500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014041089 |\n",
      "|    clip_fraction        | 0.089       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | -0.00809    |\n",
      "|    learning_rate        | 9.92e-05    |\n",
      "|    loss                 | 0.0142      |\n",
      "|    n_updates            | 1350        |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    value_loss           | 0.226       |\n",
      "-----------------------------------------\n",
      "New best mean reward: 0.53938889503479. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=35000, episode_reward=1.36 +/- 0.43\n",
      "Episode length: 15.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 15         |\n",
      "|    mean_reward          | 1.36       |\n",
      "| mean_reward             | 0.5393889  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 35000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01054864 |\n",
      "|    clip_fraction        | 0.062      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.35      |\n",
      "|    explained_variance   | 0.0507     |\n",
      "|    learning_rate        | 9.91e-05   |\n",
      "|    loss                 | 0.00661    |\n",
      "|    n_updates            | 1380       |\n",
      "|    policy_gradient_loss | -0.0266    |\n",
      "|    value_loss           | 0.213      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 0.505    |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 2384     |\n",
      "|    total_timesteps | 35250    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 0.42799872  |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 0.52        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 2434        |\n",
      "|    total_timesteps      | 36000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011130118 |\n",
      "|    clip_fraction        | 0.0732      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | -0.0783     |\n",
      "|    learning_rate        | 9.91e-05    |\n",
      "|    loss                 | -0.0122     |\n",
      "|    n_updates            | 1410        |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    value_loss           | 0.165       |\n",
      "-----------------------------------------\n",
      "New best mean reward: 0.6368305683135986. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "-----------------------------------------\n",
      "| mean_reward             | 0.63683057  |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 0.623       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 2484        |\n",
      "|    total_timesteps      | 36750       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010703254 |\n",
      "|    clip_fraction        | 0.0453      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.0584      |\n",
      "|    learning_rate        | 9.91e-05    |\n",
      "|    loss                 | -0.0234     |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0216     |\n",
      "|    value_loss           | 0.133       |\n",
      "-----------------------------------------\n",
      "New best mean reward: 0.6671940684318542. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "----------------------------------------\n",
      "| mean_reward             | 0.66719407 |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15         |\n",
      "|    ep_rew_mean          | 0.657      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 50         |\n",
      "|    time_elapsed         | 2533       |\n",
      "|    total_timesteps      | 37500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01022849 |\n",
      "|    clip_fraction        | 0.0861     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.28      |\n",
      "|    explained_variance   | 0.097      |\n",
      "|    learning_rate        | 9.91e-05   |\n",
      "|    loss                 | 0.007      |\n",
      "|    n_updates            | 1470       |\n",
      "|    policy_gradient_loss | -0.0253    |\n",
      "|    value_loss           | 0.21       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 0.6634063  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15         |\n",
      "|    ep_rew_mean          | 0.705      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 51         |\n",
      "|    time_elapsed         | 2583       |\n",
      "|    total_timesteps      | 38250      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01175911 |\n",
      "|    clip_fraction        | 0.0895     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.25      |\n",
      "|    explained_variance   | -0.123     |\n",
      "|    learning_rate        | 9.91e-05   |\n",
      "|    loss                 | -0.0108    |\n",
      "|    n_updates            | 1500       |\n",
      "|    policy_gradient_loss | -0.026     |\n",
      "|    value_loss           | 0.169      |\n",
      "----------------------------------------\n",
      "New best mean reward: 0.7084660530090332. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "-----------------------------------------\n",
      "| mean_reward             | 0.70846605  |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 2633        |\n",
      "|    total_timesteps      | 39000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013006976 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 9.9e-05     |\n",
      "|    loss                 | 0.00713     |\n",
      "|    n_updates            | 1530        |\n",
      "|    policy_gradient_loss | -0.0332     |\n",
      "|    value_loss           | 0.225       |\n",
      "-----------------------------------------\n",
      "New best mean reward: 0.7257538437843323. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "-----------------------------------------\n",
      "| mean_reward             | 0.72575384  |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 2683        |\n",
      "|    total_timesteps      | 39750       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011204121 |\n",
      "|    clip_fraction        | 0.0876      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.0476      |\n",
      "|    learning_rate        | 9.9e-05     |\n",
      "|    loss                 | 0.00539     |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    value_loss           | 0.187       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=40000, episode_reward=1.59 +/- 0.03\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.59        |\n",
      "| mean_reward             | 0.71006423  |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009286648 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.00449     |\n",
      "|    learning_rate        | 9.9e-05     |\n",
      "|    loss                 | -0.0176     |\n",
      "|    n_updates            | 1590        |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    value_loss           | 0.163       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 0.712    |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 2737     |\n",
      "|    total_timesteps | 40500    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 0.63708     |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 2787        |\n",
      "|    total_timesteps      | 41250       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011946194 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.0418      |\n",
      "|    learning_rate        | 9.9e-05     |\n",
      "|    loss                 | -0.0104     |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    value_loss           | 0.176       |\n",
      "-----------------------------------------\n",
      "New best mean reward: 1.0342566967010498. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.0342567   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 0.898       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 2837        |\n",
      "|    total_timesteps      | 42000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008452384 |\n",
      "|    clip_fraction        | 0.0443      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.0251      |\n",
      "|    learning_rate        | 9.9e-05     |\n",
      "|    loss                 | 0.00498     |\n",
      "|    n_updates            | 1650        |\n",
      "|    policy_gradient_loss | -0.0175     |\n",
      "|    value_loss           | 0.176       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 0.8002823   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 0.901       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 2886        |\n",
      "|    total_timesteps      | 42750       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012179538 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.0385      |\n",
      "|    learning_rate        | 9.9e-05     |\n",
      "|    loss                 | 0.00477     |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    value_loss           | 0.215       |\n",
      "-----------------------------------------\n",
      "New best mean reward: 1.034990906715393. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "------------------------------------------\n",
      "| mean_reward             | 1.0349909    |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15           |\n",
      "|    ep_rew_mean          | 1.04         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 2936         |\n",
      "|    total_timesteps      | 43500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0084704105 |\n",
      "|    clip_fraction        | 0.086        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | 0.0521       |\n",
      "|    learning_rate        | 9.89e-05     |\n",
      "|    loss                 | -0.0054      |\n",
      "|    n_updates            | 1710         |\n",
      "|    policy_gradient_loss | -0.0236      |\n",
      "|    value_loss           | 0.162        |\n",
      "------------------------------------------\n",
      "New best mean reward: 1.056597113609314. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.0565971   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.04        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 2986        |\n",
      "|    total_timesteps      | 44250       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009610641 |\n",
      "|    clip_fraction        | 0.0909      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.0924      |\n",
      "|    learning_rate        | 9.89e-05    |\n",
      "|    loss                 | -0.0266     |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=45000, episode_reward=1.56 +/- 0.03\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.56        |\n",
      "| mean_reward             | 1.0287206   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 45000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010671355 |\n",
      "|    clip_fraction        | 0.0791      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.0353      |\n",
      "|    learning_rate        | 9.89e-05    |\n",
      "|    loss                 | -0.00503    |\n",
      "|    n_updates            | 1770        |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    value_loss           | 0.169       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15       |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 3041     |\n",
      "|    total_timesteps | 45000    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 0.93784434  |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.04        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 3090        |\n",
      "|    total_timesteps      | 45750       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010921918 |\n",
      "|    clip_fraction        | 0.095       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.0832      |\n",
      "|    learning_rate        | 9.89e-05    |\n",
      "|    loss                 | 0.0115      |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | -0.0243     |\n",
      "|    value_loss           | 0.191       |\n",
      "-----------------------------------------\n",
      "New best mean reward: 1.1781392097473145. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.1781392   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.17        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 3140        |\n",
      "|    total_timesteps      | 46500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010448618 |\n",
      "|    clip_fraction        | 0.0901      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.00966     |\n",
      "|    learning_rate        | 9.89e-05    |\n",
      "|    loss                 | -0.0123     |\n",
      "|    n_updates            | 1830        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    value_loss           | 0.148       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| mean_reward             | 1.1438373    |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15           |\n",
      "|    ep_rew_mean          | 1.14         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 63           |\n",
      "|    time_elapsed         | 3190         |\n",
      "|    total_timesteps      | 47250        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0095361555 |\n",
      "|    clip_fraction        | 0.111        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.0893       |\n",
      "|    learning_rate        | 9.88e-05     |\n",
      "|    loss                 | -0.0144      |\n",
      "|    n_updates            | 1860         |\n",
      "|    policy_gradient_loss | -0.0265      |\n",
      "|    value_loss           | 0.138        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.108772    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.16        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 3240        |\n",
      "|    total_timesteps      | 48000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011984916 |\n",
      "|    clip_fraction        | 0.0969      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.0313      |\n",
      "|    learning_rate        | 9.88e-05    |\n",
      "|    loss                 | 0.00811     |\n",
      "|    n_updates            | 1890        |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    value_loss           | 0.208       |\n",
      "-----------------------------------------\n",
      "New best mean reward: 1.2245455980300903. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2245456   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.27        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 3289        |\n",
      "|    total_timesteps      | 48750       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010607262 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | -0.0916     |\n",
      "|    learning_rate        | 9.88e-05    |\n",
      "|    loss                 | -0.00335    |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.022      |\n",
      "|    value_loss           | 0.158       |\n",
      "-----------------------------------------\n",
      "New best mean reward: 1.2904475927352905. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "------------------------------------------\n",
      "| mean_reward             | 1.2904476    |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15           |\n",
      "|    ep_rew_mean          | 1.27         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 66           |\n",
      "|    time_elapsed         | 3339         |\n",
      "|    total_timesteps      | 49500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0128150135 |\n",
      "|    clip_fraction        | 0.155        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.131        |\n",
      "|    learning_rate        | 9.88e-05     |\n",
      "|    loss                 | -0.0288      |\n",
      "|    n_updates            | 1950         |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    value_loss           | 0.118        |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=50000, episode_reward=1.54 +/- 0.02\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.54        |\n",
      "| mean_reward             | 1.278904    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008696487 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | -0.209      |\n",
      "|    learning_rate        | 9.88e-05    |\n",
      "|    loss                 | -0.0241     |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | -0.0228     |\n",
      "|    value_loss           | 0.117       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.1      |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 67       |\n",
      "|    time_elapsed    | 3394     |\n",
      "|    total_timesteps | 50250    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 0.92875993  |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.11        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 3444        |\n",
      "|    total_timesteps      | 51000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011353963 |\n",
      "|    clip_fraction        | 0.0787      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | -0.0478     |\n",
      "|    learning_rate        | 9.87e-05    |\n",
      "|    loss                 | 0.0524      |\n",
      "|    n_updates            | 2010        |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    value_loss           | 0.296       |\n",
      "-----------------------------------------\n",
      "New best mean reward: 1.3084526062011719. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.3084526  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15         |\n",
      "|    ep_rew_mean          | 1.26       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 69         |\n",
      "|    time_elapsed         | 3494       |\n",
      "|    total_timesteps      | 51750      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00825763 |\n",
      "|    clip_fraction        | 0.0741     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.05      |\n",
      "|    explained_variance   | -0.21      |\n",
      "|    learning_rate        | 9.87e-05   |\n",
      "|    loss                 | -0.0085    |\n",
      "|    n_updates            | 2040       |\n",
      "|    policy_gradient_loss | -0.0158    |\n",
      "|    value_loss           | 0.135      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2589393   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.26        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 3543        |\n",
      "|    total_timesteps      | 52500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014954095 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.0985      |\n",
      "|    learning_rate        | 9.87e-05    |\n",
      "|    loss                 | -0.0156     |\n",
      "|    n_updates            | 2070        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    value_loss           | 0.14        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2103199   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.25        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 3593        |\n",
      "|    total_timesteps      | 53250       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007979286 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.0163      |\n",
      "|    learning_rate        | 9.87e-05    |\n",
      "|    loss                 | -0.0122     |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.0225     |\n",
      "|    value_loss           | 0.133       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2830603   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.24        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 3643        |\n",
      "|    total_timesteps      | 54000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010570928 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.0743      |\n",
      "|    learning_rate        | 9.87e-05    |\n",
      "|    loss                 | -0.0138     |\n",
      "|    n_updates            | 2130        |\n",
      "|    policy_gradient_loss | -0.0245     |\n",
      "|    value_loss           | 0.136       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| mean_reward             | 1.2349567    |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15           |\n",
      "|    ep_rew_mean          | 1.27         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 73           |\n",
      "|    time_elapsed         | 3693         |\n",
      "|    total_timesteps      | 54750        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0103683565 |\n",
      "|    clip_fraction        | 0.0974       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.01        |\n",
      "|    explained_variance   | -0.193       |\n",
      "|    learning_rate        | 9.87e-05     |\n",
      "|    loss                 | -0.0317      |\n",
      "|    n_updates            | 2160         |\n",
      "|    policy_gradient_loss | -0.0258      |\n",
      "|    value_loss           | 0.106        |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=55000, episode_reward=1.35 +/- 0.42\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.35        |\n",
      "| mean_reward             | 1.301578    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 55000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015001429 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | -0.0228     |\n",
      "|    learning_rate        | 9.86e-05    |\n",
      "|    loss                 | -0.0298     |\n",
      "|    n_updates            | 2190        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    value_loss           | 0.12        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.28     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 3747     |\n",
      "|    total_timesteps | 55500    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2627794   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.21        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 3797        |\n",
      "|    total_timesteps      | 56250       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011308002 |\n",
      "|    clip_fraction        | 0.0924      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.026       |\n",
      "|    learning_rate        | 9.86e-05    |\n",
      "|    loss                 | -0.00812    |\n",
      "|    n_updates            | 2220        |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    value_loss           | 0.137       |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| mean_reward             | 1.1838045 |\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 15        |\n",
      "|    ep_rew_mean          | 1.17      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 14        |\n",
      "|    iterations           | 76        |\n",
      "|    time_elapsed         | 3847      |\n",
      "|    total_timesteps      | 57000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0089478 |\n",
      "|    clip_fraction        | 0.0792    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.02     |\n",
      "|    explained_variance   | 0.0634    |\n",
      "|    learning_rate        | 9.86e-05  |\n",
      "|    loss                 | 0.00342   |\n",
      "|    n_updates            | 2250      |\n",
      "|    policy_gradient_loss | -0.0242   |\n",
      "|    value_loss           | 0.174     |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.104352    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.19        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 3897        |\n",
      "|    total_timesteps      | 57750       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011261247 |\n",
      "|    clip_fraction        | 0.0895      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | -0.0462     |\n",
      "|    learning_rate        | 9.86e-05    |\n",
      "|    loss                 | -0.0245     |\n",
      "|    n_updates            | 2280        |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2328609   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.24        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 3946        |\n",
      "|    total_timesteps      | 58500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015494583 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | -0.0958     |\n",
      "|    learning_rate        | 9.86e-05    |\n",
      "|    loss                 | -0.0231     |\n",
      "|    n_updates            | 2310        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    value_loss           | 0.135       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2795157   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.32        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 3996        |\n",
      "|    total_timesteps      | 59250       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011466381 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.996      |\n",
      "|    explained_variance   | -0.00625    |\n",
      "|    learning_rate        | 9.85e-05    |\n",
      "|    loss                 | -0.0233     |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    value_loss           | 0.122       |\n",
      "-----------------------------------------\n",
      "New best mean reward: 1.361055612564087. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=60000, episode_reward=1.57 +/- 0.03\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.57        |\n",
      "| mean_reward             | 1.3610556   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012301587 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.994      |\n",
      "|    explained_variance   | -0.217      |\n",
      "|    learning_rate        | 9.85e-05    |\n",
      "|    loss                 | -0.0457     |\n",
      "|    n_updates            | 2370        |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    value_loss           | 0.0772      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15       |\n",
      "|    ep_rew_mean     | 1.35     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 80       |\n",
      "|    time_elapsed    | 4051     |\n",
      "|    total_timesteps | 60000    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3217183   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.3         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 4101        |\n",
      "|    total_timesteps      | 60750       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011172673 |\n",
      "|    clip_fraction        | 0.0906      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.982      |\n",
      "|    explained_variance   | -0.162      |\n",
      "|    learning_rate        | 9.85e-05    |\n",
      "|    loss                 | -0.0299     |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.0219     |\n",
      "|    value_loss           | 0.0878      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3280276   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.36        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 4151        |\n",
      "|    total_timesteps      | 61500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010214089 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.956      |\n",
      "|    explained_variance   | 0.0393      |\n",
      "|    learning_rate        | 9.85e-05    |\n",
      "|    loss                 | -0.0241     |\n",
      "|    n_updates            | 2430        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    value_loss           | 0.118       |\n",
      "-----------------------------------------\n",
      "New best mean reward: 1.4481287002563477. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4481287   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 4200        |\n",
      "|    total_timesteps      | 62250       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011377286 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.956      |\n",
      "|    explained_variance   | 0.0163      |\n",
      "|    learning_rate        | 9.85e-05    |\n",
      "|    loss                 | -0.0413     |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.0245     |\n",
      "|    value_loss           | 0.0677      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4005322   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 4250        |\n",
      "|    total_timesteps      | 63000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017288072 |\n",
      "|    clip_fraction        | 0.0947      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.958      |\n",
      "|    explained_variance   | 0.0458      |\n",
      "|    learning_rate        | 9.84e-05    |\n",
      "|    loss                 | -0.0521     |\n",
      "|    n_updates            | 2490        |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    value_loss           | 0.0503      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3853459   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.36        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 4300        |\n",
      "|    total_timesteps      | 63750       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013633237 |\n",
      "|    clip_fraction        | 0.0854      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.982      |\n",
      "|    explained_variance   | 0.0801      |\n",
      "|    learning_rate        | 9.84e-05    |\n",
      "|    loss                 | -0.0297     |\n",
      "|    n_updates            | 2520        |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    value_loss           | 0.0961      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3480395   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.36        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 4350        |\n",
      "|    total_timesteps      | 64500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013925907 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.98       |\n",
      "|    explained_variance   | 0.0779      |\n",
      "|    learning_rate        | 9.84e-05    |\n",
      "|    loss                 | -0.0301     |\n",
      "|    n_updates            | 2550        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    value_loss           | 0.103       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=65000, episode_reward=1.55 +/- 0.05\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.55        |\n",
      "| mean_reward             | 1.3280863   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 65000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019828457 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.981      |\n",
      "|    explained_variance   | 0.00942     |\n",
      "|    learning_rate        | 9.84e-05    |\n",
      "|    loss                 | -0.0134     |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    value_loss           | 0.131       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.32     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 87       |\n",
      "|    time_elapsed    | 4405     |\n",
      "|    total_timesteps | 65250    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3431398   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.3         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 4455        |\n",
      "|    total_timesteps      | 66000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013983595 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.967      |\n",
      "|    explained_variance   | 0.00875     |\n",
      "|    learning_rate        | 9.84e-05    |\n",
      "|    loss                 | -0.0367     |\n",
      "|    n_updates            | 2610        |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    value_loss           | 0.0879      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2830591   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.35        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 4505        |\n",
      "|    total_timesteps      | 66750       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010427512 |\n",
      "|    clip_fraction        | 0.098       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.98       |\n",
      "|    explained_variance   | 0.0293      |\n",
      "|    learning_rate        | 9.84e-05    |\n",
      "|    loss                 | -0.0219     |\n",
      "|    n_updates            | 2640        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    value_loss           | 0.119       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| mean_reward             | 1.4064727    |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15           |\n",
      "|    ep_rew_mean          | 1.4          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 90           |\n",
      "|    time_elapsed         | 4555         |\n",
      "|    total_timesteps      | 67500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0091240695 |\n",
      "|    clip_fraction        | 0.0815       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.962       |\n",
      "|    explained_variance   | -0.0635      |\n",
      "|    learning_rate        | 9.83e-05     |\n",
      "|    loss                 | -0.0455      |\n",
      "|    n_updates            | 2670         |\n",
      "|    policy_gradient_loss | -0.0264      |\n",
      "|    value_loss           | 0.0742       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4103453   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 4604        |\n",
      "|    total_timesteps      | 68250       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011984613 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.976      |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 9.83e-05    |\n",
      "|    loss                 | -0.0282     |\n",
      "|    n_updates            | 2700        |\n",
      "|    policy_gradient_loss | -0.0236     |\n",
      "|    value_loss           | 0.0898      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4261944   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 4654        |\n",
      "|    total_timesteps      | 69000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009798338 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.976      |\n",
      "|    explained_variance   | -0.169      |\n",
      "|    learning_rate        | 9.83e-05    |\n",
      "|    loss                 | -0.0396     |\n",
      "|    n_updates            | 2730        |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    value_loss           | 0.0709      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3360709   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 4704        |\n",
      "|    total_timesteps      | 69750       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016585398 |\n",
      "|    clip_fraction        | 0.0971      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.967      |\n",
      "|    explained_variance   | -0.0634     |\n",
      "|    learning_rate        | 9.83e-05    |\n",
      "|    loss                 | -0.0178     |\n",
      "|    n_updates            | 2760        |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    value_loss           | 0.117       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=70000, episode_reward=1.60 +/- 0.01\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.6         |\n",
      "| mean_reward             | 1.2670211   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 70000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017888805 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.969      |\n",
      "|    explained_variance   | -0.00114    |\n",
      "|    learning_rate        | 9.83e-05    |\n",
      "|    loss                 | -0.0267     |\n",
      "|    n_updates            | 2790        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    value_loss           | 0.107       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.25     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 94       |\n",
      "|    time_elapsed    | 4759     |\n",
      "|    total_timesteps | 70500    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2653698   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.32        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 4809        |\n",
      "|    total_timesteps      | 71250       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011056499 |\n",
      "|    clip_fraction        | 0.081       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.963      |\n",
      "|    explained_variance   | 0.0523      |\n",
      "|    learning_rate        | 9.82e-05    |\n",
      "|    loss                 | 0.000555    |\n",
      "|    n_updates            | 2820        |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    value_loss           | 0.171       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3814249   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.35        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 4858        |\n",
      "|    total_timesteps      | 72000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010273399 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.938      |\n",
      "|    explained_variance   | 0.00879     |\n",
      "|    learning_rate        | 9.82e-05    |\n",
      "|    loss                 | -0.0406     |\n",
      "|    n_updates            | 2850        |\n",
      "|    policy_gradient_loss | -0.0236     |\n",
      "|    value_loss           | 0.0701      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3703961   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.32        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 4908        |\n",
      "|    total_timesteps      | 72750       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013447889 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.963      |\n",
      "|    explained_variance   | 0.0753      |\n",
      "|    learning_rate        | 9.82e-05    |\n",
      "|    loss                 | -0.0171     |\n",
      "|    n_updates            | 2880        |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    value_loss           | 0.12        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2966638   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.27        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 4958        |\n",
      "|    total_timesteps      | 73500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011891979 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.941      |\n",
      "|    explained_variance   | -0.11       |\n",
      "|    learning_rate        | 9.82e-05    |\n",
      "|    loss                 | -0.0243     |\n",
      "|    n_updates            | 2910        |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    value_loss           | 0.105       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.20932    |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15         |\n",
      "|    ep_rew_mean          | 1.24       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 99         |\n",
      "|    time_elapsed         | 5008       |\n",
      "|    total_timesteps      | 74250      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01208156 |\n",
      "|    clip_fraction        | 0.114      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.933     |\n",
      "|    explained_variance   | 0.014      |\n",
      "|    learning_rate        | 9.82e-05   |\n",
      "|    loss                 | 0.0231     |\n",
      "|    n_updates            | 2940       |\n",
      "|    policy_gradient_loss | -0.0299    |\n",
      "|    value_loss           | 0.216      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=75000, episode_reward=1.57 +/- 0.03\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.57        |\n",
      "| mean_reward             | 1.3194362   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 75000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010431653 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.913      |\n",
      "|    explained_variance   | -0.000571   |\n",
      "|    learning_rate        | 9.81e-05    |\n",
      "|    loss                 | 0.00314     |\n",
      "|    n_updates            | 2970        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    value_loss           | 0.166       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15       |\n",
      "|    ep_rew_mean     | 1.34     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 100      |\n",
      "|    time_elapsed    | 5062     |\n",
      "|    total_timesteps | 75000    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3998728   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 101         |\n",
      "|    time_elapsed         | 5112        |\n",
      "|    total_timesteps      | 75750       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014409086 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.924      |\n",
      "|    explained_variance   | 0.138       |\n",
      "|    learning_rate        | 9.81e-05    |\n",
      "|    loss                 | -0.0353     |\n",
      "|    n_updates            | 3000        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    value_loss           | 0.092       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2358822   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.3         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 102         |\n",
      "|    time_elapsed         | 5162        |\n",
      "|    total_timesteps      | 76500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009433822 |\n",
      "|    clip_fraction        | 0.0842      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.915      |\n",
      "|    explained_variance   | -0.147      |\n",
      "|    learning_rate        | 9.81e-05    |\n",
      "|    loss                 | 0.0159      |\n",
      "|    n_updates            | 3030        |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    value_loss           | 0.192       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3711061   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 103         |\n",
      "|    time_elapsed         | 5212        |\n",
      "|    total_timesteps      | 77250       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015299023 |\n",
      "|    clip_fraction        | 0.0966      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.874      |\n",
      "|    explained_variance   | -0.0239     |\n",
      "|    learning_rate        | 9.81e-05    |\n",
      "|    loss                 | -0.0191     |\n",
      "|    n_updates            | 3060        |\n",
      "|    policy_gradient_loss | -0.0248     |\n",
      "|    value_loss           | 0.116       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.408973   |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15         |\n",
      "|    ep_rew_mean          | 1.41       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 104        |\n",
      "|    time_elapsed         | 5262       |\n",
      "|    total_timesteps      | 78000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01218504 |\n",
      "|    clip_fraction        | 0.114      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.883     |\n",
      "|    explained_variance   | 0.00566    |\n",
      "|    learning_rate        | 9.81e-05   |\n",
      "|    loss                 | -0.0351    |\n",
      "|    n_updates            | 3090       |\n",
      "|    policy_gradient_loss | -0.0225    |\n",
      "|    value_loss           | 0.0705     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.388099    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.33        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 105         |\n",
      "|    time_elapsed         | 5312        |\n",
      "|    total_timesteps      | 78750       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010371202 |\n",
      "|    clip_fraction        | 0.0728      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.886      |\n",
      "|    explained_variance   | 0.0188      |\n",
      "|    learning_rate        | 9.8e-05     |\n",
      "|    loss                 | -0.0233     |\n",
      "|    n_updates            | 3120        |\n",
      "|    policy_gradient_loss | -0.0217     |\n",
      "|    value_loss           | 0.0947      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2843835   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.3         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 106         |\n",
      "|    time_elapsed         | 5361        |\n",
      "|    total_timesteps      | 79500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011973228 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.893      |\n",
      "|    explained_variance   | -0.0359     |\n",
      "|    learning_rate        | 9.8e-05     |\n",
      "|    loss                 | -0.0224     |\n",
      "|    n_updates            | 3150        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    value_loss           | 0.119       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=80000, episode_reward=1.60 +/- 0.01\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.6         |\n",
      "| mean_reward             | 1.3397735   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014715511 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.925      |\n",
      "|    explained_variance   | 0.0973      |\n",
      "|    learning_rate        | 9.8e-05     |\n",
      "|    loss                 | -0.0167     |\n",
      "|    n_updates            | 3180        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.39     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 107      |\n",
      "|    time_elapsed    | 5416     |\n",
      "|    total_timesteps | 80250    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.3942829  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15.1       |\n",
      "|    ep_rew_mean          | 1.44       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 108        |\n",
      "|    time_elapsed         | 5466       |\n",
      "|    total_timesteps      | 81000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01175383 |\n",
      "|    clip_fraction        | 0.15       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.897     |\n",
      "|    explained_variance   | 0.00735    |\n",
      "|    learning_rate        | 9.8e-05    |\n",
      "|    loss                 | -0.0496    |\n",
      "|    n_updates            | 3210       |\n",
      "|    policy_gradient_loss | -0.0269    |\n",
      "|    value_loss           | 0.0558     |\n",
      "----------------------------------------\n",
      "New best mean reward: 1.4749553203582764. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.4749553  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15         |\n",
      "|    ep_rew_mean          | 1.45       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 109        |\n",
      "|    time_elapsed         | 5516       |\n",
      "|    total_timesteps      | 81750      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01458587 |\n",
      "|    clip_fraction        | 0.191      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.882     |\n",
      "|    explained_variance   | -0.0635    |\n",
      "|    learning_rate        | 9.8e-05    |\n",
      "|    loss                 | -0.0357    |\n",
      "|    n_updates            | 3240       |\n",
      "|    policy_gradient_loss | -0.0279    |\n",
      "|    value_loss           | 0.0804     |\n",
      "----------------------------------------\n",
      "New best mean reward: 1.490881085395813. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4908811   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.47        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 110         |\n",
      "|    time_elapsed         | 5566        |\n",
      "|    total_timesteps      | 82500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008462761 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.927      |\n",
      "|    explained_variance   | 0.093       |\n",
      "|    learning_rate        | 9.8e-05     |\n",
      "|    loss                 | -0.0479     |\n",
      "|    n_updates            | 3270        |\n",
      "|    policy_gradient_loss | -0.0198     |\n",
      "|    value_loss           | 0.0341      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4481884   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 111         |\n",
      "|    time_elapsed         | 5615        |\n",
      "|    total_timesteps      | 83250       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013856739 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.939      |\n",
      "|    explained_variance   | 0.051       |\n",
      "|    learning_rate        | 9.79e-05    |\n",
      "|    loss                 | -0.0512     |\n",
      "|    n_updates            | 3300        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    value_loss           | 0.0501      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3307463   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 112         |\n",
      "|    time_elapsed         | 5665        |\n",
      "|    total_timesteps      | 84000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011770251 |\n",
      "|    clip_fraction        | 0.0816      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.916      |\n",
      "|    explained_variance   | -0.0975     |\n",
      "|    learning_rate        | 9.79e-05    |\n",
      "|    loss                 | -0.0374     |\n",
      "|    n_updates            | 3330        |\n",
      "|    policy_gradient_loss | -0.0243     |\n",
      "|    value_loss           | 0.0771      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4594331   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.4         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 113         |\n",
      "|    time_elapsed         | 5715        |\n",
      "|    total_timesteps      | 84750       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015899496 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.881      |\n",
      "|    explained_variance   | 0.0423      |\n",
      "|    learning_rate        | 9.79e-05    |\n",
      "|    loss                 | -0.0217     |\n",
      "|    n_updates            | 3360        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    value_loss           | 0.112       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=85000, episode_reward=1.59 +/- 0.03\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.59        |\n",
      "| mean_reward             | 1.3651633   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 85000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012583502 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.862      |\n",
      "|    explained_variance   | -0.0309     |\n",
      "|    learning_rate        | 9.79e-05    |\n",
      "|    loss                 | -0.0315     |\n",
      "|    n_updates            | 3390        |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    value_loss           | 0.0778      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.42     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 114      |\n",
      "|    time_elapsed    | 5770     |\n",
      "|    total_timesteps | 85500    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4400936   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 115         |\n",
      "|    time_elapsed         | 5820        |\n",
      "|    total_timesteps      | 86250       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011548562 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.869      |\n",
      "|    explained_variance   | -0.0463     |\n",
      "|    learning_rate        | 9.79e-05    |\n",
      "|    loss                 | -0.0287     |\n",
      "|    n_updates            | 3420        |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    value_loss           | 0.0885      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4540074   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 116         |\n",
      "|    time_elapsed         | 5870        |\n",
      "|    total_timesteps      | 87000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010620581 |\n",
      "|    clip_fraction        | 0.0961      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.894      |\n",
      "|    explained_variance   | -0.0345     |\n",
      "|    learning_rate        | 9.78e-05    |\n",
      "|    loss                 | -0.0338     |\n",
      "|    n_updates            | 3450        |\n",
      "|    policy_gradient_loss | -0.0217     |\n",
      "|    value_loss           | 0.07        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3756901   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.36        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 117         |\n",
      "|    time_elapsed         | 5920        |\n",
      "|    total_timesteps      | 87750       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010253474 |\n",
      "|    clip_fraction        | 0.0924      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.9        |\n",
      "|    explained_variance   | -0.0277     |\n",
      "|    learning_rate        | 9.78e-05    |\n",
      "|    loss                 | -0.0271     |\n",
      "|    n_updates            | 3480        |\n",
      "|    policy_gradient_loss | -0.023      |\n",
      "|    value_loss           | 0.0935      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.3464148  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15         |\n",
      "|    ep_rew_mean          | 1.41       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 118        |\n",
      "|    time_elapsed         | 5969       |\n",
      "|    total_timesteps      | 88500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01658958 |\n",
      "|    clip_fraction        | 0.151      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.908     |\n",
      "|    explained_variance   | -0.00966   |\n",
      "|    learning_rate        | 9.78e-05   |\n",
      "|    loss                 | -0.0395    |\n",
      "|    n_updates            | 3510       |\n",
      "|    policy_gradient_loss | -0.03      |\n",
      "|    value_loss           | 0.0832     |\n",
      "----------------------------------------\n",
      "New best mean reward: 1.4947922229766846. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4947922   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 119         |\n",
      "|    time_elapsed         | 6019        |\n",
      "|    total_timesteps      | 89250       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011542438 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.914      |\n",
      "|    explained_variance   | 0.0279      |\n",
      "|    learning_rate        | 9.78e-05    |\n",
      "|    loss                 | -0.0433     |\n",
      "|    n_updates            | 3540        |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    value_loss           | 0.0532      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=90000, episode_reward=1.60 +/- 0.03\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.6         |\n",
      "| mean_reward             | 1.3901687   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 90000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020684076 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.906      |\n",
      "|    explained_variance   | 0.0302      |\n",
      "|    learning_rate        | 9.78e-05    |\n",
      "|    loss                 | -0.0268     |\n",
      "|    n_updates            | 3570        |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    value_loss           | 0.0929      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15       |\n",
      "|    ep_rew_mean     | 1.41     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 120      |\n",
      "|    time_elapsed    | 6074     |\n",
      "|    total_timesteps | 90000    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3795166   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.38        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 121         |\n",
      "|    time_elapsed         | 6124        |\n",
      "|    total_timesteps      | 90750       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017510608 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.936      |\n",
      "|    explained_variance   | -0.0507     |\n",
      "|    learning_rate        | 9.78e-05    |\n",
      "|    loss                 | -0.0145     |\n",
      "|    n_updates            | 3600        |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    value_loss           | 0.12        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| mean_reward             | 1.3364666    |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15.1         |\n",
      "|    ep_rew_mean          | 1.38         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 122          |\n",
      "|    time_elapsed         | 6174         |\n",
      "|    total_timesteps      | 91500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0121818725 |\n",
      "|    clip_fraction        | 0.129        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.918       |\n",
      "|    explained_variance   | -0.033       |\n",
      "|    learning_rate        | 9.77e-05     |\n",
      "|    loss                 | -0.0352      |\n",
      "|    n_updates            | 3630         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    value_loss           | 0.0848       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4278822   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 123         |\n",
      "|    time_elapsed         | 6223        |\n",
      "|    total_timesteps      | 92250       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014618608 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.934      |\n",
      "|    explained_variance   | 0.0746      |\n",
      "|    learning_rate        | 9.77e-05    |\n",
      "|    loss                 | -0.0383     |\n",
      "|    n_updates            | 3660        |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    value_loss           | 0.0724      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4501472   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.33        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 124         |\n",
      "|    time_elapsed         | 6273        |\n",
      "|    total_timesteps      | 93000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017033296 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.943      |\n",
      "|    explained_variance   | -0.00818    |\n",
      "|    learning_rate        | 9.77e-05    |\n",
      "|    loss                 | -0.0427     |\n",
      "|    n_updates            | 3690        |\n",
      "|    policy_gradient_loss | -0.022      |\n",
      "|    value_loss           | 0.0552      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2673067   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 125         |\n",
      "|    time_elapsed         | 6323        |\n",
      "|    total_timesteps      | 93750       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015833179 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.944      |\n",
      "|    explained_variance   | -0.00392    |\n",
      "|    learning_rate        | 9.77e-05    |\n",
      "|    loss                 | 0.0199      |\n",
      "|    n_updates            | 3720        |\n",
      "|    policy_gradient_loss | -0.0316     |\n",
      "|    value_loss           | 0.218       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| mean_reward             | 1.4554737    |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15           |\n",
      "|    ep_rew_mean          | 1.42         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 126          |\n",
      "|    time_elapsed         | 6373         |\n",
      "|    total_timesteps      | 94500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0104880305 |\n",
      "|    clip_fraction        | 0.147        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.968       |\n",
      "|    explained_variance   | -0.0469      |\n",
      "|    learning_rate        | 9.77e-05     |\n",
      "|    loss                 | -0.0471      |\n",
      "|    n_updates            | 3750         |\n",
      "|    policy_gradient_loss | -0.0202      |\n",
      "|    value_loss           | 0.0466       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=95000, episode_reward=1.37 +/- 0.39\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.37        |\n",
      "| mean_reward             | 1.3800464   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 95000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011628063 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.951      |\n",
      "|    explained_variance   | -0.061      |\n",
      "|    learning_rate        | 9.76e-05    |\n",
      "|    loss                 | -0.0267     |\n",
      "|    n_updates            | 3780        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    value_loss           | 0.111       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.4      |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 127      |\n",
      "|    time_elapsed    | 6428     |\n",
      "|    total_timesteps | 95250    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.4330446  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15.1       |\n",
      "|    ep_rew_mean          | 1.39       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 128        |\n",
      "|    time_elapsed         | 6478       |\n",
      "|    total_timesteps      | 96000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01302324 |\n",
      "|    clip_fraction        | 0.119      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.936     |\n",
      "|    explained_variance   | 0.0296     |\n",
      "|    learning_rate        | 9.76e-05   |\n",
      "|    loss                 | -0.029     |\n",
      "|    n_updates            | 3810       |\n",
      "|    policy_gradient_loss | -0.0255    |\n",
      "|    value_loss           | 0.0914     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3393359   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.4         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 129         |\n",
      "|    time_elapsed         | 6528        |\n",
      "|    total_timesteps      | 96750       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011258682 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.944      |\n",
      "|    explained_variance   | 0.0567      |\n",
      "|    learning_rate        | 9.76e-05    |\n",
      "|    loss                 | -0.0125     |\n",
      "|    n_updates            | 3840        |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    value_loss           | 0.129       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4294908   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 130         |\n",
      "|    time_elapsed         | 6577        |\n",
      "|    total_timesteps      | 97500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016557047 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.926      |\n",
      "|    explained_variance   | 0.124       |\n",
      "|    learning_rate        | 9.76e-05    |\n",
      "|    loss                 | -0.0462     |\n",
      "|    n_updates            | 3870        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    value_loss           | 0.0657      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3911744   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 131         |\n",
      "|    time_elapsed         | 6627        |\n",
      "|    total_timesteps      | 98250       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015131579 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.951      |\n",
      "|    explained_variance   | -0.0131     |\n",
      "|    learning_rate        | 9.76e-05    |\n",
      "|    loss                 | -0.0222     |\n",
      "|    n_updates            | 3900        |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    value_loss           | 0.108       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3775628   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 132         |\n",
      "|    time_elapsed         | 6677        |\n",
      "|    total_timesteps      | 99000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011192381 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.94       |\n",
      "|    explained_variance   | 0.00738     |\n",
      "|    learning_rate        | 9.75e-05    |\n",
      "|    loss                 | -0.0305     |\n",
      "|    n_updates            | 3930        |\n",
      "|    policy_gradient_loss | -0.0239     |\n",
      "|    value_loss           | 0.0802      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4507936   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.37        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 133         |\n",
      "|    time_elapsed         | 6727        |\n",
      "|    total_timesteps      | 99750       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015099963 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.908      |\n",
      "|    explained_variance   | -0.0376     |\n",
      "|    learning_rate        | 9.75e-05    |\n",
      "|    loss                 | -0.0305     |\n",
      "|    n_updates            | 3960        |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    value_loss           | 0.0871      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=100000, episode_reward=1.38 +/- 0.39\n",
      "Episode length: 15.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 15         |\n",
      "|    mean_reward          | 1.38       |\n",
      "| mean_reward             | 1.3336513  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 100000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01139333 |\n",
      "|    clip_fraction        | 0.112      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.926     |\n",
      "|    explained_variance   | 0.0432     |\n",
      "|    learning_rate        | 9.75e-05   |\n",
      "|    loss                 | -0.00562   |\n",
      "|    n_updates            | 3990       |\n",
      "|    policy_gradient_loss | -0.0262    |\n",
      "|    value_loss           | 0.139      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.37     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 134      |\n",
      "|    time_elapsed    | 6782     |\n",
      "|    total_timesteps | 100500   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4105128   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 135         |\n",
      "|    time_elapsed         | 6831        |\n",
      "|    total_timesteps      | 101250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009826245 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.975      |\n",
      "|    explained_variance   | 0.0657      |\n",
      "|    learning_rate        | 9.75e-05    |\n",
      "|    loss                 | -0.0392     |\n",
      "|    n_updates            | 4020        |\n",
      "|    policy_gradient_loss | -0.0224     |\n",
      "|    value_loss           | 0.0651      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.455592    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 136         |\n",
      "|    time_elapsed         | 6881        |\n",
      "|    total_timesteps      | 102000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010241485 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.956      |\n",
      "|    explained_variance   | 0.0486      |\n",
      "|    learning_rate        | 9.75e-05    |\n",
      "|    loss                 | -0.0472     |\n",
      "|    n_updates            | 4050        |\n",
      "|    policy_gradient_loss | -0.0219     |\n",
      "|    value_loss           | 0.0442      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.384551    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 137         |\n",
      "|    time_elapsed         | 6931        |\n",
      "|    total_timesteps      | 102750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013416175 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.959      |\n",
      "|    explained_variance   | -0.00253    |\n",
      "|    learning_rate        | 9.75e-05    |\n",
      "|    loss                 | -0.0198     |\n",
      "|    n_updates            | 4080        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    value_loss           | 0.114       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4720635   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.49        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 138         |\n",
      "|    time_elapsed         | 6981        |\n",
      "|    total_timesteps      | 103500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012386553 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.949      |\n",
      "|    explained_variance   | 0.0929      |\n",
      "|    learning_rate        | 9.74e-05    |\n",
      "|    loss                 | -0.0494     |\n",
      "|    n_updates            | 4110        |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    value_loss           | 0.0521      |\n",
      "-----------------------------------------\n",
      "New best mean reward: 1.4950203895568848. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4950204   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.5         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 139         |\n",
      "|    time_elapsed         | 7031        |\n",
      "|    total_timesteps      | 104250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010973644 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.967      |\n",
      "|    explained_variance   | 0.0926      |\n",
      "|    learning_rate        | 9.74e-05    |\n",
      "|    loss                 | -0.0519     |\n",
      "|    n_updates            | 4140        |\n",
      "|    policy_gradient_loss | -0.0191     |\n",
      "|    value_loss           | 0.0219      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=105000, episode_reward=1.58 +/- 0.04\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.58        |\n",
      "| mean_reward             | 1.4737097   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 105000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011383781 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.936      |\n",
      "|    explained_variance   | 0.0259      |\n",
      "|    learning_rate        | 9.74e-05    |\n",
      "|    loss                 | -0.0355     |\n",
      "|    n_updates            | 4170        |\n",
      "|    policy_gradient_loss | -0.024      |\n",
      "|    value_loss           | 0.066       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15       |\n",
      "|    ep_rew_mean     | 1.41     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 140      |\n",
      "|    time_elapsed    | 7086     |\n",
      "|    total_timesteps | 105000   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3359189   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.37        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 141         |\n",
      "|    time_elapsed         | 7136        |\n",
      "|    total_timesteps      | 105750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010837032 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.961      |\n",
      "|    explained_variance   | 0.0203      |\n",
      "|    learning_rate        | 9.74e-05    |\n",
      "|    loss                 | -0.0128     |\n",
      "|    n_updates            | 4200        |\n",
      "|    policy_gradient_loss | -0.0227     |\n",
      "|    value_loss           | 0.12        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3966355   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.38        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 142         |\n",
      "|    time_elapsed         | 7185        |\n",
      "|    total_timesteps      | 106500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013474674 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.983      |\n",
      "|    explained_variance   | 0.0059      |\n",
      "|    learning_rate        | 9.74e-05    |\n",
      "|    loss                 | -0.0448     |\n",
      "|    n_updates            | 4230        |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    value_loss           | 0.077       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3906946   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 143         |\n",
      "|    time_elapsed         | 7235        |\n",
      "|    total_timesteps      | 107250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008791973 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.975      |\n",
      "|    explained_variance   | 0.00421     |\n",
      "|    learning_rate        | 9.73e-05    |\n",
      "|    loss                 | -0.026      |\n",
      "|    n_updates            | 4260        |\n",
      "|    policy_gradient_loss | -0.0216     |\n",
      "|    value_loss           | 0.088       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4112227   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.22        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 144         |\n",
      "|    time_elapsed         | 7285        |\n",
      "|    total_timesteps      | 108000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013684039 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.968      |\n",
      "|    explained_variance   | 0.0767      |\n",
      "|    learning_rate        | 9.73e-05    |\n",
      "|    loss                 | -0.0493     |\n",
      "|    n_updates            | 4290        |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    value_loss           | 0.0507      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.0410149   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.09        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 145         |\n",
      "|    time_elapsed         | 7335        |\n",
      "|    total_timesteps      | 108750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017269623 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.955      |\n",
      "|    explained_variance   | -0.0256     |\n",
      "|    learning_rate        | 9.73e-05    |\n",
      "|    loss                 | 0.081       |\n",
      "|    n_updates            | 4320        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    value_loss           | 0.346       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.1731329   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.32        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 146         |\n",
      "|    time_elapsed         | 7385        |\n",
      "|    total_timesteps      | 109500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014523337 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.935      |\n",
      "|    explained_variance   | -0.0413     |\n",
      "|    learning_rate        | 9.73e-05    |\n",
      "|    loss                 | 0.0259      |\n",
      "|    n_updates            | 4350        |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    value_loss           | 0.211       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=110000, episode_reward=1.59 +/- 0.03\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.59        |\n",
      "| mean_reward             | 1.4669025   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 110000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009917396 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.946      |\n",
      "|    explained_variance   | 0.1         |\n",
      "|    learning_rate        | 9.73e-05    |\n",
      "|    loss                 | -0.0381     |\n",
      "|    n_updates            | 4380        |\n",
      "|    policy_gradient_loss | -0.0206     |\n",
      "|    value_loss           | 0.0631      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.46     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 147      |\n",
      "|    time_elapsed    | 7440     |\n",
      "|    total_timesteps | 110250   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4520007   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.49        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 148         |\n",
      "|    time_elapsed         | 7490        |\n",
      "|    total_timesteps      | 111000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012593904 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.942      |\n",
      "|    explained_variance   | 0.0904      |\n",
      "|    learning_rate        | 9.72e-05    |\n",
      "|    loss                 | -0.0425     |\n",
      "|    n_updates            | 4410        |\n",
      "|    policy_gradient_loss | -0.0226     |\n",
      "|    value_loss           | 0.0501      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| mean_reward             | 1.4535977    |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15           |\n",
      "|    ep_rew_mean          | 1.43         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 149          |\n",
      "|    time_elapsed         | 7539         |\n",
      "|    total_timesteps      | 111750       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0117582325 |\n",
      "|    clip_fraction        | 0.114        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.934       |\n",
      "|    explained_variance   | 0.101        |\n",
      "|    learning_rate        | 9.72e-05     |\n",
      "|    loss                 | -0.0491      |\n",
      "|    n_updates            | 4440         |\n",
      "|    policy_gradient_loss | -0.0229      |\n",
      "|    value_loss           | 0.0367       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3595277   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.4         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 150         |\n",
      "|    time_elapsed         | 7589        |\n",
      "|    total_timesteps      | 112500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014371215 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.905      |\n",
      "|    explained_variance   | 0.0387      |\n",
      "|    learning_rate        | 9.72e-05    |\n",
      "|    loss                 | -0.0181     |\n",
      "|    n_updates            | 4470        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    value_loss           | 0.116       |\n",
      "-----------------------------------------\n",
      "New best mean reward: 1.506729245185852. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "------------------------------------------\n",
      "| mean_reward             | 1.5067292    |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15           |\n",
      "|    ep_rew_mean          | 1.47         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 151          |\n",
      "|    time_elapsed         | 7639         |\n",
      "|    total_timesteps      | 113250       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0155142965 |\n",
      "|    clip_fraction        | 0.14         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.914       |\n",
      "|    explained_variance   | 0.053        |\n",
      "|    learning_rate        | 9.72e-05     |\n",
      "|    loss                 | -0.0472      |\n",
      "|    n_updates            | 4500         |\n",
      "|    policy_gradient_loss | -0.025       |\n",
      "|    value_loss           | 0.0513       |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| mean_reward             | 1.4795285 |\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 15        |\n",
      "|    ep_rew_mean          | 1.46      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 14        |\n",
      "|    iterations           | 152       |\n",
      "|    time_elapsed         | 7689      |\n",
      "|    total_timesteps      | 114000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0096625 |\n",
      "|    clip_fraction        | 0.141     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.93     |\n",
      "|    explained_variance   | 0.11      |\n",
      "|    learning_rate        | 9.72e-05  |\n",
      "|    loss                 | -0.0522   |\n",
      "|    n_updates            | 4530      |\n",
      "|    policy_gradient_loss | -0.0225   |\n",
      "|    value_loss           | 0.0285    |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4099703   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 153         |\n",
      "|    time_elapsed         | 7739        |\n",
      "|    total_timesteps      | 114750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014631286 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.915      |\n",
      "|    explained_variance   | -0.00814    |\n",
      "|    learning_rate        | 9.72e-05    |\n",
      "|    loss                 | -0.0303     |\n",
      "|    n_updates            | 4560        |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    value_loss           | 0.0813      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=115000, episode_reward=1.60 +/- 0.02\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.6         |\n",
      "| mean_reward             | 1.440525    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 115000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011833066 |\n",
      "|    clip_fraction        | 0.0936      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.935      |\n",
      "|    explained_variance   | 0.0284      |\n",
      "|    learning_rate        | 9.71e-05    |\n",
      "|    loss                 | -0.0175     |\n",
      "|    n_updates            | 4590        |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    value_loss           | 0.0967      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.44     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 154      |\n",
      "|    time_elapsed    | 7794     |\n",
      "|    total_timesteps | 115500   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4728872   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.46        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 155         |\n",
      "|    time_elapsed         | 7844        |\n",
      "|    total_timesteps      | 116250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012952032 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.942      |\n",
      "|    explained_variance   | 0.061       |\n",
      "|    learning_rate        | 9.71e-05    |\n",
      "|    loss                 | -0.0392     |\n",
      "|    n_updates            | 4620        |\n",
      "|    policy_gradient_loss | -0.024      |\n",
      "|    value_loss           | 0.0638      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4375516   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 156         |\n",
      "|    time_elapsed         | 7893        |\n",
      "|    total_timesteps      | 117000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012302823 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.944      |\n",
      "|    explained_variance   | -0.0681     |\n",
      "|    learning_rate        | 9.71e-05    |\n",
      "|    loss                 | -0.0326     |\n",
      "|    n_updates            | 4650        |\n",
      "|    policy_gradient_loss | -0.023      |\n",
      "|    value_loss           | 0.0726      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.421434    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 157         |\n",
      "|    time_elapsed         | 7943        |\n",
      "|    total_timesteps      | 117750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010850823 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.951      |\n",
      "|    explained_variance   | 0.05        |\n",
      "|    learning_rate        | 9.71e-05    |\n",
      "|    loss                 | -0.0308     |\n",
      "|    n_updates            | 4680        |\n",
      "|    policy_gradient_loss | -0.022      |\n",
      "|    value_loss           | 0.0714      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4011362   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.38        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 158         |\n",
      "|    time_elapsed         | 7993        |\n",
      "|    total_timesteps      | 118500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009227463 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.99       |\n",
      "|    explained_variance   | 0.0294      |\n",
      "|    learning_rate        | 9.71e-05    |\n",
      "|    loss                 | -0.0122     |\n",
      "|    n_updates            | 4710        |\n",
      "|    policy_gradient_loss | -0.0257     |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3517144   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.32        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 159         |\n",
      "|    time_elapsed         | 8043        |\n",
      "|    total_timesteps      | 119250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018441759 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.991      |\n",
      "|    explained_variance   | 0.0273      |\n",
      "|    learning_rate        | 9.7e-05     |\n",
      "|    loss                 | -0.0257     |\n",
      "|    n_updates            | 4740        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    value_loss           | 0.117       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=120000, episode_reward=1.58 +/- 0.03\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.58        |\n",
      "| mean_reward             | 1.3267859   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 120000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013424411 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.96       |\n",
      "|    explained_variance   | 0.0257      |\n",
      "|    learning_rate        | 9.7e-05     |\n",
      "|    loss                 | -0.0243     |\n",
      "|    n_updates            | 4770        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15       |\n",
      "|    ep_rew_mean     | 1.35     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 160      |\n",
      "|    time_elapsed    | 8098     |\n",
      "|    total_timesteps | 120000   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3869454   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 161         |\n",
      "|    time_elapsed         | 8148        |\n",
      "|    total_timesteps      | 120750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013926691 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.984      |\n",
      "|    explained_variance   | -0.142      |\n",
      "|    learning_rate        | 9.7e-05     |\n",
      "|    loss                 | -0.0343     |\n",
      "|    n_updates            | 4800        |\n",
      "|    policy_gradient_loss | -0.0229     |\n",
      "|    value_loss           | 0.0835      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4240075   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 162         |\n",
      "|    time_elapsed         | 8197        |\n",
      "|    total_timesteps      | 121500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011504463 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.955      |\n",
      "|    explained_variance   | 0.0537      |\n",
      "|    learning_rate        | 9.7e-05     |\n",
      "|    loss                 | -0.0432     |\n",
      "|    n_updates            | 4830        |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    value_loss           | 0.0617      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.37191     |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.37        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 163         |\n",
      "|    time_elapsed         | 8247        |\n",
      "|    total_timesteps      | 122250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008339267 |\n",
      "|    clip_fraction        | 0.0958      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.979      |\n",
      "|    explained_variance   | -0.0188     |\n",
      "|    learning_rate        | 9.7e-05     |\n",
      "|    loss                 | -0.00721    |\n",
      "|    n_updates            | 4860        |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3707123   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 164         |\n",
      "|    time_elapsed         | 8297        |\n",
      "|    total_timesteps      | 123000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010691577 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.97       |\n",
      "|    explained_variance   | 0.0594      |\n",
      "|    learning_rate        | 9.69e-05    |\n",
      "|    loss                 | -0.0274     |\n",
      "|    n_updates            | 4890        |\n",
      "|    policy_gradient_loss | -0.0217     |\n",
      "|    value_loss           | 0.0841      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3880826   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.38        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 165         |\n",
      "|    time_elapsed         | 8347        |\n",
      "|    total_timesteps      | 123750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009319822 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.961      |\n",
      "|    explained_variance   | 0.0112      |\n",
      "|    learning_rate        | 9.69e-05    |\n",
      "|    loss                 | -0.0417     |\n",
      "|    n_updates            | 4920        |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    value_loss           | 0.0552      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3830572   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.38        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 166         |\n",
      "|    time_elapsed         | 8397        |\n",
      "|    total_timesteps      | 124500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014614021 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.991      |\n",
      "|    explained_variance   | -0.0248     |\n",
      "|    learning_rate        | 9.69e-05    |\n",
      "|    loss                 | -0.0231     |\n",
      "|    n_updates            | 4950        |\n",
      "|    policy_gradient_loss | -0.0246     |\n",
      "|    value_loss           | 0.104       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=125000, episode_reward=1.61 +/- 0.01\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.61        |\n",
      "| mean_reward             | 1.433998    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 125000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014279555 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.998      |\n",
      "|    explained_variance   | 0.078       |\n",
      "|    learning_rate        | 9.69e-05    |\n",
      "|    loss                 | -0.0467     |\n",
      "|    n_updates            | 4980        |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    value_loss           | 0.0551      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.34     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 167      |\n",
      "|    time_elapsed    | 8452     |\n",
      "|    total_timesteps | 125250   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2354487   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.31        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 168         |\n",
      "|    time_elapsed         | 8502        |\n",
      "|    total_timesteps      | 126000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012653243 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.96       |\n",
      "|    explained_variance   | 0.00949     |\n",
      "|    learning_rate        | 9.69e-05    |\n",
      "|    loss                 | -0.00595    |\n",
      "|    n_updates            | 5010        |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    value_loss           | 0.138       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3988266   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.38        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 169         |\n",
      "|    time_elapsed         | 8552        |\n",
      "|    total_timesteps      | 126750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010205911 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.964      |\n",
      "|    explained_variance   | -0.0131     |\n",
      "|    learning_rate        | 9.69e-05    |\n",
      "|    loss                 | -0.0259     |\n",
      "|    n_updates            | 5040        |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    value_loss           | 0.098       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3339773   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 170         |\n",
      "|    time_elapsed         | 8601        |\n",
      "|    total_timesteps      | 127500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013706605 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.989      |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 9.68e-05    |\n",
      "|    loss                 | -0.0365     |\n",
      "|    n_updates            | 5070        |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    value_loss           | 0.0766      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4970213   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 171         |\n",
      "|    time_elapsed         | 8651        |\n",
      "|    total_timesteps      | 128250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012882247 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.964      |\n",
      "|    explained_variance   | 0.0481      |\n",
      "|    learning_rate        | 9.68e-05    |\n",
      "|    loss                 | -0.0445     |\n",
      "|    n_updates            | 5100        |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    value_loss           | 0.0545      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3613577   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.4         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 172         |\n",
      "|    time_elapsed         | 8701        |\n",
      "|    total_timesteps      | 129000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010606126 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.999      |\n",
      "|    explained_variance   | -0.0383     |\n",
      "|    learning_rate        | 9.68e-05    |\n",
      "|    loss                 | -0.0248     |\n",
      "|    n_updates            | 5130        |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    value_loss           | 0.105       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4092937   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 173         |\n",
      "|    time_elapsed         | 8751        |\n",
      "|    total_timesteps      | 129750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012690014 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.992      |\n",
      "|    explained_variance   | -0.0146     |\n",
      "|    learning_rate        | 9.68e-05    |\n",
      "|    loss                 | -0.0309     |\n",
      "|    n_updates            | 5160        |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    value_loss           | 0.0801      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=130000, episode_reward=1.59 +/- 0.04\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.59        |\n",
      "| mean_reward             | 1.3850491   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 130000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012522893 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.981      |\n",
      "|    explained_variance   | 0.0739      |\n",
      "|    learning_rate        | 9.68e-05    |\n",
      "|    loss                 | -0.0322     |\n",
      "|    n_updates            | 5190        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    value_loss           | 0.0904      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.39     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 174      |\n",
      "|    time_elapsed    | 8806     |\n",
      "|    total_timesteps | 130500   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3515975   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.28        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 175         |\n",
      "|    time_elapsed         | 8856        |\n",
      "|    total_timesteps      | 131250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011186924 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.962      |\n",
      "|    explained_variance   | 0.0266      |\n",
      "|    learning_rate        | 9.67e-05    |\n",
      "|    loss                 | -0.0503     |\n",
      "|    n_updates            | 5220        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    value_loss           | 0.0587      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.1593719  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15         |\n",
      "|    ep_rew_mean          | 1.2        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 176        |\n",
      "|    time_elapsed         | 8906       |\n",
      "|    total_timesteps      | 132000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01202928 |\n",
      "|    clip_fraction        | 0.0883     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.984     |\n",
      "|    explained_variance   | -0.0614    |\n",
      "|    learning_rate        | 9.67e-05   |\n",
      "|    loss                 | 0.0399     |\n",
      "|    n_updates            | 5250       |\n",
      "|    policy_gradient_loss | -0.0246    |\n",
      "|    value_loss           | 0.249      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3048286   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.31        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 177         |\n",
      "|    time_elapsed         | 8955        |\n",
      "|    total_timesteps      | 132750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013917838 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.978      |\n",
      "|    explained_variance   | -0.0334     |\n",
      "|    learning_rate        | 9.67e-05    |\n",
      "|    loss                 | -0.00756    |\n",
      "|    n_updates            | 5280        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    value_loss           | 0.155       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3557025   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 178         |\n",
      "|    time_elapsed         | 9005        |\n",
      "|    total_timesteps      | 133500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012897036 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.981      |\n",
      "|    explained_variance   | 0.0667      |\n",
      "|    learning_rate        | 9.67e-05    |\n",
      "|    loss                 | -0.0338     |\n",
      "|    n_updates            | 5310        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    value_loss           | 0.0937      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4217764   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.46        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 179         |\n",
      "|    time_elapsed         | 9055        |\n",
      "|    total_timesteps      | 134250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014057539 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.966      |\n",
      "|    explained_variance   | 0.113       |\n",
      "|    learning_rate        | 9.67e-05    |\n",
      "|    loss                 | -0.0538     |\n",
      "|    n_updates            | 5340        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    value_loss           | 0.0454      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=135000, episode_reward=1.60 +/- 0.02\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.6         |\n",
      "| mean_reward             | 1.4409183   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 135000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014109458 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.982      |\n",
      "|    explained_variance   | -0.00904    |\n",
      "|    learning_rate        | 9.66e-05    |\n",
      "|    loss                 | -0.0403     |\n",
      "|    n_updates            | 5370        |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    value_loss           | 0.0589      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15       |\n",
      "|    ep_rew_mean     | 1.42     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 180      |\n",
      "|    time_elapsed    | 9110     |\n",
      "|    total_timesteps | 135000   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4080489   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 181         |\n",
      "|    time_elapsed         | 9160        |\n",
      "|    total_timesteps      | 135750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013145565 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.999      |\n",
      "|    explained_variance   | 0.0447      |\n",
      "|    learning_rate        | 9.66e-05    |\n",
      "|    loss                 | -0.0423     |\n",
      "|    n_updates            | 5400        |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    value_loss           | 0.0708      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3029311   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.33        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 182         |\n",
      "|    time_elapsed         | 9210        |\n",
      "|    total_timesteps      | 136500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013762008 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.998      |\n",
      "|    explained_variance   | 0.00626     |\n",
      "|    learning_rate        | 9.66e-05    |\n",
      "|    loss                 | -0.0269     |\n",
      "|    n_updates            | 5430        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    value_loss           | 0.119       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3082286   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.35        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 183         |\n",
      "|    time_elapsed         | 9259        |\n",
      "|    total_timesteps      | 137250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015098638 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.988      |\n",
      "|    explained_variance   | 0.0349      |\n",
      "|    learning_rate        | 9.66e-05    |\n",
      "|    loss                 | -0.0343     |\n",
      "|    n_updates            | 5460        |\n",
      "|    policy_gradient_loss | -0.0329     |\n",
      "|    value_loss           | 0.107       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.433763    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.37        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 184         |\n",
      "|    time_elapsed         | 9310        |\n",
      "|    total_timesteps      | 138000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011162173 |\n",
      "|    clip_fraction        | 0.0912      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.993      |\n",
      "|    explained_variance   | 0.0228      |\n",
      "|    learning_rate        | 9.66e-05    |\n",
      "|    loss                 | -0.0233     |\n",
      "|    n_updates            | 5490        |\n",
      "|    policy_gradient_loss | -0.0239     |\n",
      "|    value_loss           | 0.107       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.36391     |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.38        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 185         |\n",
      "|    time_elapsed         | 9359        |\n",
      "|    total_timesteps      | 138750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012329842 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.0413      |\n",
      "|    learning_rate        | 9.66e-05    |\n",
      "|    loss                 | -0.0265     |\n",
      "|    n_updates            | 5520        |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    value_loss           | 0.0968      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.346912    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.32        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 186         |\n",
      "|    time_elapsed         | 9409        |\n",
      "|    total_timesteps      | 139500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011789899 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.0156      |\n",
      "|    learning_rate        | 9.65e-05    |\n",
      "|    loss                 | -0.0098     |\n",
      "|    n_updates            | 5550        |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    value_loss           | 0.138       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=140000, episode_reward=1.58 +/- 0.04\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.58        |\n",
      "| mean_reward             | 1.2854531   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 140000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014155312 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | -0.0295     |\n",
      "|    learning_rate        | 9.65e-05    |\n",
      "|    loss                 | -0.0176     |\n",
      "|    n_updates            | 5580        |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.35     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 187      |\n",
      "|    time_elapsed    | 9464     |\n",
      "|    total_timesteps | 140250   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4439759   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 188         |\n",
      "|    time_elapsed         | 9514        |\n",
      "|    total_timesteps      | 141000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011453401 |\n",
      "|    clip_fraction        | 0.0895      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.999      |\n",
      "|    explained_variance   | 0.126       |\n",
      "|    learning_rate        | 9.65e-05    |\n",
      "|    loss                 | -0.0508     |\n",
      "|    n_updates            | 5610        |\n",
      "|    policy_gradient_loss | -0.0251     |\n",
      "|    value_loss           | 0.0565      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4641298   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 189         |\n",
      "|    time_elapsed         | 9564        |\n",
      "|    total_timesteps      | 141750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013129247 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.00101     |\n",
      "|    learning_rate        | 9.65e-05    |\n",
      "|    loss                 | -0.0471     |\n",
      "|    n_updates            | 5640        |\n",
      "|    policy_gradient_loss | -0.0236     |\n",
      "|    value_loss           | 0.0497      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3759419   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 190         |\n",
      "|    time_elapsed         | 9614        |\n",
      "|    total_timesteps      | 142500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012972416 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.999      |\n",
      "|    explained_variance   | -0.0255     |\n",
      "|    learning_rate        | 9.65e-05    |\n",
      "|    loss                 | -0.0325     |\n",
      "|    n_updates            | 5670        |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    value_loss           | 0.0887      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4359369   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.46        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 191         |\n",
      "|    time_elapsed         | 9663        |\n",
      "|    total_timesteps      | 143250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011587163 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.982      |\n",
      "|    explained_variance   | 0.0515      |\n",
      "|    learning_rate        | 9.64e-05    |\n",
      "|    loss                 | -0.0476     |\n",
      "|    n_updates            | 5700        |\n",
      "|    policy_gradient_loss | -0.0251     |\n",
      "|    value_loss           | 0.0519      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4108794   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.38        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 192         |\n",
      "|    time_elapsed         | 9713        |\n",
      "|    total_timesteps      | 144000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013081157 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.977      |\n",
      "|    explained_variance   | 0.12        |\n",
      "|    learning_rate        | 9.64e-05    |\n",
      "|    loss                 | -0.0479     |\n",
      "|    n_updates            | 5730        |\n",
      "|    policy_gradient_loss | -0.0223     |\n",
      "|    value_loss           | 0.0353      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3281506   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.33        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 193         |\n",
      "|    time_elapsed         | 9763        |\n",
      "|    total_timesteps      | 144750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015867015 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.953      |\n",
      "|    explained_variance   | -0.0436     |\n",
      "|    learning_rate        | 9.64e-05    |\n",
      "|    loss                 | 0.000634    |\n",
      "|    n_updates            | 5760        |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    value_loss           | 0.16        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=145000, episode_reward=1.58 +/- 0.03\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.58        |\n",
      "| mean_reward             | 1.4353534   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 145000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014598134 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.949      |\n",
      "|    explained_variance   | 0.0316      |\n",
      "|    learning_rate        | 9.64e-05    |\n",
      "|    loss                 | -0.0305     |\n",
      "|    n_updates            | 5790        |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    value_loss           | 0.0883      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.46     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 194      |\n",
      "|    time_elapsed    | 9818     |\n",
      "|    total_timesteps | 145500   |\n",
      "---------------------------------\n",
      "New best mean reward: 1.5079772472381592. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.5079772   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 195         |\n",
      "|    time_elapsed         | 9868        |\n",
      "|    total_timesteps      | 146250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011554306 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.983      |\n",
      "|    explained_variance   | 0.157       |\n",
      "|    learning_rate        | 9.64e-05    |\n",
      "|    loss                 | -0.0521     |\n",
      "|    n_updates            | 5820        |\n",
      "|    policy_gradient_loss | -0.0195     |\n",
      "|    value_loss           | 0.0244      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.3762143  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15         |\n",
      "|    ep_rew_mean          | 1.4        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 196        |\n",
      "|    time_elapsed         | 9918       |\n",
      "|    total_timesteps      | 147000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01546609 |\n",
      "|    clip_fraction        | 0.176      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.98      |\n",
      "|    explained_variance   | 0.000851   |\n",
      "|    learning_rate        | 9.63e-05   |\n",
      "|    loss                 | -0.0261    |\n",
      "|    n_updates            | 5850       |\n",
      "|    policy_gradient_loss | -0.0306    |\n",
      "|    value_loss           | 0.106      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.3570137  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15         |\n",
      "|    ep_rew_mean          | 1.38       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 197        |\n",
      "|    time_elapsed         | 9968       |\n",
      "|    total_timesteps      | 147750     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01373356 |\n",
      "|    clip_fraction        | 0.168      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.953     |\n",
      "|    explained_variance   | 0.0715     |\n",
      "|    learning_rate        | 9.63e-05   |\n",
      "|    loss                 | -0.0377    |\n",
      "|    n_updates            | 5880       |\n",
      "|    policy_gradient_loss | -0.0281    |\n",
      "|    value_loss           | 0.0746     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.409006    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 198         |\n",
      "|    time_elapsed         | 10018       |\n",
      "|    total_timesteps      | 148500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010204325 |\n",
      "|    clip_fraction        | 0.0947      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.963      |\n",
      "|    explained_variance   | 0.0559      |\n",
      "|    learning_rate        | 9.63e-05    |\n",
      "|    loss                 | -0.0358     |\n",
      "|    n_updates            | 5910        |\n",
      "|    policy_gradient_loss | -0.0252     |\n",
      "|    value_loss           | 0.0791      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4971362   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.48        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 199         |\n",
      "|    time_elapsed         | 10067       |\n",
      "|    total_timesteps      | 149250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013613276 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.948      |\n",
      "|    explained_variance   | -0.0153     |\n",
      "|    learning_rate        | 9.63e-05    |\n",
      "|    loss                 | -0.0562     |\n",
      "|    n_updates            | 5940        |\n",
      "|    policy_gradient_loss | -0.0248     |\n",
      "|    value_loss           | 0.0313      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=150000, episode_reward=1.60 +/- 0.03\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.6         |\n",
      "| mean_reward             | 1.4412001   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 150000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011944209 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.929      |\n",
      "|    explained_variance   | 0.0237      |\n",
      "|    learning_rate        | 9.63e-05    |\n",
      "|    loss                 | -0.0248     |\n",
      "|    n_updates            | 5970        |\n",
      "|    policy_gradient_loss | -0.0248     |\n",
      "|    value_loss           | 0.0894      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15       |\n",
      "|    ep_rew_mean     | 1.41     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 200      |\n",
      "|    time_elapsed    | 10122    |\n",
      "|    total_timesteps | 150000   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4056102   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.4         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 201         |\n",
      "|    time_elapsed         | 10172       |\n",
      "|    total_timesteps      | 150750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008521878 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.93       |\n",
      "|    explained_variance   | 0.0607      |\n",
      "|    learning_rate        | 9.63e-05    |\n",
      "|    loss                 | -0.0104     |\n",
      "|    n_updates            | 6000        |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3958087   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 202         |\n",
      "|    time_elapsed         | 10222       |\n",
      "|    total_timesteps      | 151500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012785378 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.894      |\n",
      "|    explained_variance   | -0.0909     |\n",
      "|    learning_rate        | 9.62e-05    |\n",
      "|    loss                 | -0.0317     |\n",
      "|    n_updates            | 6030        |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    value_loss           | 0.0789      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3019482   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 203         |\n",
      "|    time_elapsed         | 10272       |\n",
      "|    total_timesteps      | 152250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014535781 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.88       |\n",
      "|    explained_variance   | -0.00406    |\n",
      "|    learning_rate        | 9.62e-05    |\n",
      "|    loss                 | -0.0151     |\n",
      "|    n_updates            | 6060        |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.3887117  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15         |\n",
      "|    ep_rew_mean          | 1.35       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 204        |\n",
      "|    time_elapsed         | 10322      |\n",
      "|    total_timesteps      | 153000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01342974 |\n",
      "|    clip_fraction        | 0.136      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.906     |\n",
      "|    explained_variance   | 0.0537     |\n",
      "|    learning_rate        | 9.62e-05   |\n",
      "|    loss                 | -0.00503   |\n",
      "|    n_updates            | 6090       |\n",
      "|    policy_gradient_loss | -0.0236    |\n",
      "|    value_loss           | 0.135      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4140139   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 205         |\n",
      "|    time_elapsed         | 10371       |\n",
      "|    total_timesteps      | 153750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013701202 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.878      |\n",
      "|    explained_variance   | -0.0754     |\n",
      "|    learning_rate        | 9.62e-05    |\n",
      "|    loss                 | -0.0155     |\n",
      "|    n_updates            | 6120        |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    value_loss           | 0.118       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3719791   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 206         |\n",
      "|    time_elapsed         | 10421       |\n",
      "|    total_timesteps      | 154500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012075178 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.899      |\n",
      "|    explained_variance   | -0.0249     |\n",
      "|    learning_rate        | 9.62e-05    |\n",
      "|    loss                 | -0.0304     |\n",
      "|    n_updates            | 6150        |\n",
      "|    policy_gradient_loss | -0.0253     |\n",
      "|    value_loss           | 0.0792      |\n",
      "-----------------------------------------\n",
      "New best mean reward: 1.515344262123108. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=155000, episode_reward=1.61 +/- 0.02\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.61        |\n",
      "| mean_reward             | 1.5153443   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 155000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010491401 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.885      |\n",
      "|    explained_variance   | 0.0775      |\n",
      "|    learning_rate        | 9.61e-05    |\n",
      "|    loss                 | -0.041      |\n",
      "|    n_updates            | 6180        |\n",
      "|    policy_gradient_loss | -0.0233     |\n",
      "|    value_loss           | 0.0487      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.47     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 207      |\n",
      "|    time_elapsed    | 10476    |\n",
      "|    total_timesteps | 155250   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| mean_reward             | 1.4181403    |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15.1         |\n",
      "|    ep_rew_mean          | 1.43         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 208          |\n",
      "|    time_elapsed         | 10527        |\n",
      "|    total_timesteps      | 156000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0102106035 |\n",
      "|    clip_fraction        | 0.119        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.92        |\n",
      "|    explained_variance   | 0.0428       |\n",
      "|    learning_rate        | 9.61e-05     |\n",
      "|    loss                 | -0.0339      |\n",
      "|    n_updates            | 6210         |\n",
      "|    policy_gradient_loss | -0.0235      |\n",
      "|    value_loss           | 0.0627       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3691568   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 209         |\n",
      "|    time_elapsed         | 10576       |\n",
      "|    total_timesteps      | 156750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012794192 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.903      |\n",
      "|    explained_variance   | 0.00494     |\n",
      "|    learning_rate        | 9.61e-05    |\n",
      "|    loss                 | -0.0284     |\n",
      "|    n_updates            | 6240        |\n",
      "|    policy_gradient_loss | -0.024      |\n",
      "|    value_loss           | 0.0846      |\n",
      "-----------------------------------------\n",
      "New best mean reward: 1.516080617904663. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.5160806   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.47        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 210         |\n",
      "|    time_elapsed         | 10626       |\n",
      "|    total_timesteps      | 157500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010198768 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.897      |\n",
      "|    explained_variance   | 0.129       |\n",
      "|    learning_rate        | 9.61e-05    |\n",
      "|    loss                 | -0.0293     |\n",
      "|    n_updates            | 6270        |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    value_loss           | 0.0567      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4904964   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 211         |\n",
      "|    time_elapsed         | 10676       |\n",
      "|    total_timesteps      | 158250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018292166 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.898      |\n",
      "|    explained_variance   | 0.0387      |\n",
      "|    learning_rate        | 9.61e-05    |\n",
      "|    loss                 | -0.0477     |\n",
      "|    n_updates            | 6300        |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    value_loss           | 0.0463      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3690476   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 212         |\n",
      "|    time_elapsed         | 10726       |\n",
      "|    total_timesteps      | 159000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015688766 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.881      |\n",
      "|    explained_variance   | -0.000369   |\n",
      "|    learning_rate        | 9.6e-05     |\n",
      "|    loss                 | -0.0163     |\n",
      "|    n_updates            | 6330        |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    value_loss           | 0.111       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4749918   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.47        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 213         |\n",
      "|    time_elapsed         | 10776       |\n",
      "|    total_timesteps      | 159750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013846759 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.902      |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 9.6e-05     |\n",
      "|    loss                 | -0.0505     |\n",
      "|    n_updates            | 6360        |\n",
      "|    policy_gradient_loss | -0.0221     |\n",
      "|    value_loss           | 0.0298      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=160000, episode_reward=1.60 +/- 0.02\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.6         |\n",
      "| mean_reward             | 1.4726223   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 160000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016756365 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.886      |\n",
      "|    explained_variance   | 0.0613      |\n",
      "|    learning_rate        | 9.6e-05     |\n",
      "|    loss                 | -0.0356     |\n",
      "|    n_updates            | 6390        |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    value_loss           | 0.0712      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.45     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 214      |\n",
      "|    time_elapsed    | 10831    |\n",
      "|    total_timesteps | 160500   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.4798337  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15.1       |\n",
      "|    ep_rew_mean          | 1.47       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 215        |\n",
      "|    time_elapsed         | 10881      |\n",
      "|    total_timesteps      | 161250     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01176489 |\n",
      "|    clip_fraction        | 0.16       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.89      |\n",
      "|    explained_variance   | -0.0663    |\n",
      "|    learning_rate        | 9.6e-05    |\n",
      "|    loss                 | -0.0367    |\n",
      "|    n_updates            | 6420       |\n",
      "|    policy_gradient_loss | -0.0215    |\n",
      "|    value_loss           | 0.0597     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.458223    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.5         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 216         |\n",
      "|    time_elapsed         | 10931       |\n",
      "|    total_timesteps      | 162000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010690358 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.909      |\n",
      "|    explained_variance   | 0.122       |\n",
      "|    learning_rate        | 9.6e-05     |\n",
      "|    loss                 | -0.0408     |\n",
      "|    n_updates            | 6450        |\n",
      "|    policy_gradient_loss | -0.0246     |\n",
      "|    value_loss           | 0.0616      |\n",
      "-----------------------------------------\n",
      "New best mean reward: 1.556127905845642. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.5561279   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 217         |\n",
      "|    time_elapsed         | 10980       |\n",
      "|    total_timesteps      | 162750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025473012 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.92       |\n",
      "|    explained_variance   | -0.0838     |\n",
      "|    learning_rate        | 9.6e-05     |\n",
      "|    loss                 | -0.0632     |\n",
      "|    n_updates            | 6480        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    value_loss           | 0.0244      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2415879   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 218         |\n",
      "|    time_elapsed         | 11030       |\n",
      "|    total_timesteps      | 163500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011270952 |\n",
      "|    clip_fraction        | 0.0825      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.893      |\n",
      "|    explained_variance   | 0.0126      |\n",
      "|    learning_rate        | 9.59e-05    |\n",
      "|    loss                 | 0.0109      |\n",
      "|    n_updates            | 6510        |\n",
      "|    policy_gradient_loss | -0.0229     |\n",
      "|    value_loss           | 0.17        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4680488   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 219         |\n",
      "|    time_elapsed         | 11080       |\n",
      "|    total_timesteps      | 164250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011660918 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.905      |\n",
      "|    explained_variance   | 0.0153      |\n",
      "|    learning_rate        | 9.59e-05    |\n",
      "|    loss                 | -0.0342     |\n",
      "|    n_updates            | 6540        |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    value_loss           | 0.0658      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=165000, episode_reward=1.59 +/- 0.03\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.59        |\n",
      "| mean_reward             | 1.3791606   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 165000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011676419 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.907      |\n",
      "|    explained_variance   | -0.00433    |\n",
      "|    learning_rate        | 9.59e-05    |\n",
      "|    loss                 | -0.0118     |\n",
      "|    n_updates            | 6570        |\n",
      "|    policy_gradient_loss | -0.0214     |\n",
      "|    value_loss           | 0.112       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15       |\n",
      "|    ep_rew_mean     | 1.39     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 220      |\n",
      "|    time_elapsed    | 11135    |\n",
      "|    total_timesteps | 165000   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4227916   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.36        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 221         |\n",
      "|    time_elapsed         | 11185       |\n",
      "|    total_timesteps      | 165750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012721931 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.92       |\n",
      "|    explained_variance   | 0.0416      |\n",
      "|    learning_rate        | 9.59e-05    |\n",
      "|    loss                 | -0.0307     |\n",
      "|    n_updates            | 6600        |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    value_loss           | 0.0871      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3318301   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.31        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 222         |\n",
      "|    time_elapsed         | 11235       |\n",
      "|    total_timesteps      | 166500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012084808 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.918      |\n",
      "|    explained_variance   | -0.0353     |\n",
      "|    learning_rate        | 9.59e-05    |\n",
      "|    loss                 | -0.02       |\n",
      "|    n_updates            | 6630        |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    value_loss           | 0.11        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3285317   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 223         |\n",
      "|    time_elapsed         | 11285       |\n",
      "|    total_timesteps      | 167250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021379638 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.922      |\n",
      "|    explained_variance   | 0.0151      |\n",
      "|    learning_rate        | 9.58e-05    |\n",
      "|    loss                 | -0.0136     |\n",
      "|    n_updates            | 6660        |\n",
      "|    policy_gradient_loss | -0.0325     |\n",
      "|    value_loss           | 0.143       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3955567   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 224         |\n",
      "|    time_elapsed         | 11335       |\n",
      "|    total_timesteps      | 168000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010906452 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.927      |\n",
      "|    explained_variance   | 0.0389      |\n",
      "|    learning_rate        | 9.58e-05    |\n",
      "|    loss                 | -0.0202     |\n",
      "|    n_updates            | 6690        |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    value_loss           | 0.107       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4780899   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 225         |\n",
      "|    time_elapsed         | 11384       |\n",
      "|    total_timesteps      | 168750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019895395 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.958      |\n",
      "|    explained_variance   | 0.035       |\n",
      "|    learning_rate        | 9.58e-05    |\n",
      "|    loss                 | -0.0351     |\n",
      "|    n_updates            | 6720        |\n",
      "|    policy_gradient_loss | -0.0234     |\n",
      "|    value_loss           | 0.0665      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.3510861  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15         |\n",
      "|    ep_rew_mean          | 1.38       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 226        |\n",
      "|    time_elapsed         | 11434      |\n",
      "|    total_timesteps      | 169500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01088319 |\n",
      "|    clip_fraction        | 0.113      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.912     |\n",
      "|    explained_variance   | 0.0459     |\n",
      "|    learning_rate        | 9.58e-05   |\n",
      "|    loss                 | -0.0253    |\n",
      "|    n_updates            | 6750       |\n",
      "|    policy_gradient_loss | -0.0255    |\n",
      "|    value_loss           | 0.0944     |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=170000, episode_reward=1.59 +/- 0.02\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.59        |\n",
      "| mean_reward             | 1.3965051   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 170000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009933837 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.924      |\n",
      "|    explained_variance   | 0.0267      |\n",
      "|    learning_rate        | 9.58e-05    |\n",
      "|    loss                 | -0.0288     |\n",
      "|    n_updates            | 6780        |\n",
      "|    policy_gradient_loss | -0.0237     |\n",
      "|    value_loss           | 0.086       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.37     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 227      |\n",
      "|    time_elapsed    | 11489    |\n",
      "|    total_timesteps | 170250   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4105849   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.35        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 228         |\n",
      "|    time_elapsed         | 11539       |\n",
      "|    total_timesteps      | 171000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012147878 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.918      |\n",
      "|    explained_variance   | -0.000698   |\n",
      "|    learning_rate        | 9.57e-05    |\n",
      "|    loss                 | -0.0182     |\n",
      "|    n_updates            | 6810        |\n",
      "|    policy_gradient_loss | -0.0233     |\n",
      "|    value_loss           | 0.109       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2772285   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 229         |\n",
      "|    time_elapsed         | 11589       |\n",
      "|    total_timesteps      | 171750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009509228 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.915      |\n",
      "|    explained_variance   | -0.0791     |\n",
      "|    learning_rate        | 9.57e-05    |\n",
      "|    loss                 | -0.0183     |\n",
      "|    n_updates            | 6840        |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    value_loss           | 0.112       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3041887   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.38        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 230         |\n",
      "|    time_elapsed         | 11639       |\n",
      "|    total_timesteps      | 172500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009721584 |\n",
      "|    clip_fraction        | 0.0844      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.936      |\n",
      "|    explained_variance   | -0.00568    |\n",
      "|    learning_rate        | 9.57e-05    |\n",
      "|    loss                 | -0.016      |\n",
      "|    n_updates            | 6870        |\n",
      "|    policy_gradient_loss | -0.0219     |\n",
      "|    value_loss           | 0.106       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.5087537   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.36        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 231         |\n",
      "|    time_elapsed         | 11689       |\n",
      "|    total_timesteps      | 173250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009809567 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.949      |\n",
      "|    explained_variance   | 0.0618      |\n",
      "|    learning_rate        | 9.57e-05    |\n",
      "|    loss                 | -0.0299     |\n",
      "|    n_updates            | 6900        |\n",
      "|    policy_gradient_loss | -0.0236     |\n",
      "|    value_loss           | 0.0737      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2113701   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.32        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 232         |\n",
      "|    time_elapsed         | 11739       |\n",
      "|    total_timesteps      | 174000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013231475 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.96       |\n",
      "|    explained_variance   | 0.0209      |\n",
      "|    learning_rate        | 9.57e-05    |\n",
      "|    loss                 | -0.0209     |\n",
      "|    n_updates            | 6930        |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    value_loss           | 0.114       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3982064   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.4         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 233         |\n",
      "|    time_elapsed         | 11789       |\n",
      "|    total_timesteps      | 174750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012815274 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.951      |\n",
      "|    explained_variance   | 0.0441      |\n",
      "|    learning_rate        | 9.57e-05    |\n",
      "|    loss                 | -0.0327     |\n",
      "|    n_updates            | 6960        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    value_loss           | 0.0997      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=175000, episode_reward=1.59 +/- 0.04\n",
      "Episode length: 15.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 15           |\n",
      "|    mean_reward          | 1.59         |\n",
      "| mean_reward             | 1.465713     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 175000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0143056605 |\n",
      "|    clip_fraction        | 0.12         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.94        |\n",
      "|    explained_variance   | -0.0642      |\n",
      "|    learning_rate        | 9.56e-05     |\n",
      "|    loss                 | -0.048       |\n",
      "|    n_updates            | 6990         |\n",
      "|    policy_gradient_loss | -0.0223      |\n",
      "|    value_loss           | 0.0475       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.45     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 234      |\n",
      "|    time_elapsed    | 11843    |\n",
      "|    total_timesteps | 175500   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.4224738  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15.1       |\n",
      "|    ep_rew_mean          | 1.41       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 235        |\n",
      "|    time_elapsed         | 11893      |\n",
      "|    total_timesteps      | 176250     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01124909 |\n",
      "|    clip_fraction        | 0.0962     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.964     |\n",
      "|    explained_variance   | -0.0224    |\n",
      "|    learning_rate        | 9.56e-05   |\n",
      "|    loss                 | -0.0326    |\n",
      "|    n_updates            | 7020       |\n",
      "|    policy_gradient_loss | -0.0206    |\n",
      "|    value_loss           | 0.0684     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| mean_reward             | 1.3844043    |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15           |\n",
      "|    ep_rew_mean          | 1.4          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 236          |\n",
      "|    time_elapsed         | 11943        |\n",
      "|    total_timesteps      | 177000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0142474165 |\n",
      "|    clip_fraction        | 0.117        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.975       |\n",
      "|    explained_variance   | -0.0164      |\n",
      "|    learning_rate        | 9.56e-05     |\n",
      "|    loss                 | -0.0413      |\n",
      "|    n_updates            | 7050         |\n",
      "|    policy_gradient_loss | -0.0264      |\n",
      "|    value_loss           | 0.074        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4446831   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 237         |\n",
      "|    time_elapsed         | 11993       |\n",
      "|    total_timesteps      | 177750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015891932 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.99       |\n",
      "|    explained_variance   | 0.0912      |\n",
      "|    learning_rate        | 9.56e-05    |\n",
      "|    loss                 | -0.0464     |\n",
      "|    n_updates            | 7080        |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    value_loss           | 0.0518      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4059395   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 238         |\n",
      "|    time_elapsed         | 12043       |\n",
      "|    total_timesteps      | 178500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012744733 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.976      |\n",
      "|    explained_variance   | 0.074       |\n",
      "|    learning_rate        | 9.56e-05    |\n",
      "|    loss                 | -0.0465     |\n",
      "|    n_updates            | 7110        |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    value_loss           | 0.0561      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.360209    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 239         |\n",
      "|    time_elapsed         | 12093       |\n",
      "|    total_timesteps      | 179250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011010368 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.998      |\n",
      "|    explained_variance   | 0.0599      |\n",
      "|    learning_rate        | 9.55e-05    |\n",
      "|    loss                 | -0.0293     |\n",
      "|    n_updates            | 7140        |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    value_loss           | 0.0908      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=180000, episode_reward=1.58 +/- 0.02\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.58        |\n",
      "| mean_reward             | 1.3809696   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 180000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011495413 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.976      |\n",
      "|    explained_variance   | 0.0175      |\n",
      "|    learning_rate        | 9.55e-05    |\n",
      "|    loss                 | -0.0456     |\n",
      "|    n_updates            | 7170        |\n",
      "|    policy_gradient_loss | -0.0252     |\n",
      "|    value_loss           | 0.0566      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15       |\n",
      "|    ep_rew_mean     | 1.37     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 240      |\n",
      "|    time_elapsed    | 12148    |\n",
      "|    total_timesteps | 180000   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.3339542  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15.1       |\n",
      "|    ep_rew_mean          | 1.38       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 241        |\n",
      "|    time_elapsed         | 12197      |\n",
      "|    total_timesteps      | 180750     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01496594 |\n",
      "|    clip_fraction        | 0.149      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.973     |\n",
      "|    explained_variance   | 0.0212     |\n",
      "|    learning_rate        | 9.55e-05   |\n",
      "|    loss                 | -0.0146    |\n",
      "|    n_updates            | 7200       |\n",
      "|    policy_gradient_loss | -0.0286    |\n",
      "|    value_loss           | 0.13       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4744896   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 242         |\n",
      "|    time_elapsed         | 12247       |\n",
      "|    total_timesteps      | 181500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013525908 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.963      |\n",
      "|    explained_variance   | 0.0978      |\n",
      "|    learning_rate        | 9.55e-05    |\n",
      "|    loss                 | -0.0423     |\n",
      "|    n_updates            | 7230        |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    value_loss           | 0.0658      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3934431   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 243         |\n",
      "|    time_elapsed         | 12297       |\n",
      "|    total_timesteps      | 182250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014280841 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.962      |\n",
      "|    explained_variance   | 0.0454      |\n",
      "|    learning_rate        | 9.55e-05    |\n",
      "|    loss                 | -0.0407     |\n",
      "|    n_updates            | 7260        |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    value_loss           | 0.0694      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.5014825   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 244         |\n",
      "|    time_elapsed         | 12347       |\n",
      "|    total_timesteps      | 183000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012296559 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.967      |\n",
      "|    explained_variance   | 0.0566      |\n",
      "|    learning_rate        | 9.54e-05    |\n",
      "|    loss                 | -0.0403     |\n",
      "|    n_updates            | 7290        |\n",
      "|    policy_gradient_loss | -0.0223     |\n",
      "|    value_loss           | 0.0447      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3907232   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.38        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 245         |\n",
      "|    time_elapsed         | 12397       |\n",
      "|    total_timesteps      | 183750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012218963 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.941      |\n",
      "|    explained_variance   | 0.0087      |\n",
      "|    learning_rate        | 9.54e-05    |\n",
      "|    loss                 | -0.0188     |\n",
      "|    n_updates            | 7320        |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    value_loss           | 0.105       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.344621    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.37        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 246         |\n",
      "|    time_elapsed         | 12447       |\n",
      "|    total_timesteps      | 184500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017981555 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.945      |\n",
      "|    explained_variance   | 0.0195      |\n",
      "|    learning_rate        | 9.54e-05    |\n",
      "|    loss                 | -0.0107     |\n",
      "|    n_updates            | 7350        |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=185000, episode_reward=1.57 +/- 0.03\n",
      "Episode length: 15.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 15         |\n",
      "|    mean_reward          | 1.57       |\n",
      "| mean_reward             | 1.4415565  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 185000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01264763 |\n",
      "|    clip_fraction        | 0.138      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.935     |\n",
      "|    explained_variance   | -0.049     |\n",
      "|    learning_rate        | 9.54e-05   |\n",
      "|    loss                 | -0.0446    |\n",
      "|    n_updates            | 7380       |\n",
      "|    policy_gradient_loss | -0.0276    |\n",
      "|    value_loss           | 0.0725     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.43     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 247      |\n",
      "|    time_elapsed    | 12502    |\n",
      "|    total_timesteps | 185250   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4531245   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.46        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 248         |\n",
      "|    time_elapsed         | 12552       |\n",
      "|    total_timesteps      | 186000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015709583 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.929      |\n",
      "|    explained_variance   | 0.0141      |\n",
      "|    learning_rate        | 9.54e-05    |\n",
      "|    loss                 | -0.0484     |\n",
      "|    n_updates            | 7410        |\n",
      "|    policy_gradient_loss | -0.0236     |\n",
      "|    value_loss           | 0.0395      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4291115   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 249         |\n",
      "|    time_elapsed         | 12602       |\n",
      "|    total_timesteps      | 186750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017543137 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.929      |\n",
      "|    explained_variance   | -0.0118     |\n",
      "|    learning_rate        | 9.54e-05    |\n",
      "|    loss                 | -0.0473     |\n",
      "|    n_updates            | 7440        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    value_loss           | 0.056       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.3121806  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15         |\n",
      "|    ep_rew_mean          | 1.36       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 250        |\n",
      "|    time_elapsed         | 12652      |\n",
      "|    total_timesteps      | 187500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01244192 |\n",
      "|    clip_fraction        | 0.116      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.942     |\n",
      "|    explained_variance   | -0.0322    |\n",
      "|    learning_rate        | 9.53e-05   |\n",
      "|    loss                 | -0.0399    |\n",
      "|    n_updates            | 7470       |\n",
      "|    policy_gradient_loss | -0.0247    |\n",
      "|    value_loss           | 0.0663     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4169867   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.36        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 251         |\n",
      "|    time_elapsed         | 12702       |\n",
      "|    total_timesteps      | 188250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013708254 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.94       |\n",
      "|    explained_variance   | 0.0265      |\n",
      "|    learning_rate        | 9.53e-05    |\n",
      "|    loss                 | -0.0224     |\n",
      "|    n_updates            | 7500        |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    value_loss           | 0.109       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.411821    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 252         |\n",
      "|    time_elapsed         | 12752       |\n",
      "|    total_timesteps      | 189000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013864636 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.956      |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 9.53e-05    |\n",
      "|    loss                 | -0.0366     |\n",
      "|    n_updates            | 7530        |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    value_loss           | 0.0786      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3304329   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.36        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 253         |\n",
      "|    time_elapsed         | 12802       |\n",
      "|    total_timesteps      | 189750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015082722 |\n",
      "|    clip_fraction        | 0.0954      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.933      |\n",
      "|    explained_variance   | 0.0503      |\n",
      "|    learning_rate        | 9.53e-05    |\n",
      "|    loss                 | -0.0364     |\n",
      "|    n_updates            | 7560        |\n",
      "|    policy_gradient_loss | -0.0252     |\n",
      "|    value_loss           | 0.0729      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=190000, episode_reward=1.61 +/- 0.01\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.61        |\n",
      "| mean_reward             | 1.3901124   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 190000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013590465 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.941      |\n",
      "|    explained_variance   | -0.0213     |\n",
      "|    learning_rate        | 9.53e-05    |\n",
      "|    loss                 | 0.0257      |\n",
      "|    n_updates            | 7590        |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    value_loss           | 0.206       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.34     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 254      |\n",
      "|    time_elapsed    | 12856    |\n",
      "|    total_timesteps | 190500   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3741691   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 255         |\n",
      "|    time_elapsed         | 12907       |\n",
      "|    total_timesteps      | 191250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012819067 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.908      |\n",
      "|    explained_variance   | -0.0132     |\n",
      "|    learning_rate        | 9.52e-05    |\n",
      "|    loss                 | -0.0173     |\n",
      "|    n_updates            | 7620        |\n",
      "|    policy_gradient_loss | -0.0253     |\n",
      "|    value_loss           | 0.105       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4565189   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 256         |\n",
      "|    time_elapsed         | 12956       |\n",
      "|    total_timesteps      | 192000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016438663 |\n",
      "|    clip_fraction        | 0.0941      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.934      |\n",
      "|    explained_variance   | 0.0199      |\n",
      "|    learning_rate        | 9.52e-05    |\n",
      "|    loss                 | -0.028      |\n",
      "|    n_updates            | 7650        |\n",
      "|    policy_gradient_loss | -0.0222     |\n",
      "|    value_loss           | 0.0843      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.4331151  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15         |\n",
      "|    ep_rew_mean          | 1.44       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 257        |\n",
      "|    time_elapsed         | 13006      |\n",
      "|    total_timesteps      | 192750     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01036777 |\n",
      "|    clip_fraction        | 0.105      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.926     |\n",
      "|    explained_variance   | 0.104      |\n",
      "|    learning_rate        | 9.52e-05   |\n",
      "|    loss                 | -0.0457    |\n",
      "|    n_updates            | 7680       |\n",
      "|    policy_gradient_loss | -0.0223    |\n",
      "|    value_loss           | 0.045      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4474181   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 258         |\n",
      "|    time_elapsed         | 13056       |\n",
      "|    total_timesteps      | 193500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008779967 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.919      |\n",
      "|    explained_variance   | -0.0191     |\n",
      "|    learning_rate        | 9.52e-05    |\n",
      "|    loss                 | -0.0402     |\n",
      "|    n_updates            | 7710        |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    value_loss           | 0.0683      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4870158   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.46        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 259         |\n",
      "|    time_elapsed         | 13106       |\n",
      "|    total_timesteps      | 194250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012460546 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.938      |\n",
      "|    explained_variance   | 0.0749      |\n",
      "|    learning_rate        | 9.52e-05    |\n",
      "|    loss                 | -0.0507     |\n",
      "|    n_updates            | 7740        |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    value_loss           | 0.037       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=195000, episode_reward=1.58 +/- 0.03\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.58        |\n",
      "| mean_reward             | 1.4473991   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 195000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011549128 |\n",
      "|    clip_fraction        | 0.0998      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.915      |\n",
      "|    explained_variance   | 0.0405      |\n",
      "|    learning_rate        | 9.51e-05    |\n",
      "|    loss                 | -0.0197     |\n",
      "|    n_updates            | 7770        |\n",
      "|    policy_gradient_loss | -0.0227     |\n",
      "|    value_loss           | 0.0969      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15       |\n",
      "|    ep_rew_mean     | 1.42     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 260      |\n",
      "|    time_elapsed    | 13161    |\n",
      "|    total_timesteps | 195000   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4168319   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 261         |\n",
      "|    time_elapsed         | 13211       |\n",
      "|    total_timesteps      | 195750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024261773 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.932      |\n",
      "|    explained_variance   | 0.0176      |\n",
      "|    learning_rate        | 9.51e-05    |\n",
      "|    loss                 | -0.0381     |\n",
      "|    n_updates            | 7800        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    value_loss           | 0.0733      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3646103   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.4         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 262         |\n",
      "|    time_elapsed         | 13261       |\n",
      "|    total_timesteps      | 196500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010279373 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.943      |\n",
      "|    explained_variance   | 0.00074     |\n",
      "|    learning_rate        | 9.51e-05    |\n",
      "|    loss                 | -0.0338     |\n",
      "|    n_updates            | 7830        |\n",
      "|    policy_gradient_loss | -0.0246     |\n",
      "|    value_loss           | 0.081       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.451635    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 263         |\n",
      "|    time_elapsed         | 13311       |\n",
      "|    total_timesteps      | 197250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013076041 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.944      |\n",
      "|    explained_variance   | 0.0384      |\n",
      "|    learning_rate        | 9.51e-05    |\n",
      "|    loss                 | -0.0368     |\n",
      "|    n_updates            | 7860        |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    value_loss           | 0.075       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.4397507  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15         |\n",
      "|    ep_rew_mean          | 1.44       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 264        |\n",
      "|    time_elapsed         | 13361      |\n",
      "|    total_timesteps      | 198000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01592168 |\n",
      "|    clip_fraction        | 0.114      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.951     |\n",
      "|    explained_variance   | -0.0186    |\n",
      "|    learning_rate        | 9.51e-05   |\n",
      "|    loss                 | -0.0268    |\n",
      "|    n_updates            | 7890       |\n",
      "|    policy_gradient_loss | -0.0244    |\n",
      "|    value_loss           | 0.088      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3840579   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 265         |\n",
      "|    time_elapsed         | 13411       |\n",
      "|    total_timesteps      | 198750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021734731 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.951      |\n",
      "|    explained_variance   | 0.0517      |\n",
      "|    learning_rate        | 9.51e-05    |\n",
      "|    loss                 | -0.0374     |\n",
      "|    n_updates            | 7920        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    value_loss           | 0.0809      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.511624    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 266         |\n",
      "|    time_elapsed         | 13461       |\n",
      "|    total_timesteps      | 199500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011662507 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.943      |\n",
      "|    explained_variance   | 0.102       |\n",
      "|    learning_rate        | 9.5e-05     |\n",
      "|    loss                 | -0.0507     |\n",
      "|    n_updates            | 7950        |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    value_loss           | 0.0476      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=200000, episode_reward=1.58 +/- 0.04\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.58        |\n",
      "| mean_reward             | 1.3860761   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 200000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013954149 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.952      |\n",
      "|    explained_variance   | -0.0247     |\n",
      "|    learning_rate        | 9.5e-05     |\n",
      "|    loss                 | -0.0342     |\n",
      "|    n_updates            | 7980        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    value_loss           | 0.0847      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.42     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 267      |\n",
      "|    time_elapsed    | 13516    |\n",
      "|    total_timesteps | 200250   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4642528   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.49        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 268         |\n",
      "|    time_elapsed         | 13566       |\n",
      "|    total_timesteps      | 201000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012187808 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.93       |\n",
      "|    explained_variance   | 0.0287      |\n",
      "|    learning_rate        | 9.5e-05     |\n",
      "|    loss                 | -0.0466     |\n",
      "|    n_updates            | 8010        |\n",
      "|    policy_gradient_loss | -0.0233     |\n",
      "|    value_loss           | 0.0485      |\n",
      "-----------------------------------------\n",
      "New best mean reward: 1.5730845928192139. Model saved to ./logs/PPO_20240830-021915/best_model_20240830-021915\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.5730846   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.46        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 269         |\n",
      "|    time_elapsed         | 13616       |\n",
      "|    total_timesteps      | 201750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026925959 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.951      |\n",
      "|    explained_variance   | 0.521       |\n",
      "|    learning_rate        | 9.5e-05     |\n",
      "|    loss                 | -0.079      |\n",
      "|    n_updates            | 8040        |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    value_loss           | 0.00384     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3298743   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.37        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 270         |\n",
      "|    time_elapsed         | 13665       |\n",
      "|    total_timesteps      | 202500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034756705 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.959      |\n",
      "|    explained_variance   | -0.0357     |\n",
      "|    learning_rate        | 9.5e-05     |\n",
      "|    loss                 | -0.00756    |\n",
      "|    n_updates            | 8070        |\n",
      "|    policy_gradient_loss | -0.0332     |\n",
      "|    value_loss           | 0.155       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| mean_reward             | 1.3497626    |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15           |\n",
      "|    ep_rew_mean          | 1.36         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 271          |\n",
      "|    time_elapsed         | 13715        |\n",
      "|    total_timesteps      | 203250       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0111108525 |\n",
      "|    clip_fraction        | 0.127        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.958       |\n",
      "|    explained_variance   | -0.0166      |\n",
      "|    learning_rate        | 9.49e-05     |\n",
      "|    loss                 | -0.0384      |\n",
      "|    n_updates            | 8100         |\n",
      "|    policy_gradient_loss | -0.0261      |\n",
      "|    value_loss           | 0.0774       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3362181   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.32        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 272         |\n",
      "|    time_elapsed         | 13765       |\n",
      "|    total_timesteps      | 204000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015330544 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.969      |\n",
      "|    explained_variance   | 0.0117      |\n",
      "|    learning_rate        | 9.49e-05    |\n",
      "|    loss                 | -0.0183     |\n",
      "|    n_updates            | 8130        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    value_loss           | 0.119       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.3013273  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15         |\n",
      "|    ep_rew_mean          | 1.34       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 273        |\n",
      "|    time_elapsed         | 13815      |\n",
      "|    total_timesteps      | 204750     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01279699 |\n",
      "|    clip_fraction        | 0.141      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.929     |\n",
      "|    explained_variance   | 0.0451     |\n",
      "|    learning_rate        | 9.49e-05   |\n",
      "|    loss                 | -0.0217    |\n",
      "|    n_updates            | 8160       |\n",
      "|    policy_gradient_loss | -0.0324    |\n",
      "|    value_loss           | 0.128      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=205000, episode_reward=1.57 +/- 0.02\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.57        |\n",
      "| mean_reward             | 1.3692163   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 205000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011185514 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.924      |\n",
      "|    explained_variance   | 0.0127      |\n",
      "|    learning_rate        | 9.49e-05    |\n",
      "|    loss                 | -0.0385     |\n",
      "|    n_updates            | 8190        |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    value_loss           | 0.0776      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.38     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 274      |\n",
      "|    time_elapsed    | 13870    |\n",
      "|    total_timesteps | 205500   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4354717   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 275         |\n",
      "|    time_elapsed         | 13920       |\n",
      "|    total_timesteps      | 206250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012045659 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.929      |\n",
      "|    explained_variance   | 0.00793     |\n",
      "|    learning_rate        | 9.49e-05    |\n",
      "|    loss                 | -0.0328     |\n",
      "|    n_updates            | 8220        |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    value_loss           | 0.0883      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4134927   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 276         |\n",
      "|    time_elapsed         | 13970       |\n",
      "|    total_timesteps      | 207000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012276983 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.926      |\n",
      "|    explained_variance   | -0.135      |\n",
      "|    learning_rate        | 9.48e-05    |\n",
      "|    loss                 | -0.0448     |\n",
      "|    n_updates            | 8250        |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    value_loss           | 0.0648      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3418665   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.36        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 277         |\n",
      "|    time_elapsed         | 14020       |\n",
      "|    total_timesteps      | 207750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013461875 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.932      |\n",
      "|    explained_variance   | 0.0435      |\n",
      "|    learning_rate        | 9.48e-05    |\n",
      "|    loss                 | -0.0259     |\n",
      "|    n_updates            | 8280        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    value_loss           | 0.098       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| mean_reward             | 1.3372597    |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15           |\n",
      "|    ep_rew_mean          | 1.27         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 278          |\n",
      "|    time_elapsed         | 14070        |\n",
      "|    total_timesteps      | 208500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0154388305 |\n",
      "|    clip_fraction        | 0.151        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.92        |\n",
      "|    explained_variance   | -0.0403      |\n",
      "|    learning_rate        | 9.48e-05     |\n",
      "|    loss                 | -0.0135      |\n",
      "|    n_updates            | 8310         |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    value_loss           | 0.132        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2820888   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.35        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 279         |\n",
      "|    time_elapsed         | 14120       |\n",
      "|    total_timesteps      | 209250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011944883 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.92       |\n",
      "|    explained_variance   | -0.0138     |\n",
      "|    learning_rate        | 9.48e-05    |\n",
      "|    loss                 | 0.00263     |\n",
      "|    n_updates            | 8340        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    value_loss           | 0.172       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=210000, episode_reward=1.60 +/- 0.00\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.6         |\n",
      "| mean_reward             | 1.4580331   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 210000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014373408 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.868      |\n",
      "|    explained_variance   | 0.0504      |\n",
      "|    learning_rate        | 9.48e-05    |\n",
      "|    loss                 | -0.0467     |\n",
      "|    n_updates            | 8370        |\n",
      "|    policy_gradient_loss | -0.0248     |\n",
      "|    value_loss           | 0.0499      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15       |\n",
      "|    ep_rew_mean     | 1.44     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 280      |\n",
      "|    time_elapsed    | 14175    |\n",
      "|    total_timesteps | 210000   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4036417   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 281         |\n",
      "|    time_elapsed         | 14225       |\n",
      "|    total_timesteps      | 210750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011231834 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.905      |\n",
      "|    explained_variance   | 0.0127      |\n",
      "|    learning_rate        | 9.47e-05    |\n",
      "|    loss                 | -0.0225     |\n",
      "|    n_updates            | 8400        |\n",
      "|    policy_gradient_loss | -0.0229     |\n",
      "|    value_loss           | 0.0913      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.435305    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.37        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 282         |\n",
      "|    time_elapsed         | 14275       |\n",
      "|    total_timesteps      | 211500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012774147 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.913      |\n",
      "|    explained_variance   | 0.0428      |\n",
      "|    learning_rate        | 9.47e-05    |\n",
      "|    loss                 | -0.0429     |\n",
      "|    n_updates            | 8430        |\n",
      "|    policy_gradient_loss | -0.0253     |\n",
      "|    value_loss           | 0.0609      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3285252   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 283         |\n",
      "|    time_elapsed         | 14325       |\n",
      "|    total_timesteps      | 212250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013038886 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.883      |\n",
      "|    explained_variance   | 0.00871     |\n",
      "|    learning_rate        | 9.47e-05    |\n",
      "|    loss                 | -0.0208     |\n",
      "|    n_updates            | 8460        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4333795   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 284         |\n",
      "|    time_elapsed         | 14375       |\n",
      "|    total_timesteps      | 213000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017338302 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.884      |\n",
      "|    explained_variance   | 0.0908      |\n",
      "|    learning_rate        | 9.47e-05    |\n",
      "|    loss                 | -0.0372     |\n",
      "|    n_updates            | 8490        |\n",
      "|    policy_gradient_loss | -0.0251     |\n",
      "|    value_loss           | 0.0704      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4130435   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 285         |\n",
      "|    time_elapsed         | 14425       |\n",
      "|    total_timesteps      | 213750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021939335 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.87       |\n",
      "|    explained_variance   | 0.0813      |\n",
      "|    learning_rate        | 9.47e-05    |\n",
      "|    loss                 | -0.0363     |\n",
      "|    n_updates            | 8520        |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    value_loss           | 0.0812      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4551142   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.46        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 286         |\n",
      "|    time_elapsed         | 14475       |\n",
      "|    total_timesteps      | 214500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016066149 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.854      |\n",
      "|    explained_variance   | 0.0371      |\n",
      "|    learning_rate        | 9.47e-05    |\n",
      "|    loss                 | -0.0428     |\n",
      "|    n_updates            | 8550        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    value_loss           | 0.0658      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=215000, episode_reward=1.60 +/- 0.03\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.6         |\n",
      "| mean_reward             | 1.4681774   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 215000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015105936 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.838      |\n",
      "|    explained_variance   | 0.0721      |\n",
      "|    learning_rate        | 9.46e-05    |\n",
      "|    loss                 | -0.0438     |\n",
      "|    n_updates            | 8580        |\n",
      "|    policy_gradient_loss | -0.0251     |\n",
      "|    value_loss           | 0.0509      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.45     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 287      |\n",
      "|    time_elapsed    | 14530    |\n",
      "|    total_timesteps | 215250   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3964665   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 288         |\n",
      "|    time_elapsed         | 14580       |\n",
      "|    total_timesteps      | 216000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015271448 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.855      |\n",
      "|    explained_variance   | 0.16        |\n",
      "|    learning_rate        | 9.46e-05    |\n",
      "|    loss                 | -0.0484     |\n",
      "|    n_updates            | 8610        |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    value_loss           | 0.0524      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.3437319  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15         |\n",
      "|    ep_rew_mean          | 1.34       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 289        |\n",
      "|    time_elapsed         | 14630      |\n",
      "|    total_timesteps      | 216750     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01498114 |\n",
      "|    clip_fraction        | 0.106      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.893     |\n",
      "|    explained_variance   | -0.102     |\n",
      "|    learning_rate        | 9.46e-05   |\n",
      "|    loss                 | -0.0205    |\n",
      "|    n_updates            | 8640       |\n",
      "|    policy_gradient_loss | -0.0285    |\n",
      "|    value_loss           | 0.121      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3270307   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.35        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 290         |\n",
      "|    time_elapsed         | 14680       |\n",
      "|    total_timesteps      | 217500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023967214 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.912      |\n",
      "|    explained_variance   | 0.111       |\n",
      "|    learning_rate        | 9.46e-05    |\n",
      "|    loss                 | -0.0366     |\n",
      "|    n_updates            | 8670        |\n",
      "|    policy_gradient_loss | -0.0337     |\n",
      "|    value_loss           | 0.106       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3780948   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 291         |\n",
      "|    time_elapsed         | 14730       |\n",
      "|    total_timesteps      | 218250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017987007 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.904      |\n",
      "|    explained_variance   | 0.0131      |\n",
      "|    learning_rate        | 9.46e-05    |\n",
      "|    loss                 | -0.0212     |\n",
      "|    n_updates            | 8700        |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    value_loss           | 0.111       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4998257   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.47        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 292         |\n",
      "|    time_elapsed         | 14780       |\n",
      "|    total_timesteps      | 219000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012203022 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.915      |\n",
      "|    explained_variance   | 0.0863      |\n",
      "|    learning_rate        | 9.45e-05    |\n",
      "|    loss                 | -0.0445     |\n",
      "|    n_updates            | 8730        |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    value_loss           | 0.069       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3781595   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 293         |\n",
      "|    time_elapsed         | 14830       |\n",
      "|    total_timesteps      | 219750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018645119 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.899      |\n",
      "|    explained_variance   | -0.229      |\n",
      "|    learning_rate        | 9.45e-05    |\n",
      "|    loss                 | -0.0515     |\n",
      "|    n_updates            | 8760        |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    value_loss           | 0.0395      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=220000, episode_reward=1.61 +/- 0.02\n",
      "Episode length: 15.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 15         |\n",
      "|    mean_reward          | 1.61       |\n",
      "| mean_reward             | 1.4612739  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 220000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01248399 |\n",
      "|    clip_fraction        | 0.117      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.906     |\n",
      "|    explained_variance   | -0.00751   |\n",
      "|    learning_rate        | 9.45e-05   |\n",
      "|    loss                 | -0.0167    |\n",
      "|    n_updates            | 8790       |\n",
      "|    policy_gradient_loss | -0.0251    |\n",
      "|    value_loss           | 0.122      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.4      |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 294      |\n",
      "|    time_elapsed    | 14885    |\n",
      "|    total_timesteps | 220500   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4737984   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.47        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 295         |\n",
      "|    time_elapsed         | 14935       |\n",
      "|    total_timesteps      | 221250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010026468 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.903      |\n",
      "|    explained_variance   | -0.0467     |\n",
      "|    learning_rate        | 9.45e-05    |\n",
      "|    loss                 | -0.0346     |\n",
      "|    n_updates            | 8820        |\n",
      "|    policy_gradient_loss | -0.0244     |\n",
      "|    value_loss           | 0.0692      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| mean_reward             | 1.3555406    |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15           |\n",
      "|    ep_rew_mean          | 1.4          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 296          |\n",
      "|    time_elapsed         | 14985        |\n",
      "|    total_timesteps      | 222000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0139438985 |\n",
      "|    clip_fraction        | 0.126        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.907       |\n",
      "|    explained_variance   | 0.0905       |\n",
      "|    learning_rate        | 9.45e-05     |\n",
      "|    loss                 | -0.0339      |\n",
      "|    n_updates            | 8850         |\n",
      "|    policy_gradient_loss | -0.0219      |\n",
      "|    value_loss           | 0.0655       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3938137   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 297         |\n",
      "|    time_elapsed         | 15035       |\n",
      "|    total_timesteps      | 222750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014495146 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.904      |\n",
      "|    explained_variance   | -0.00862    |\n",
      "|    learning_rate        | 9.45e-05    |\n",
      "|    loss                 | -0.0231     |\n",
      "|    n_updates            | 8880        |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    value_loss           | 0.113       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4405326   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 298         |\n",
      "|    time_elapsed         | 15085       |\n",
      "|    total_timesteps      | 223500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014105595 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.943      |\n",
      "|    explained_variance   | 0.068       |\n",
      "|    learning_rate        | 9.44e-05    |\n",
      "|    loss                 | -0.0466     |\n",
      "|    n_updates            | 8910        |\n",
      "|    policy_gradient_loss | -0.0196     |\n",
      "|    value_loss           | 0.0353      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4473495   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 299         |\n",
      "|    time_elapsed         | 15135       |\n",
      "|    total_timesteps      | 224250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009058197 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.913      |\n",
      "|    explained_variance   | 0.0369      |\n",
      "|    learning_rate        | 9.44e-05    |\n",
      "|    loss                 | -0.0226     |\n",
      "|    n_updates            | 8940        |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    value_loss           | 0.103       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=225000, episode_reward=1.60 +/- 0.04\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.6         |\n",
      "| mean_reward             | 1.4060624   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 225000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014623354 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.922      |\n",
      "|    explained_variance   | -0.112      |\n",
      "|    learning_rate        | 9.44e-05    |\n",
      "|    loss                 | -0.0296     |\n",
      "|    n_updates            | 8970        |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    value_loss           | 0.0889      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15       |\n",
      "|    ep_rew_mean     | 1.42     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 300      |\n",
      "|    time_elapsed    | 15190    |\n",
      "|    total_timesteps | 225000   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3699403   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 301         |\n",
      "|    time_elapsed         | 15240       |\n",
      "|    total_timesteps      | 225750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013739286 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.932      |\n",
      "|    explained_variance   | 0.122       |\n",
      "|    learning_rate        | 9.44e-05    |\n",
      "|    loss                 | -0.0428     |\n",
      "|    n_updates            | 9000        |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    value_loss           | 0.0601      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4069365   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 302         |\n",
      "|    time_elapsed         | 15290       |\n",
      "|    total_timesteps      | 226500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014854173 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.932      |\n",
      "|    explained_variance   | 0.00399     |\n",
      "|    learning_rate        | 9.44e-05    |\n",
      "|    loss                 | -0.0288     |\n",
      "|    n_updates            | 9030        |\n",
      "|    policy_gradient_loss | -0.023      |\n",
      "|    value_loss           | 0.0832      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3640534   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.38        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 303         |\n",
      "|    time_elapsed         | 15340       |\n",
      "|    total_timesteps      | 227250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017332852 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.947      |\n",
      "|    explained_variance   | -0.0244     |\n",
      "|    learning_rate        | 9.43e-05    |\n",
      "|    loss                 | -0.0236     |\n",
      "|    n_updates            | 9060        |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    value_loss           | 0.122       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| mean_reward             | 1.4267637    |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15           |\n",
      "|    ep_rew_mean          | 1.4          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 304          |\n",
      "|    time_elapsed         | 15390        |\n",
      "|    total_timesteps      | 228000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0102975415 |\n",
      "|    clip_fraction        | 0.137        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.919       |\n",
      "|    explained_variance   | 0.0399       |\n",
      "|    learning_rate        | 9.43e-05     |\n",
      "|    loss                 | -0.0326      |\n",
      "|    n_updates            | 9090         |\n",
      "|    policy_gradient_loss | -0.0278      |\n",
      "|    value_loss           | 0.0874       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4458183   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.46        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 305         |\n",
      "|    time_elapsed         | 15440       |\n",
      "|    total_timesteps      | 228750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016752463 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.921      |\n",
      "|    explained_variance   | 0.109       |\n",
      "|    learning_rate        | 9.43e-05    |\n",
      "|    loss                 | -0.0434     |\n",
      "|    n_updates            | 9120        |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    value_loss           | 0.075       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.5070087   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.48        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 306         |\n",
      "|    time_elapsed         | 15490       |\n",
      "|    total_timesteps      | 229500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016247291 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.917      |\n",
      "|    explained_variance   | -0.0943     |\n",
      "|    learning_rate        | 9.43e-05    |\n",
      "|    loss                 | -0.0386     |\n",
      "|    n_updates            | 9150        |\n",
      "|    policy_gradient_loss | -0.0196     |\n",
      "|    value_loss           | 0.0493      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=230000, episode_reward=1.59 +/- 0.02\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.59        |\n",
      "| mean_reward             | 1.3573588   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 230000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011504069 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.92       |\n",
      "|    explained_variance   | 0.059       |\n",
      "|    learning_rate        | 9.43e-05    |\n",
      "|    loss                 | -0.036      |\n",
      "|    n_updates            | 9180        |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    value_loss           | 0.0665      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.41     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 307      |\n",
      "|    time_elapsed    | 15545    |\n",
      "|    total_timesteps | 230250   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.41517     |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 308         |\n",
      "|    time_elapsed         | 15595       |\n",
      "|    total_timesteps      | 231000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012297207 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.921      |\n",
      "|    explained_variance   | 0.0049      |\n",
      "|    learning_rate        | 9.42e-05    |\n",
      "|    loss                 | -0.0379     |\n",
      "|    n_updates            | 9210        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    value_loss           | 0.0902      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4879702   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 309         |\n",
      "|    time_elapsed         | 15645       |\n",
      "|    total_timesteps      | 231750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010797681 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.894      |\n",
      "|    explained_variance   | 0.0361      |\n",
      "|    learning_rate        | 9.42e-05    |\n",
      "|    loss                 | -0.0426     |\n",
      "|    n_updates            | 9240        |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    value_loss           | 0.04        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4210173   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.38        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 310         |\n",
      "|    time_elapsed         | 15695       |\n",
      "|    total_timesteps      | 232500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011739698 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.925      |\n",
      "|    explained_variance   | 0.00962     |\n",
      "|    learning_rate        | 9.42e-05    |\n",
      "|    loss                 | -0.0294     |\n",
      "|    n_updates            | 9270        |\n",
      "|    policy_gradient_loss | -0.023      |\n",
      "|    value_loss           | 0.0777      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| mean_reward             | 1.3773837    |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15           |\n",
      "|    ep_rew_mean          | 1.44         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 311          |\n",
      "|    time_elapsed         | 15745        |\n",
      "|    total_timesteps      | 233250       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0150643485 |\n",
      "|    clip_fraction        | 0.118        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.929       |\n",
      "|    explained_variance   | -0.00539     |\n",
      "|    learning_rate        | 9.42e-05     |\n",
      "|    loss                 | -0.0379      |\n",
      "|    n_updates            | 9300         |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    value_loss           | 0.0905       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4895245   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.48        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 312         |\n",
      "|    time_elapsed         | 15795       |\n",
      "|    total_timesteps      | 234000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009343987 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.891      |\n",
      "|    explained_variance   | 0.165       |\n",
      "|    learning_rate        | 9.42e-05    |\n",
      "|    loss                 | -0.0521     |\n",
      "|    n_updates            | 9330        |\n",
      "|    policy_gradient_loss | -0.0215     |\n",
      "|    value_loss           | 0.0221      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| mean_reward             | 1.485405     |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15           |\n",
      "|    ep_rew_mean          | 1.42         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 313          |\n",
      "|    time_elapsed         | 15845        |\n",
      "|    total_timesteps      | 234750       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0104214195 |\n",
      "|    clip_fraction        | 0.139        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.92        |\n",
      "|    explained_variance   | -0.0186      |\n",
      "|    learning_rate        | 9.42e-05     |\n",
      "|    loss                 | -0.0321      |\n",
      "|    n_updates            | 9360         |\n",
      "|    policy_gradient_loss | -0.0251      |\n",
      "|    value_loss           | 0.0748       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=235000, episode_reward=1.60 +/- 0.03\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.6         |\n",
      "| mean_reward             | 1.3799205   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 235000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011284605 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.913      |\n",
      "|    explained_variance   | 0.0245      |\n",
      "|    learning_rate        | 9.41e-05    |\n",
      "|    loss                 | -0.0421     |\n",
      "|    n_updates            | 9390        |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    value_loss           | 0.0577      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.44     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 314      |\n",
      "|    time_elapsed    | 15901    |\n",
      "|    total_timesteps | 235500   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4823487   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.38        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 315         |\n",
      "|    time_elapsed         | 15951       |\n",
      "|    total_timesteps      | 236250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012680559 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.905      |\n",
      "|    explained_variance   | 0.0743      |\n",
      "|    learning_rate        | 9.41e-05    |\n",
      "|    loss                 | -0.0471     |\n",
      "|    n_updates            | 9420        |\n",
      "|    policy_gradient_loss | -0.0234     |\n",
      "|    value_loss           | 0.0414      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3023344   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 316         |\n",
      "|    time_elapsed         | 16001       |\n",
      "|    total_timesteps      | 237000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022847345 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.929      |\n",
      "|    explained_variance   | -0.111      |\n",
      "|    learning_rate        | 9.41e-05    |\n",
      "|    loss                 | -0.00996    |\n",
      "|    n_updates            | 9450        |\n",
      "|    policy_gradient_loss | -0.0318     |\n",
      "|    value_loss           | 0.162       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4678174   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.5         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 317         |\n",
      "|    time_elapsed         | 16051       |\n",
      "|    total_timesteps      | 237750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014691047 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.923      |\n",
      "|    explained_variance   | -0.019      |\n",
      "|    learning_rate        | 9.41e-05    |\n",
      "|    loss                 | -0.0545     |\n",
      "|    n_updates            | 9480        |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    value_loss           | 0.0336      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4758075   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.5         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 318         |\n",
      "|    time_elapsed         | 16101       |\n",
      "|    total_timesteps      | 238500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019277977 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.893      |\n",
      "|    explained_variance   | 0.0898      |\n",
      "|    learning_rate        | 9.41e-05    |\n",
      "|    loss                 | -0.0562     |\n",
      "|    n_updates            | 9510        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    value_loss           | 0.0359      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.5167676   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.5         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 319         |\n",
      "|    time_elapsed         | 16151       |\n",
      "|    total_timesteps      | 239250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013306472 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.896      |\n",
      "|    explained_variance   | 0.0389      |\n",
      "|    learning_rate        | 9.4e-05     |\n",
      "|    loss                 | -0.0314     |\n",
      "|    n_updates            | 9540        |\n",
      "|    policy_gradient_loss | -0.0198     |\n",
      "|    value_loss           | 0.0547      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=240000, episode_reward=1.59 +/- 0.06\n",
      "Episode length: 15.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 15         |\n",
      "|    mean_reward          | 1.59       |\n",
      "| mean_reward             | 1.4892093  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 240000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01217069 |\n",
      "|    clip_fraction        | 0.154      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.903     |\n",
      "|    explained_variance   | 0.024      |\n",
      "|    learning_rate        | 9.4e-05    |\n",
      "|    loss                 | -0.0428    |\n",
      "|    n_updates            | 9570       |\n",
      "|    policy_gradient_loss | -0.0205    |\n",
      "|    value_loss           | 0.0361     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15       |\n",
      "|    ep_rew_mean     | 1.46     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 320      |\n",
      "|    time_elapsed    | 16205    |\n",
      "|    total_timesteps | 240000   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3985156   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 321         |\n",
      "|    time_elapsed         | 16255       |\n",
      "|    total_timesteps      | 240750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013269201 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.935      |\n",
      "|    explained_variance   | -0.00151    |\n",
      "|    learning_rate        | 9.4e-05     |\n",
      "|    loss                 | -0.0141     |\n",
      "|    n_updates            | 9600        |\n",
      "|    policy_gradient_loss | -0.024      |\n",
      "|    value_loss           | 0.11        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.438529    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 322         |\n",
      "|    time_elapsed         | 16305       |\n",
      "|    total_timesteps      | 241500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014572489 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.94       |\n",
      "|    explained_variance   | 0.0627      |\n",
      "|    learning_rate        | 9.4e-05     |\n",
      "|    loss                 | -0.0264     |\n",
      "|    n_updates            | 9630        |\n",
      "|    policy_gradient_loss | -0.0237     |\n",
      "|    value_loss           | 0.0881      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.435299    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 323         |\n",
      "|    time_elapsed         | 16355       |\n",
      "|    total_timesteps      | 242250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011884029 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.935      |\n",
      "|    explained_variance   | -0.0466     |\n",
      "|    learning_rate        | 9.4e-05     |\n",
      "|    loss                 | -0.029      |\n",
      "|    n_updates            | 9660        |\n",
      "|    policy_gradient_loss | -0.0222     |\n",
      "|    value_loss           | 0.0764      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4110008   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 324         |\n",
      "|    time_elapsed         | 16405       |\n",
      "|    total_timesteps      | 243000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012288605 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.951      |\n",
      "|    explained_variance   | 0.0183      |\n",
      "|    learning_rate        | 9.39e-05    |\n",
      "|    loss                 | -0.0258     |\n",
      "|    n_updates            | 9690        |\n",
      "|    policy_gradient_loss | -0.024      |\n",
      "|    value_loss           | 0.0901      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4263623   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 325         |\n",
      "|    time_elapsed         | 16455       |\n",
      "|    total_timesteps      | 243750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016361455 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.971      |\n",
      "|    explained_variance   | 0.0388      |\n",
      "|    learning_rate        | 9.39e-05    |\n",
      "|    loss                 | -0.038      |\n",
      "|    n_updates            | 9720        |\n",
      "|    policy_gradient_loss | -0.0215     |\n",
      "|    value_loss           | 0.0547      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4771744   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 326         |\n",
      "|    time_elapsed         | 16505       |\n",
      "|    total_timesteps      | 244500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012429798 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.942      |\n",
      "|    explained_variance   | 0.0493      |\n",
      "|    learning_rate        | 9.39e-05    |\n",
      "|    loss                 | -0.0402     |\n",
      "|    n_updates            | 9750        |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    value_loss           | 0.0703      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=245000, episode_reward=1.61 +/- 0.02\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.61        |\n",
      "| mean_reward             | 1.4023381   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 245000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012614552 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.961      |\n",
      "|    explained_variance   | 0.13        |\n",
      "|    learning_rate        | 9.39e-05    |\n",
      "|    loss                 | -0.0475     |\n",
      "|    n_updates            | 9780        |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    value_loss           | 0.0549      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.37     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 327      |\n",
      "|    time_elapsed    | 16561    |\n",
      "|    total_timesteps | 245250   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2901137   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.3         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 328         |\n",
      "|    time_elapsed         | 16611       |\n",
      "|    total_timesteps      | 246000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011678104 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.954      |\n",
      "|    explained_variance   | -0.00635    |\n",
      "|    learning_rate        | 9.39e-05    |\n",
      "|    loss                 | -0.0341     |\n",
      "|    n_updates            | 9810        |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    value_loss           | 0.0957      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3502041   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.38        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 329         |\n",
      "|    time_elapsed         | 16661       |\n",
      "|    total_timesteps      | 246750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015500412 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.937      |\n",
      "|    explained_variance   | 0.0214      |\n",
      "|    learning_rate        | 9.39e-05    |\n",
      "|    loss                 | 0.019       |\n",
      "|    n_updates            | 9840        |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    value_loss           | 0.197       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4739144   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.49        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 330         |\n",
      "|    time_elapsed         | 16711       |\n",
      "|    total_timesteps      | 247500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012240608 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.932      |\n",
      "|    explained_variance   | 0.0515      |\n",
      "|    learning_rate        | 9.38e-05    |\n",
      "|    loss                 | -0.0289     |\n",
      "|    n_updates            | 9870        |\n",
      "|    policy_gradient_loss | -0.0206     |\n",
      "|    value_loss           | 0.075       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.481152    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.47        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 331         |\n",
      "|    time_elapsed         | 16761       |\n",
      "|    total_timesteps      | 248250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007034968 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.923      |\n",
      "|    explained_variance   | 0.0322      |\n",
      "|    learning_rate        | 9.38e-05    |\n",
      "|    loss                 | -0.0483     |\n",
      "|    n_updates            | 9900        |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    value_loss           | 0.0269      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4308326   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.37        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 332         |\n",
      "|    time_elapsed         | 16811       |\n",
      "|    total_timesteps      | 249000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011575107 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.917      |\n",
      "|    explained_variance   | 0.0276      |\n",
      "|    learning_rate        | 9.38e-05    |\n",
      "|    loss                 | -0.0392     |\n",
      "|    n_updates            | 9930        |\n",
      "|    policy_gradient_loss | -0.0215     |\n",
      "|    value_loss           | 0.057       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3066086   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.37        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 333         |\n",
      "|    time_elapsed         | 16861       |\n",
      "|    total_timesteps      | 249750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012922388 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.909      |\n",
      "|    explained_variance   | 0.0355      |\n",
      "|    learning_rate        | 9.38e-05    |\n",
      "|    loss                 | -0.0046     |\n",
      "|    n_updates            | 9960        |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    value_loss           | 0.142       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=250000, episode_reward=1.61 +/- 0.03\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.61        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 250000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022814423 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.911      |\n",
      "|    explained_variance   | -0.202      |\n",
      "|    learning_rate        | 9.38e-05    |\n",
      "|    loss                 | -0.0181     |\n",
      "|    n_updates            | 9990        |\n",
      "|    policy_gradient_loss | -0.0227     |\n",
      "|    value_loss           | 0.106       |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| mean_reward        | 1.3611373 |\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 15.1      |\n",
      "|    ep_rew_mean     | 1.43      |\n",
      "| time/              |           |\n",
      "|    fps             | 14        |\n",
      "|    iterations      | 334       |\n",
      "|    time_elapsed    | 16916     |\n",
      "|    total_timesteps | 250500    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.53526     |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.48        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 335         |\n",
      "|    time_elapsed         | 16966       |\n",
      "|    total_timesteps      | 251250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012528956 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.939      |\n",
      "|    explained_variance   | 0.0547      |\n",
      "|    learning_rate        | 9.37e-05    |\n",
      "|    loss                 | -0.0285     |\n",
      "|    n_updates            | 10020       |\n",
      "|    policy_gradient_loss | -0.0216     |\n",
      "|    value_loss           | 0.0758      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.506377    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 336         |\n",
      "|    time_elapsed         | 17016       |\n",
      "|    total_timesteps      | 252000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013777443 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.97       |\n",
      "|    explained_variance   | 0.253       |\n",
      "|    learning_rate        | 9.37e-05    |\n",
      "|    loss                 | -0.0576     |\n",
      "|    n_updates            | 10050       |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    value_loss           | 0.0178      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2707517   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.38        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 337         |\n",
      "|    time_elapsed         | 17066       |\n",
      "|    total_timesteps      | 252750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017169308 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.934      |\n",
      "|    explained_variance   | 0.00301     |\n",
      "|    learning_rate        | 9.37e-05    |\n",
      "|    loss                 | 0.00218     |\n",
      "|    n_updates            | 10080       |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    value_loss           | 0.174       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.5111568   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.5         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 338         |\n",
      "|    time_elapsed         | 17116       |\n",
      "|    total_timesteps      | 253500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012057159 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.917      |\n",
      "|    explained_variance   | -0.0499     |\n",
      "|    learning_rate        | 9.37e-05    |\n",
      "|    loss                 | -0.0466     |\n",
      "|    n_updates            | 10110       |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    value_loss           | 0.0506      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.502933   |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15         |\n",
      "|    ep_rew_mean          | 1.5        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 339        |\n",
      "|    time_elapsed         | 17166      |\n",
      "|    total_timesteps      | 254250     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01123619 |\n",
      "|    clip_fraction        | 0.136      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.883     |\n",
      "|    explained_variance   | 0.218      |\n",
      "|    learning_rate        | 9.37e-05   |\n",
      "|    loss                 | -0.0471    |\n",
      "|    n_updates            | 10140      |\n",
      "|    policy_gradient_loss | -0.0174    |\n",
      "|    value_loss           | 0.0205     |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=255000, episode_reward=1.62 +/- 0.01\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.62        |\n",
      "| mean_reward             | 1.4602834   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 255000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009884182 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.894      |\n",
      "|    explained_variance   | 0.054       |\n",
      "|    learning_rate        | 9.36e-05    |\n",
      "|    loss                 | -0.0432     |\n",
      "|    n_updates            | 10170       |\n",
      "|    policy_gradient_loss | -0.023      |\n",
      "|    value_loss           | 0.0506      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15       |\n",
      "|    ep_rew_mean     | 1.46     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 340      |\n",
      "|    time_elapsed    | 17221    |\n",
      "|    total_timesteps | 255000   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| mean_reward             | 1.5094941 |\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 15.1      |\n",
      "|    ep_rew_mean          | 1.42      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 14        |\n",
      "|    iterations           | 341       |\n",
      "|    time_elapsed         | 17271     |\n",
      "|    total_timesteps      | 255750    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0118413 |\n",
      "|    clip_fraction        | 0.118     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.898    |\n",
      "|    explained_variance   | -0.0223   |\n",
      "|    learning_rate        | 9.36e-05  |\n",
      "|    loss                 | -0.0129   |\n",
      "|    n_updates            | 10200     |\n",
      "|    policy_gradient_loss | -0.0227   |\n",
      "|    value_loss           | 0.109     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.3390743  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15.1       |\n",
      "|    ep_rew_mean          | 1.39       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 342        |\n",
      "|    time_elapsed         | 17321      |\n",
      "|    total_timesteps      | 256500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01085334 |\n",
      "|    clip_fraction        | 0.107      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.948     |\n",
      "|    explained_variance   | 0.0539     |\n",
      "|    learning_rate        | 9.36e-05   |\n",
      "|    loss                 | -0.0189    |\n",
      "|    n_updates            | 10230      |\n",
      "|    policy_gradient_loss | -0.0232    |\n",
      "|    value_loss           | 0.1        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3779106   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 343         |\n",
      "|    time_elapsed         | 17371       |\n",
      "|    total_timesteps      | 257250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013460145 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.91       |\n",
      "|    explained_variance   | 0.0273      |\n",
      "|    learning_rate        | 9.36e-05    |\n",
      "|    loss                 | -0.0208     |\n",
      "|    n_updates            | 10260       |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.5166849   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.47        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 344         |\n",
      "|    time_elapsed         | 17421       |\n",
      "|    total_timesteps      | 258000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009597526 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.894      |\n",
      "|    explained_variance   | 0.0247      |\n",
      "|    learning_rate        | 9.36e-05    |\n",
      "|    loss                 | -0.0436     |\n",
      "|    n_updates            | 10290       |\n",
      "|    policy_gradient_loss | -0.0248     |\n",
      "|    value_loss           | 0.0536      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4734131   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.5         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 345         |\n",
      "|    time_elapsed         | 17471       |\n",
      "|    total_timesteps      | 258750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014323353 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.908      |\n",
      "|    explained_variance   | 0.00254     |\n",
      "|    learning_rate        | 9.35e-05    |\n",
      "|    loss                 | -0.0356     |\n",
      "|    n_updates            | 10320       |\n",
      "|    policy_gradient_loss | -0.0206     |\n",
      "|    value_loss           | 0.0544      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4877666   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 346         |\n",
      "|    time_elapsed         | 17521       |\n",
      "|    total_timesteps      | 259500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012183169 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.916      |\n",
      "|    explained_variance   | 0.0365      |\n",
      "|    learning_rate        | 9.35e-05    |\n",
      "|    loss                 | -0.0399     |\n",
      "|    n_updates            | 10350       |\n",
      "|    policy_gradient_loss | -0.0227     |\n",
      "|    value_loss           | 0.0533      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=260000, episode_reward=1.61 +/- 0.03\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.61        |\n",
      "| mean_reward             | 1.3303834   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 260000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009984848 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.913      |\n",
      "|    explained_variance   | 0.00597     |\n",
      "|    learning_rate        | 9.35e-05    |\n",
      "|    loss                 | -0.0118     |\n",
      "|    n_updates            | 10380       |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    value_loss           | 0.131       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.39     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 347      |\n",
      "|    time_elapsed    | 17576    |\n",
      "|    total_timesteps | 260250   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4524231   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.46        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 348         |\n",
      "|    time_elapsed         | 17626       |\n",
      "|    total_timesteps      | 261000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011914096 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.907      |\n",
      "|    explained_variance   | -0.087      |\n",
      "|    learning_rate        | 9.35e-05    |\n",
      "|    loss                 | -0.0321     |\n",
      "|    n_updates            | 10410       |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    value_loss           | 0.0878      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.481491    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 349         |\n",
      "|    time_elapsed         | 17676       |\n",
      "|    total_timesteps      | 261750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011664047 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.911      |\n",
      "|    explained_variance   | 0.0361      |\n",
      "|    learning_rate        | 9.35e-05    |\n",
      "|    loss                 | -0.0503     |\n",
      "|    n_updates            | 10440       |\n",
      "|    policy_gradient_loss | -0.024      |\n",
      "|    value_loss           | 0.0394      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3402809   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.4         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 350         |\n",
      "|    time_elapsed         | 17727       |\n",
      "|    total_timesteps      | 262500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012790797 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.91       |\n",
      "|    explained_variance   | -0.0186     |\n",
      "|    learning_rate        | 9.35e-05    |\n",
      "|    loss                 | -0.00667    |\n",
      "|    n_updates            | 10470       |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    value_loss           | 0.14        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3717377   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.37        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 351         |\n",
      "|    time_elapsed         | 17777       |\n",
      "|    total_timesteps      | 263250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012613365 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.902      |\n",
      "|    explained_variance   | 0.042       |\n",
      "|    learning_rate        | 9.34e-05    |\n",
      "|    loss                 | -0.0387     |\n",
      "|    n_updates            | 10500       |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    value_loss           | 0.0672      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.35441     |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.33        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 352         |\n",
      "|    time_elapsed         | 17827       |\n",
      "|    total_timesteps      | 264000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016495947 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.924      |\n",
      "|    explained_variance   | -0.00283    |\n",
      "|    learning_rate        | 9.34e-05    |\n",
      "|    loss                 | 0.0049      |\n",
      "|    n_updates            | 10530       |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    value_loss           | 0.171       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3920015   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 353         |\n",
      "|    time_elapsed         | 17877       |\n",
      "|    total_timesteps      | 264750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011233208 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.904      |\n",
      "|    explained_variance   | -0.0284     |\n",
      "|    learning_rate        | 9.34e-05    |\n",
      "|    loss                 | -0.0331     |\n",
      "|    n_updates            | 10560       |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    value_loss           | 0.0893      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=265000, episode_reward=1.59 +/- 0.03\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.59        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 265000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016943702 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.913      |\n",
      "|    explained_variance   | 0.0264      |\n",
      "|    learning_rate        | 9.34e-05    |\n",
      "|    loss                 | -0.00973    |\n",
      "|    n_updates            | 10590       |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    value_loss           | 0.119       |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| mean_reward        | 1.4309301 |\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 15.1      |\n",
      "|    ep_rew_mean     | 1.44      |\n",
      "| time/              |           |\n",
      "|    fps             | 14        |\n",
      "|    iterations      | 354       |\n",
      "|    time_elapsed    | 17932     |\n",
      "|    total_timesteps | 265500    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| mean_reward             | 1.4391247    |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15.1         |\n",
      "|    ep_rew_mean          | 1.41         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 355          |\n",
      "|    time_elapsed         | 17982        |\n",
      "|    total_timesteps      | 266250       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0141986795 |\n",
      "|    clip_fraction        | 0.163        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.929       |\n",
      "|    explained_variance   | 0.0506       |\n",
      "|    learning_rate        | 9.34e-05     |\n",
      "|    loss                 | -0.0469      |\n",
      "|    n_updates            | 10620        |\n",
      "|    policy_gradient_loss | -0.0217      |\n",
      "|    value_loss           | 0.0415       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3524954   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.37        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 356         |\n",
      "|    time_elapsed         | 18032       |\n",
      "|    total_timesteps      | 267000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013860441 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.943      |\n",
      "|    explained_variance   | -0.0321     |\n",
      "|    learning_rate        | 9.33e-05    |\n",
      "|    loss                 | -0.0273     |\n",
      "|    n_updates            | 10650       |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    value_loss           | 0.104       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3321886   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.37        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 357         |\n",
      "|    time_elapsed         | 18082       |\n",
      "|    total_timesteps      | 267750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015816499 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.935      |\n",
      "|    explained_variance   | 0.0907      |\n",
      "|    learning_rate        | 9.33e-05    |\n",
      "|    loss                 | -0.0346     |\n",
      "|    n_updates            | 10680       |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    value_loss           | 0.0841      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4664552   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 358         |\n",
      "|    time_elapsed         | 18132       |\n",
      "|    total_timesteps      | 268500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012119236 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.937      |\n",
      "|    explained_variance   | 0.0837      |\n",
      "|    learning_rate        | 9.33e-05    |\n",
      "|    loss                 | -0.0413     |\n",
      "|    n_updates            | 10710       |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    value_loss           | 0.0794      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| mean_reward             | 1.4723396    |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15           |\n",
      "|    ep_rew_mean          | 1.48         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 359          |\n",
      "|    time_elapsed         | 18182        |\n",
      "|    total_timesteps      | 269250       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0131020965 |\n",
      "|    clip_fraction        | 0.165        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.911       |\n",
      "|    explained_variance   | 0.0271       |\n",
      "|    learning_rate        | 9.33e-05     |\n",
      "|    loss                 | -0.0567      |\n",
      "|    n_updates            | 10740        |\n",
      "|    policy_gradient_loss | -0.0278      |\n",
      "|    value_loss           | 0.0329       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=270000, episode_reward=1.58 +/- 0.03\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.58        |\n",
      "| mean_reward             | 1.4975426   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 270000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013750789 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.914      |\n",
      "|    explained_variance   | 0.00791     |\n",
      "|    learning_rate        | 9.33e-05    |\n",
      "|    loss                 | -0.048      |\n",
      "|    n_updates            | 10770       |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    value_loss           | 0.0522      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15       |\n",
      "|    ep_rew_mean     | 1.48     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 360      |\n",
      "|    time_elapsed    | 18237    |\n",
      "|    total_timesteps | 270000   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.4537132  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15.1       |\n",
      "|    ep_rew_mean          | 1.47       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 361        |\n",
      "|    time_elapsed         | 18287      |\n",
      "|    total_timesteps      | 270750     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01866062 |\n",
      "|    clip_fraction        | 0.187      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.919     |\n",
      "|    explained_variance   | 0.0464     |\n",
      "|    learning_rate        | 9.33e-05   |\n",
      "|    loss                 | -0.0369    |\n",
      "|    n_updates            | 10800      |\n",
      "|    policy_gradient_loss | -0.023     |\n",
      "|    value_loss           | 0.0542     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4517583   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 362         |\n",
      "|    time_elapsed         | 18337       |\n",
      "|    total_timesteps      | 271500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020507943 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.935      |\n",
      "|    explained_variance   | 0.0685      |\n",
      "|    learning_rate        | 9.32e-05    |\n",
      "|    loss                 | -0.0389     |\n",
      "|    n_updates            | 10830       |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    value_loss           | 0.0768      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4730201   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.46        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 363         |\n",
      "|    time_elapsed         | 18387       |\n",
      "|    total_timesteps      | 272250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017785639 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.914      |\n",
      "|    explained_variance   | 0.0482      |\n",
      "|    learning_rate        | 9.32e-05    |\n",
      "|    loss                 | -0.0466     |\n",
      "|    n_updates            | 10860       |\n",
      "|    policy_gradient_loss | -0.0245     |\n",
      "|    value_loss           | 0.045       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4143525   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 364         |\n",
      "|    time_elapsed         | 18437       |\n",
      "|    total_timesteps      | 273000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017944787 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.927      |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 9.32e-05    |\n",
      "|    loss                 | -0.0507     |\n",
      "|    n_updates            | 10890       |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    value_loss           | 0.0493      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4798476   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.48        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 365         |\n",
      "|    time_elapsed         | 18487       |\n",
      "|    total_timesteps      | 273750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010112907 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.93       |\n",
      "|    explained_variance   | -0.00103    |\n",
      "|    learning_rate        | 9.32e-05    |\n",
      "|    loss                 | -0.0291     |\n",
      "|    n_updates            | 10920       |\n",
      "|    policy_gradient_loss | -0.0239     |\n",
      "|    value_loss           | 0.0783      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.5084497   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.47        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 366         |\n",
      "|    time_elapsed         | 18537       |\n",
      "|    total_timesteps      | 274500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017353265 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.931      |\n",
      "|    explained_variance   | 0.0388      |\n",
      "|    learning_rate        | 9.32e-05    |\n",
      "|    loss                 | -0.0491     |\n",
      "|    n_updates            | 10950       |\n",
      "|    policy_gradient_loss | -0.024      |\n",
      "|    value_loss           | 0.0306      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=275000, episode_reward=1.58 +/- 0.01\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.58        |\n",
      "| mean_reward             | 1.4325502   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 275000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014152037 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.92       |\n",
      "|    explained_variance   | 0.0462      |\n",
      "|    learning_rate        | 9.31e-05    |\n",
      "|    loss                 | -0.0429     |\n",
      "|    n_updates            | 10980       |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    value_loss           | 0.0593      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.43     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 367      |\n",
      "|    time_elapsed    | 18592    |\n",
      "|    total_timesteps | 275250   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4276406   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.46        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 368         |\n",
      "|    time_elapsed         | 18642       |\n",
      "|    total_timesteps      | 276000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015345055 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.963      |\n",
      "|    explained_variance   | 0.0488      |\n",
      "|    learning_rate        | 9.31e-05    |\n",
      "|    loss                 | -0.0338     |\n",
      "|    n_updates            | 11010       |\n",
      "|    policy_gradient_loss | -0.0252     |\n",
      "|    value_loss           | 0.0762      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4709631   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 369         |\n",
      "|    time_elapsed         | 18692       |\n",
      "|    total_timesteps      | 276750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012730546 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.96       |\n",
      "|    explained_variance   | 0.122       |\n",
      "|    learning_rate        | 9.31e-05    |\n",
      "|    loss                 | -0.0495     |\n",
      "|    n_updates            | 11040       |\n",
      "|    policy_gradient_loss | -0.0223     |\n",
      "|    value_loss           | 0.0301      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3616577   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.4         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 370         |\n",
      "|    time_elapsed         | 18742       |\n",
      "|    total_timesteps      | 277500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008196092 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.96       |\n",
      "|    explained_variance   | 0.0142      |\n",
      "|    learning_rate        | 9.31e-05    |\n",
      "|    loss                 | -0.0347     |\n",
      "|    n_updates            | 11070       |\n",
      "|    policy_gradient_loss | -0.0234     |\n",
      "|    value_loss           | 0.0797      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3929367   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.33        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 371         |\n",
      "|    time_elapsed         | 18792       |\n",
      "|    total_timesteps      | 278250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011057242 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.974      |\n",
      "|    explained_variance   | 0.0337      |\n",
      "|    learning_rate        | 9.31e-05    |\n",
      "|    loss                 | -0.0328     |\n",
      "|    n_updates            | 11100       |\n",
      "|    policy_gradient_loss | -0.0211     |\n",
      "|    value_loss           | 0.0755      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2270919   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.31        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 372         |\n",
      "|    time_elapsed         | 18843       |\n",
      "|    total_timesteps      | 279000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012501456 |\n",
      "|    clip_fraction        | 0.097       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.97       |\n",
      "|    explained_variance   | -0.0123     |\n",
      "|    learning_rate        | 9.3e-05     |\n",
      "|    loss                 | 0.0113      |\n",
      "|    n_updates            | 11130       |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    value_loss           | 0.181       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3950822   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.37        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 373         |\n",
      "|    time_elapsed         | 18893       |\n",
      "|    total_timesteps      | 279750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011019988 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.986      |\n",
      "|    explained_variance   | -0.0107     |\n",
      "|    learning_rate        | 9.3e-05     |\n",
      "|    loss                 | -0.0321     |\n",
      "|    n_updates            | 11160       |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    value_loss           | 0.0897      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=280000, episode_reward=1.57 +/- 0.05\n",
      "Episode length: 15.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 15         |\n",
      "|    mean_reward          | 1.57       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 280000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01256447 |\n",
      "|    clip_fraction        | 0.165      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.963     |\n",
      "|    explained_variance   | 0.0352     |\n",
      "|    learning_rate        | 9.3e-05    |\n",
      "|    loss                 | -0.0263    |\n",
      "|    n_updates            | 11190      |\n",
      "|    policy_gradient_loss | -0.0287    |\n",
      "|    value_loss           | 0.107      |\n",
      "----------------------------------------\n",
      "----------------------------------\n",
      "| mean_reward        | 1.4252143 |\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 15.1      |\n",
      "|    ep_rew_mean     | 1.42      |\n",
      "| time/              |           |\n",
      "|    fps             | 14        |\n",
      "|    iterations      | 374       |\n",
      "|    time_elapsed    | 18948     |\n",
      "|    total_timesteps | 280500    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3983737   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.4         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 375         |\n",
      "|    time_elapsed         | 18998       |\n",
      "|    total_timesteps      | 281250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015345945 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.966      |\n",
      "|    explained_variance   | 0.026       |\n",
      "|    learning_rate        | 9.3e-05     |\n",
      "|    loss                 | -0.0504     |\n",
      "|    n_updates            | 11220       |\n",
      "|    policy_gradient_loss | -0.0237     |\n",
      "|    value_loss           | 0.0399      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3564509   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.36        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 376         |\n",
      "|    time_elapsed         | 19048       |\n",
      "|    total_timesteps      | 282000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017135812 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.94       |\n",
      "|    explained_variance   | 0.0104      |\n",
      "|    learning_rate        | 9.3e-05     |\n",
      "|    loss                 | -0.0189     |\n",
      "|    n_updates            | 11250       |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3549826   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.38        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 377         |\n",
      "|    time_elapsed         | 19098       |\n",
      "|    total_timesteps      | 282750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009712006 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.934      |\n",
      "|    explained_variance   | 0.0681      |\n",
      "|    learning_rate        | 9.3e-05     |\n",
      "|    loss                 | -0.0408     |\n",
      "|    n_updates            | 11280       |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    value_loss           | 0.0662      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3613452   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 378         |\n",
      "|    time_elapsed         | 19148       |\n",
      "|    total_timesteps      | 283500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013794878 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.967      |\n",
      "|    explained_variance   | 0.00121     |\n",
      "|    learning_rate        | 9.29e-05    |\n",
      "|    loss                 | -0.035      |\n",
      "|    n_updates            | 11310       |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    value_loss           | 0.0895      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4968925   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 379         |\n",
      "|    time_elapsed         | 19198       |\n",
      "|    total_timesteps      | 284250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012751864 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.963      |\n",
      "|    explained_variance   | -0.0186     |\n",
      "|    learning_rate        | 9.29e-05    |\n",
      "|    loss                 | -0.0346     |\n",
      "|    n_updates            | 11340       |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    value_loss           | 0.0759      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=285000, episode_reward=1.59 +/- 0.03\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.59        |\n",
      "| mean_reward             | 1.4278373   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 285000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011318701 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.973      |\n",
      "|    explained_variance   | 0.0768      |\n",
      "|    learning_rate        | 9.29e-05    |\n",
      "|    loss                 | -0.0459     |\n",
      "|    n_updates            | 11370       |\n",
      "|    policy_gradient_loss | -0.0244     |\n",
      "|    value_loss           | 0.0488      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15       |\n",
      "|    ep_rew_mean     | 1.44     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 380      |\n",
      "|    time_elapsed    | 19253    |\n",
      "|    total_timesteps | 285000   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4073257   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.38        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 381         |\n",
      "|    time_elapsed         | 19303       |\n",
      "|    total_timesteps      | 285750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010014906 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.948      |\n",
      "|    explained_variance   | 0.0474      |\n",
      "|    learning_rate        | 9.29e-05    |\n",
      "|    loss                 | -0.039      |\n",
      "|    n_updates            | 11400       |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    value_loss           | 0.0657      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3933307   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 382         |\n",
      "|    time_elapsed         | 19353       |\n",
      "|    total_timesteps      | 286500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012534473 |\n",
      "|    clip_fraction        | 0.0842      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.949      |\n",
      "|    explained_variance   | 0.042       |\n",
      "|    learning_rate        | 9.29e-05    |\n",
      "|    loss                 | -0.0287     |\n",
      "|    n_updates            | 11430       |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    value_loss           | 0.1         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3770089   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.36        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 383         |\n",
      "|    time_elapsed         | 19403       |\n",
      "|    total_timesteps      | 287250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013162622 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.963      |\n",
      "|    explained_variance   | 0.00039     |\n",
      "|    learning_rate        | 9.28e-05    |\n",
      "|    loss                 | -0.0413     |\n",
      "|    n_updates            | 11460       |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    value_loss           | 0.063       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2700317   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.3         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 384         |\n",
      "|    time_elapsed         | 19453       |\n",
      "|    total_timesteps      | 288000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013499604 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.959      |\n",
      "|    explained_variance   | 0.00183     |\n",
      "|    learning_rate        | 9.28e-05    |\n",
      "|    loss                 | -0.0101     |\n",
      "|    n_updates            | 11490       |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    value_loss           | 0.14        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4231807   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.37        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 385         |\n",
      "|    time_elapsed         | 19503       |\n",
      "|    total_timesteps      | 288750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012922757 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.963      |\n",
      "|    explained_variance   | 0.025       |\n",
      "|    learning_rate        | 9.28e-05    |\n",
      "|    loss                 | -0.0362     |\n",
      "|    n_updates            | 11520       |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    value_loss           | 0.0892      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4382194   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.47        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 386         |\n",
      "|    time_elapsed         | 19553       |\n",
      "|    total_timesteps      | 289500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011305674 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.93       |\n",
      "|    explained_variance   | 0.0389      |\n",
      "|    learning_rate        | 9.28e-05    |\n",
      "|    loss                 | -0.0204     |\n",
      "|    n_updates            | 11550       |\n",
      "|    policy_gradient_loss | -0.024      |\n",
      "|    value_loss           | 0.0938      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=290000, episode_reward=1.60 +/- 0.02\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.6         |\n",
      "| mean_reward             | 1.4445595   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 290000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008322719 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.966      |\n",
      "|    explained_variance   | 0.25        |\n",
      "|    learning_rate        | 9.28e-05    |\n",
      "|    loss                 | -0.0512     |\n",
      "|    n_updates            | 11580       |\n",
      "|    policy_gradient_loss | -0.0204     |\n",
      "|    value_loss           | 0.0238      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.49     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 387      |\n",
      "|    time_elapsed    | 19608    |\n",
      "|    total_timesteps | 290250   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4686034   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.47        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 388         |\n",
      "|    time_elapsed         | 19658       |\n",
      "|    total_timesteps      | 291000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012137937 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.958      |\n",
      "|    explained_variance   | 0.0715      |\n",
      "|    learning_rate        | 9.27e-05    |\n",
      "|    loss                 | -0.035      |\n",
      "|    n_updates            | 11610       |\n",
      "|    policy_gradient_loss | -0.0252     |\n",
      "|    value_loss           | 0.067       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4291661   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 389         |\n",
      "|    time_elapsed         | 19708       |\n",
      "|    total_timesteps      | 291750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014101185 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.953      |\n",
      "|    explained_variance   | 0.0384      |\n",
      "|    learning_rate        | 9.27e-05    |\n",
      "|    loss                 | -0.0517     |\n",
      "|    n_updates            | 11640       |\n",
      "|    policy_gradient_loss | -0.0257     |\n",
      "|    value_loss           | 0.0385      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4141159   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 390         |\n",
      "|    time_elapsed         | 19758       |\n",
      "|    total_timesteps      | 292500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012577266 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.924      |\n",
      "|    explained_variance   | 0.00177     |\n",
      "|    learning_rate        | 9.27e-05    |\n",
      "|    loss                 | -0.0212     |\n",
      "|    n_updates            | 11670       |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    value_loss           | 0.11        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| mean_reward             | 1.5122751    |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15           |\n",
      "|    ep_rew_mean          | 1.52         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 391          |\n",
      "|    time_elapsed         | 19808        |\n",
      "|    total_timesteps      | 293250       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0104909735 |\n",
      "|    clip_fraction        | 0.141        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.929       |\n",
      "|    explained_variance   | 0.0449       |\n",
      "|    learning_rate        | 9.27e-05     |\n",
      "|    loss                 | -0.0502      |\n",
      "|    n_updates            | 11700        |\n",
      "|    policy_gradient_loss | -0.0252      |\n",
      "|    value_loss           | 0.0401       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.550027    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.53        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 392         |\n",
      "|    time_elapsed         | 19858       |\n",
      "|    total_timesteps      | 294000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012646812 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.932      |\n",
      "|    explained_variance   | 0.325       |\n",
      "|    learning_rate        | 9.27e-05    |\n",
      "|    loss                 | -0.0548     |\n",
      "|    n_updates            | 11730       |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    value_loss           | 0.0113      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3737156   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 393         |\n",
      "|    time_elapsed         | 19908       |\n",
      "|    total_timesteps      | 294750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012279531 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.927      |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 9.27e-05    |\n",
      "|    loss                 | -0.0496     |\n",
      "|    n_updates            | 11760       |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    value_loss           | 0.0286      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=295000, episode_reward=1.60 +/- 0.02\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.6         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 295000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011367826 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.936      |\n",
      "|    explained_variance   | 0.0111      |\n",
      "|    learning_rate        | 9.26e-05    |\n",
      "|    loss                 | -0.0127     |\n",
      "|    n_updates            | 11790       |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    value_loss           | 0.122       |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| mean_reward        | 1.4391608 |\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 15.1      |\n",
      "|    ep_rew_mean     | 1.37      |\n",
      "| time/              |           |\n",
      "|    fps             | 14        |\n",
      "|    iterations      | 394       |\n",
      "|    time_elapsed    | 19963     |\n",
      "|    total_timesteps | 295500    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4324743   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 395         |\n",
      "|    time_elapsed         | 20014       |\n",
      "|    total_timesteps      | 296250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012247708 |\n",
      "|    clip_fraction        | 0.0953      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.951      |\n",
      "|    explained_variance   | 0.0311      |\n",
      "|    learning_rate        | 9.26e-05    |\n",
      "|    loss                 | -0.0285     |\n",
      "|    n_updates            | 11820       |\n",
      "|    policy_gradient_loss | -0.0236     |\n",
      "|    value_loss           | 0.0839      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4917623   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.46        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 396         |\n",
      "|    time_elapsed         | 20064       |\n",
      "|    total_timesteps      | 297000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018699573 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.989      |\n",
      "|    explained_variance   | 0.0779      |\n",
      "|    learning_rate        | 9.26e-05    |\n",
      "|    loss                 | -0.0385     |\n",
      "|    n_updates            | 11850       |\n",
      "|    policy_gradient_loss | -0.024      |\n",
      "|    value_loss           | 0.0623      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4732836   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 397         |\n",
      "|    time_elapsed         | 20114       |\n",
      "|    total_timesteps      | 297750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010764071 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.999      |\n",
      "|    explained_variance   | 0.0629      |\n",
      "|    learning_rate        | 9.26e-05    |\n",
      "|    loss                 | -0.0455     |\n",
      "|    n_updates            | 11880       |\n",
      "|    policy_gradient_loss | -0.0248     |\n",
      "|    value_loss           | 0.0527      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3533826   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 398         |\n",
      "|    time_elapsed         | 20164       |\n",
      "|    total_timesteps      | 298500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013125402 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.994      |\n",
      "|    explained_variance   | 0.0175      |\n",
      "|    learning_rate        | 9.26e-05    |\n",
      "|    loss                 | -0.0245     |\n",
      "|    n_updates            | 11910       |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    value_loss           | 0.0982      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4524919   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 399         |\n",
      "|    time_elapsed         | 20214       |\n",
      "|    total_timesteps      | 299250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015158404 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.988      |\n",
      "|    explained_variance   | 0.0242      |\n",
      "|    learning_rate        | 9.25e-05    |\n",
      "|    loss                 | -0.0347     |\n",
      "|    n_updates            | 11940       |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    value_loss           | 0.0802      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=300000, episode_reward=1.38 +/- 0.39\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.38        |\n",
      "| mean_reward             | 1.3405471   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 300000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009256889 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0.0136      |\n",
      "|    learning_rate        | 9.25e-05    |\n",
      "|    loss                 | -0.034      |\n",
      "|    n_updates            | 11970       |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    value_loss           | 0.086       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15       |\n",
      "|    ep_rew_mean     | 1.41     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 400      |\n",
      "|    time_elapsed    | 20269    |\n",
      "|    total_timesteps | 300000   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4302063   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 401         |\n",
      "|    time_elapsed         | 20319       |\n",
      "|    total_timesteps      | 300750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015674468 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.981      |\n",
      "|    explained_variance   | 0.0931      |\n",
      "|    learning_rate        | 9.25e-05    |\n",
      "|    loss                 | -0.0437     |\n",
      "|    n_updates            | 12000       |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    value_loss           | 0.0757      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.4285663  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15.1       |\n",
      "|    ep_rew_mean          | 1.41       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 402        |\n",
      "|    time_elapsed         | 20369      |\n",
      "|    total_timesteps      | 301500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01030164 |\n",
      "|    clip_fraction        | 0.114      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.968     |\n",
      "|    explained_variance   | -0.139     |\n",
      "|    learning_rate        | 9.25e-05   |\n",
      "|    loss                 | -0.0353    |\n",
      "|    n_updates            | 12030      |\n",
      "|    policy_gradient_loss | -0.0243    |\n",
      "|    value_loss           | 0.0736     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3100644   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.37        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 403         |\n",
      "|    time_elapsed         | 20419       |\n",
      "|    total_timesteps      | 302250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015036771 |\n",
      "|    clip_fraction        | 0.0962      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.951      |\n",
      "|    explained_variance   | 0.0265      |\n",
      "|    learning_rate        | 9.25e-05    |\n",
      "|    loss                 | -0.0256     |\n",
      "|    n_updates            | 12060       |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    value_loss           | 0.111       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4410539   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 404         |\n",
      "|    time_elapsed         | 20469       |\n",
      "|    total_timesteps      | 303000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011730538 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.968      |\n",
      "|    explained_variance   | -0.00137    |\n",
      "|    learning_rate        | 9.24e-05    |\n",
      "|    loss                 | -0.0434     |\n",
      "|    n_updates            | 12090       |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    value_loss           | 0.0752      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4441948   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 405         |\n",
      "|    time_elapsed         | 20519       |\n",
      "|    total_timesteps      | 303750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012564023 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.927      |\n",
      "|    explained_variance   | 0.14        |\n",
      "|    learning_rate        | 9.24e-05    |\n",
      "|    loss                 | -0.0547     |\n",
      "|    n_updates            | 12120       |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    value_loss           | 0.0443      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3772894   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.36        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 406         |\n",
      "|    time_elapsed         | 20570       |\n",
      "|    total_timesteps      | 304500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016572976 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.939      |\n",
      "|    explained_variance   | -0.0139     |\n",
      "|    learning_rate        | 9.24e-05    |\n",
      "|    loss                 | -0.0239     |\n",
      "|    n_updates            | 12150       |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    value_loss           | 0.112       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=305000, episode_reward=1.57 +/- 0.04\n",
      "Episode length: 15.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 15           |\n",
      "|    mean_reward          | 1.57         |\n",
      "| mean_reward             | 1.3590741    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 305000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0124807125 |\n",
      "|    clip_fraction        | 0.122        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.952       |\n",
      "|    explained_variance   | -0.0257      |\n",
      "|    learning_rate        | 9.24e-05     |\n",
      "|    loss                 | -0.0259      |\n",
      "|    n_updates            | 12180        |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    value_loss           | 0.105        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.36     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 407      |\n",
      "|    time_elapsed    | 20625    |\n",
      "|    total_timesteps | 305250   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3261766   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.4         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 408         |\n",
      "|    time_elapsed         | 20675       |\n",
      "|    total_timesteps      | 306000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011765786 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.927      |\n",
      "|    explained_variance   | -0.0144     |\n",
      "|    learning_rate        | 9.24e-05    |\n",
      "|    loss                 | -0.0288     |\n",
      "|    n_updates            | 12210       |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    value_loss           | 0.0924      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.5282768   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.47        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 409         |\n",
      "|    time_elapsed         | 20725       |\n",
      "|    total_timesteps      | 306750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009591478 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.954      |\n",
      "|    explained_variance   | -0.0405     |\n",
      "|    learning_rate        | 9.23e-05    |\n",
      "|    loss                 | -0.0224     |\n",
      "|    n_updates            | 12240       |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    value_loss           | 0.1         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4643626   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.49        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 410         |\n",
      "|    time_elapsed         | 20775       |\n",
      "|    total_timesteps      | 307500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012208191 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.89       |\n",
      "|    explained_variance   | 0.176       |\n",
      "|    learning_rate        | 9.23e-05    |\n",
      "|    loss                 | -0.0516     |\n",
      "|    n_updates            | 12270       |\n",
      "|    policy_gradient_loss | -0.0234     |\n",
      "|    value_loss           | 0.0223      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4780437   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 411         |\n",
      "|    time_elapsed         | 20825       |\n",
      "|    total_timesteps      | 308250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008720319 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.918      |\n",
      "|    explained_variance   | 0.0624      |\n",
      "|    learning_rate        | 9.23e-05    |\n",
      "|    loss                 | -0.0432     |\n",
      "|    n_updates            | 12300       |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    value_loss           | 0.0541      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4471939   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 412         |\n",
      "|    time_elapsed         | 20875       |\n",
      "|    total_timesteps      | 309000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013248673 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.946      |\n",
      "|    explained_variance   | 0.0369      |\n",
      "|    learning_rate        | 9.23e-05    |\n",
      "|    loss                 | -0.0253     |\n",
      "|    n_updates            | 12330       |\n",
      "|    policy_gradient_loss | -0.0257     |\n",
      "|    value_loss           | 0.0928      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4622169   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.48        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 413         |\n",
      "|    time_elapsed         | 20926       |\n",
      "|    total_timesteps      | 309750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014351079 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.923      |\n",
      "|    explained_variance   | 0.0777      |\n",
      "|    learning_rate        | 9.23e-05    |\n",
      "|    loss                 | -0.0478     |\n",
      "|    n_updates            | 12360       |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    value_loss           | 0.0535      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=310000, episode_reward=1.60 +/- 0.02\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.6         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 310000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011208356 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.906      |\n",
      "|    explained_variance   | 0.119       |\n",
      "|    learning_rate        | 9.23e-05    |\n",
      "|    loss                 | -0.0513     |\n",
      "|    n_updates            | 12390       |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    value_loss           | 0.0363      |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| mean_reward        | 1.5191102 |\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 15.1      |\n",
      "|    ep_rew_mean     | 1.5       |\n",
      "| time/              |           |\n",
      "|    fps             | 14        |\n",
      "|    iterations      | 414       |\n",
      "|    time_elapsed    | 20981     |\n",
      "|    total_timesteps | 310500    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.5187232   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.52        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 415         |\n",
      "|    time_elapsed         | 21031       |\n",
      "|    total_timesteps      | 311250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012668612 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.866      |\n",
      "|    explained_variance   | 0.0516      |\n",
      "|    learning_rate        | 9.22e-05    |\n",
      "|    loss                 | -0.0437     |\n",
      "|    n_updates            | 12420       |\n",
      "|    policy_gradient_loss | -0.0225     |\n",
      "|    value_loss           | 0.038       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.4621475  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15         |\n",
      "|    ep_rew_mean          | 1.47       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 416        |\n",
      "|    time_elapsed         | 21081      |\n",
      "|    total_timesteps      | 312000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01335366 |\n",
      "|    clip_fraction        | 0.149      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.914     |\n",
      "|    explained_variance   | 0.0679     |\n",
      "|    learning_rate        | 9.22e-05   |\n",
      "|    loss                 | -0.0328    |\n",
      "|    n_updates            | 12450      |\n",
      "|    policy_gradient_loss | -0.0136    |\n",
      "|    value_loss           | 0.0368     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4128228   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 417         |\n",
      "|    time_elapsed         | 21131       |\n",
      "|    total_timesteps      | 312750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009410633 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.937      |\n",
      "|    explained_variance   | 0.00826     |\n",
      "|    learning_rate        | 9.22e-05    |\n",
      "|    loss                 | -0.035      |\n",
      "|    n_updates            | 12480       |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    value_loss           | 0.0755      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4792106   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.47        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 418         |\n",
      "|    time_elapsed         | 21181       |\n",
      "|    total_timesteps      | 313500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009321903 |\n",
      "|    clip_fraction        | 0.0876      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.948      |\n",
      "|    explained_variance   | 0.0281      |\n",
      "|    learning_rate        | 9.22e-05    |\n",
      "|    loss                 | -0.0242     |\n",
      "|    n_updates            | 12510       |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    value_loss           | 0.0865      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4486191   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 419         |\n",
      "|    time_elapsed         | 21231       |\n",
      "|    total_timesteps      | 314250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013487242 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.935      |\n",
      "|    explained_variance   | 0.117       |\n",
      "|    learning_rate        | 9.22e-05    |\n",
      "|    loss                 | -0.0505     |\n",
      "|    n_updates            | 12540       |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    value_loss           | 0.0354      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=315000, episode_reward=1.59 +/- 0.04\n",
      "Episode length: 15.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 15         |\n",
      "|    mean_reward          | 1.59       |\n",
      "| mean_reward             | 1.4213191  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 315000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01045542 |\n",
      "|    clip_fraction        | 0.108      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.947     |\n",
      "|    explained_variance   | 0.0193     |\n",
      "|    learning_rate        | 9.21e-05   |\n",
      "|    loss                 | -0.0106    |\n",
      "|    n_updates            | 12570      |\n",
      "|    policy_gradient_loss | -0.0273    |\n",
      "|    value_loss           | 0.132      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15       |\n",
      "|    ep_rew_mean     | 1.43     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 420      |\n",
      "|    time_elapsed    | 21286    |\n",
      "|    total_timesteps | 315000   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.456295    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.49        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 421         |\n",
      "|    time_elapsed         | 21336       |\n",
      "|    total_timesteps      | 315750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017686166 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.952      |\n",
      "|    explained_variance   | 0.0897      |\n",
      "|    learning_rate        | 9.21e-05    |\n",
      "|    loss                 | -0.0504     |\n",
      "|    n_updates            | 12600       |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    value_loss           | 0.0422      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4463893   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 422         |\n",
      "|    time_elapsed         | 21387       |\n",
      "|    total_timesteps      | 316500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010114284 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.984      |\n",
      "|    explained_variance   | 0.0872      |\n",
      "|    learning_rate        | 9.21e-05    |\n",
      "|    loss                 | -0.0488     |\n",
      "|    n_updates            | 12630       |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    value_loss           | 0.0402      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2570107   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.28        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 423         |\n",
      "|    time_elapsed         | 21437       |\n",
      "|    total_timesteps      | 317250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012607067 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.979      |\n",
      "|    explained_variance   | -0.0219     |\n",
      "|    learning_rate        | 9.21e-05    |\n",
      "|    loss                 | 0.0271      |\n",
      "|    n_updates            | 12660       |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    value_loss           | 0.223       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2868079   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.32        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 424         |\n",
      "|    time_elapsed         | 21487       |\n",
      "|    total_timesteps      | 318000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018386442 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.962      |\n",
      "|    explained_variance   | 0.00627     |\n",
      "|    learning_rate        | 9.21e-05    |\n",
      "|    loss                 | -0.0138     |\n",
      "|    n_updates            | 12690       |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2580969   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.3         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 425         |\n",
      "|    time_elapsed         | 21537       |\n",
      "|    total_timesteps      | 318750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012498554 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.955      |\n",
      "|    explained_variance   | 0.0201      |\n",
      "|    learning_rate        | 9.21e-05    |\n",
      "|    loss                 | -0.018      |\n",
      "|    n_updates            | 12720       |\n",
      "|    policy_gradient_loss | -0.0319     |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3975726   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.37        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 426         |\n",
      "|    time_elapsed         | 21587       |\n",
      "|    total_timesteps      | 319500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013381488 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.979      |\n",
      "|    explained_variance   | -0.0371     |\n",
      "|    learning_rate        | 9.2e-05     |\n",
      "|    loss                 | -0.00559    |\n",
      "|    n_updates            | 12750       |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    value_loss           | 0.148       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=320000, episode_reward=1.59 +/- 0.04\n",
      "Episode length: 15.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 15         |\n",
      "|    mean_reward          | 1.59       |\n",
      "| mean_reward             | 1.4306215  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 320000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01505064 |\n",
      "|    clip_fraction        | 0.141      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.935     |\n",
      "|    explained_variance   | 0.0625     |\n",
      "|    learning_rate        | 9.2e-05    |\n",
      "|    loss                 | -0.0351    |\n",
      "|    n_updates            | 12780      |\n",
      "|    policy_gradient_loss | -0.0295    |\n",
      "|    value_loss           | 0.0846     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.37     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 427      |\n",
      "|    time_elapsed    | 21642    |\n",
      "|    total_timesteps | 320250   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| mean_reward             | 1.3313028 |\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 15.1      |\n",
      "|    ep_rew_mean          | 1.4       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 14        |\n",
      "|    iterations           | 428       |\n",
      "|    time_elapsed         | 21692     |\n",
      "|    total_timesteps      | 321000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0108621 |\n",
      "|    clip_fraction        | 0.109     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.947    |\n",
      "|    explained_variance   | -0.0811   |\n",
      "|    learning_rate        | 9.2e-05   |\n",
      "|    loss                 | -0.0221   |\n",
      "|    n_updates            | 12810     |\n",
      "|    policy_gradient_loss | -0.0248   |\n",
      "|    value_loss           | 0.105     |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3921721   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 429         |\n",
      "|    time_elapsed         | 21742       |\n",
      "|    total_timesteps      | 321750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009867951 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.953      |\n",
      "|    explained_variance   | 0.058       |\n",
      "|    learning_rate        | 9.2e-05     |\n",
      "|    loss                 | -0.0497     |\n",
      "|    n_updates            | 12840       |\n",
      "|    policy_gradient_loss | -0.0248     |\n",
      "|    value_loss           | 0.0446      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.468897    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 430         |\n",
      "|    time_elapsed         | 21792       |\n",
      "|    total_timesteps      | 322500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014258166 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.958      |\n",
      "|    explained_variance   | 0.0102      |\n",
      "|    learning_rate        | 9.2e-05     |\n",
      "|    loss                 | -0.0326     |\n",
      "|    n_updates            | 12870       |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    value_loss           | 0.0953      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3478595   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.38        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 431         |\n",
      "|    time_elapsed         | 21842       |\n",
      "|    total_timesteps      | 323250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011001965 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.953      |\n",
      "|    explained_variance   | 0.128       |\n",
      "|    learning_rate        | 9.19e-05    |\n",
      "|    loss                 | -0.0494     |\n",
      "|    n_updates            | 12900       |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    value_loss           | 0.0496      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3601948   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.29        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 432         |\n",
      "|    time_elapsed         | 21893       |\n",
      "|    total_timesteps      | 324000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012470618 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.927      |\n",
      "|    explained_variance   | 0.0181      |\n",
      "|    learning_rate        | 9.19e-05    |\n",
      "|    loss                 | -0.0108     |\n",
      "|    n_updates            | 12930       |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    value_loss           | 0.143       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.2869052  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15         |\n",
      "|    ep_rew_mean          | 1.37       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 433        |\n",
      "|    time_elapsed         | 21943      |\n",
      "|    total_timesteps      | 324750     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01316759 |\n",
      "|    clip_fraction        | 0.129      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.908     |\n",
      "|    explained_variance   | -0.0212    |\n",
      "|    learning_rate        | 9.19e-05   |\n",
      "|    loss                 | -0.0121    |\n",
      "|    n_updates            | 12960      |\n",
      "|    policy_gradient_loss | -0.0266    |\n",
      "|    value_loss           | 0.134      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=325000, episode_reward=1.62 +/- 0.01\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.62        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 325000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018146966 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.943      |\n",
      "|    explained_variance   | -0.00334    |\n",
      "|    learning_rate        | 9.19e-05    |\n",
      "|    loss                 | -0.0423     |\n",
      "|    n_updates            | 12990       |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    value_loss           | 0.0639      |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| mean_reward        | 1.4785409 |\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 15.1      |\n",
      "|    ep_rew_mean     | 1.43      |\n",
      "| time/              |           |\n",
      "|    fps             | 14        |\n",
      "|    iterations      | 434       |\n",
      "|    time_elapsed    | 21998     |\n",
      "|    total_timesteps | 325500    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3510784   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 435         |\n",
      "|    time_elapsed         | 22048       |\n",
      "|    total_timesteps      | 326250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016237108 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.959      |\n",
      "|    explained_variance   | 0.0329      |\n",
      "|    learning_rate        | 9.19e-05    |\n",
      "|    loss                 | -0.0392     |\n",
      "|    n_updates            | 13020       |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    value_loss           | 0.0692      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4344115   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 436         |\n",
      "|    time_elapsed         | 22098       |\n",
      "|    total_timesteps      | 327000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013093981 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.967      |\n",
      "|    explained_variance   | 0.0352      |\n",
      "|    learning_rate        | 9.18e-05    |\n",
      "|    loss                 | -0.0373     |\n",
      "|    n_updates            | 13050       |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    value_loss           | 0.0837      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.485904    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.48        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 437         |\n",
      "|    time_elapsed         | 22148       |\n",
      "|    total_timesteps      | 327750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012551972 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.971      |\n",
      "|    explained_variance   | 0.013       |\n",
      "|    learning_rate        | 9.18e-05    |\n",
      "|    loss                 | -0.0374     |\n",
      "|    n_updates            | 13080       |\n",
      "|    policy_gradient_loss | -0.0239     |\n",
      "|    value_loss           | 0.0665      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4941117   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 438         |\n",
      "|    time_elapsed         | 22198       |\n",
      "|    total_timesteps      | 328500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011940913 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.97       |\n",
      "|    explained_variance   | 0.118       |\n",
      "|    learning_rate        | 9.18e-05    |\n",
      "|    loss                 | -0.0528     |\n",
      "|    n_updates            | 13110       |\n",
      "|    policy_gradient_loss | -0.0215     |\n",
      "|    value_loss           | 0.0277      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3545141   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.37        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 439         |\n",
      "|    time_elapsed         | 22248       |\n",
      "|    total_timesteps      | 329250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010696112 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.964      |\n",
      "|    explained_variance   | -0.0354     |\n",
      "|    learning_rate        | 9.18e-05    |\n",
      "|    loss                 | -0.0405     |\n",
      "|    n_updates            | 13140       |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    value_loss           | 0.0759      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=330000, episode_reward=1.37 +/- 0.38\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.37        |\n",
      "| mean_reward             | 1.3769509   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 330000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011201967 |\n",
      "|    clip_fraction        | 0.0985      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.954      |\n",
      "|    explained_variance   | 0.0127      |\n",
      "|    learning_rate        | 9.18e-05    |\n",
      "|    loss                 | -0.0329     |\n",
      "|    n_updates            | 13170       |\n",
      "|    policy_gradient_loss | -0.0245     |\n",
      "|    value_loss           | 0.0813      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15       |\n",
      "|    ep_rew_mean     | 1.38     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 440      |\n",
      "|    time_elapsed    | 22303    |\n",
      "|    total_timesteps | 330000   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4400606   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 441         |\n",
      "|    time_elapsed         | 22354       |\n",
      "|    total_timesteps      | 330750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013589978 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.961      |\n",
      "|    explained_variance   | 0.0306      |\n",
      "|    learning_rate        | 9.18e-05    |\n",
      "|    loss                 | -0.032      |\n",
      "|    n_updates            | 13200       |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    value_loss           | 0.0911      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4451183   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.48        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 442         |\n",
      "|    time_elapsed         | 22404       |\n",
      "|    total_timesteps      | 331500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011691131 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.929      |\n",
      "|    explained_variance   | 0.149       |\n",
      "|    learning_rate        | 9.17e-05    |\n",
      "|    loss                 | -0.0451     |\n",
      "|    n_updates            | 13230       |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    value_loss           | 0.0364      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.5390961   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.46        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 443         |\n",
      "|    time_elapsed         | 22454       |\n",
      "|    total_timesteps      | 332250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012049048 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.934      |\n",
      "|    explained_variance   | 0.0735      |\n",
      "|    learning_rate        | 9.17e-05    |\n",
      "|    loss                 | -0.0458     |\n",
      "|    n_updates            | 13260       |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    value_loss           | 0.0467      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| mean_reward             | 1.3985649 |\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 15        |\n",
      "|    ep_rew_mean          | 1.44      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 14        |\n",
      "|    iterations           | 444       |\n",
      "|    time_elapsed         | 22504     |\n",
      "|    total_timesteps      | 333000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0136452 |\n",
      "|    clip_fraction        | 0.147     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.925    |\n",
      "|    explained_variance   | 0.0209    |\n",
      "|    learning_rate        | 9.17e-05  |\n",
      "|    loss                 | -0.0435   |\n",
      "|    n_updates            | 13290     |\n",
      "|    policy_gradient_loss | -0.0235   |\n",
      "|    value_loss           | 0.0518    |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.474242    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.49        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 445         |\n",
      "|    time_elapsed         | 22555       |\n",
      "|    total_timesteps      | 333750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013580733 |\n",
      "|    clip_fraction        | 0.0924      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.916      |\n",
      "|    explained_variance   | 0.0246      |\n",
      "|    learning_rate        | 9.17e-05    |\n",
      "|    loss                 | -0.0288     |\n",
      "|    n_updates            | 13320       |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    value_loss           | 0.0829      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.5236017  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15         |\n",
      "|    ep_rew_mean          | 1.51       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 446        |\n",
      "|    time_elapsed         | 22605      |\n",
      "|    total_timesteps      | 334500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01286431 |\n",
      "|    clip_fraction        | 0.169      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.954     |\n",
      "|    explained_variance   | 0.302      |\n",
      "|    learning_rate        | 9.17e-05   |\n",
      "|    loss                 | -0.0559    |\n",
      "|    n_updates            | 13350      |\n",
      "|    policy_gradient_loss | -0.0194    |\n",
      "|    value_loss           | 0.0115     |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=335000, episode_reward=1.57 +/- 0.04\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.57        |\n",
      "| mean_reward             | 1.4650362   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 335000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021353483 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.956      |\n",
      "|    explained_variance   | -0.0101     |\n",
      "|    learning_rate        | 9.16e-05    |\n",
      "|    loss                 | -0.0193     |\n",
      "|    n_updates            | 13380       |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    value_loss           | 0.0881      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.44     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 447      |\n",
      "|    time_elapsed    | 22660    |\n",
      "|    total_timesteps | 335250   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3919185   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 448         |\n",
      "|    time_elapsed         | 22710       |\n",
      "|    total_timesteps      | 336000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010903474 |\n",
      "|    clip_fraction        | 0.0899      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.996      |\n",
      "|    explained_variance   | 0.0253      |\n",
      "|    learning_rate        | 9.16e-05    |\n",
      "|    loss                 | -0.0385     |\n",
      "|    n_updates            | 13410       |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    value_loss           | 0.0662      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4677478   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.46        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 449         |\n",
      "|    time_elapsed         | 22760       |\n",
      "|    total_timesteps      | 336750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015107249 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.981      |\n",
      "|    explained_variance   | 0.0384      |\n",
      "|    learning_rate        | 9.16e-05    |\n",
      "|    loss                 | -0.0477     |\n",
      "|    n_updates            | 13440       |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    value_loss           | 0.056       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.4230461  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15         |\n",
      "|    ep_rew_mean          | 1.43       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 450        |\n",
      "|    time_elapsed         | 22810      |\n",
      "|    total_timesteps      | 337500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02014268 |\n",
      "|    clip_fraction        | 0.215      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.998     |\n",
      "|    explained_variance   | 0.129      |\n",
      "|    learning_rate        | 9.16e-05   |\n",
      "|    loss                 | -0.0588    |\n",
      "|    n_updates            | 13470      |\n",
      "|    policy_gradient_loss | -0.0295    |\n",
      "|    value_loss           | 0.0329     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.3662144  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15         |\n",
      "|    ep_rew_mean          | 1.38       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 451        |\n",
      "|    time_elapsed         | 22860      |\n",
      "|    total_timesteps      | 338250     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01765336 |\n",
      "|    clip_fraction        | 0.176      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.997     |\n",
      "|    explained_variance   | -0.00235   |\n",
      "|    learning_rate        | 9.16e-05   |\n",
      "|    loss                 | -0.0203    |\n",
      "|    n_updates            | 13500      |\n",
      "|    policy_gradient_loss | -0.0316    |\n",
      "|    value_loss           | 0.127      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4677259   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 452         |\n",
      "|    time_elapsed         | 22910       |\n",
      "|    total_timesteps      | 339000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013314955 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0.0375      |\n",
      "|    learning_rate        | 9.15e-05    |\n",
      "|    loss                 | -0.0297     |\n",
      "|    n_updates            | 13530       |\n",
      "|    policy_gradient_loss | -0.024      |\n",
      "|    value_loss           | 0.0882      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4435407   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.48        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 453         |\n",
      "|    time_elapsed         | 22961       |\n",
      "|    total_timesteps      | 339750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013222537 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.136       |\n",
      "|    learning_rate        | 9.15e-05    |\n",
      "|    loss                 | -0.0463     |\n",
      "|    n_updates            | 13560       |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    value_loss           | 0.0634      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=340000, episode_reward=1.37 +/- 0.41\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.37        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 340000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017335769 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 9.15e-05    |\n",
      "|    loss                 | -0.0635     |\n",
      "|    n_updates            | 13590       |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    value_loss           | 0.0221      |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| mean_reward        | 1.4642303 |\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 15.1      |\n",
      "|    ep_rew_mean     | 1.39      |\n",
      "| time/              |           |\n",
      "|    fps             | 14        |\n",
      "|    iterations      | 454       |\n",
      "|    time_elapsed    | 23016     |\n",
      "|    total_timesteps | 340500    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2440784   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.3         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 455         |\n",
      "|    time_elapsed         | 23066       |\n",
      "|    total_timesteps      | 341250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011835207 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | -0.0154     |\n",
      "|    learning_rate        | 9.15e-05    |\n",
      "|    loss                 | -0.0111     |\n",
      "|    n_updates            | 13620       |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    value_loss           | 0.142       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3750209   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.35        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 456         |\n",
      "|    time_elapsed         | 23116       |\n",
      "|    total_timesteps      | 342000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015193568 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.027       |\n",
      "|    learning_rate        | 9.15e-05    |\n",
      "|    loss                 | -0.0208     |\n",
      "|    n_updates            | 13650       |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    value_loss           | 0.113       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2961029   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.32        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 457         |\n",
      "|    time_elapsed         | 23166       |\n",
      "|    total_timesteps      | 342750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011694257 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.996      |\n",
      "|    explained_variance   | 0.00336     |\n",
      "|    learning_rate        | 9.15e-05    |\n",
      "|    loss                 | -0.0393     |\n",
      "|    n_updates            | 13680       |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    value_loss           | 0.0787      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3603133   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.35        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 458         |\n",
      "|    time_elapsed         | 23217       |\n",
      "|    total_timesteps      | 343500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012919964 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.993      |\n",
      "|    explained_variance   | 0.00496     |\n",
      "|    learning_rate        | 9.14e-05    |\n",
      "|    loss                 | -0.0149     |\n",
      "|    n_updates            | 13710       |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    value_loss           | 0.145       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4256533   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 459         |\n",
      "|    time_elapsed         | 23267       |\n",
      "|    total_timesteps      | 344250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012528663 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.979      |\n",
      "|    explained_variance   | 0.00336     |\n",
      "|    learning_rate        | 9.14e-05    |\n",
      "|    loss                 | -0.0394     |\n",
      "|    n_updates            | 13740       |\n",
      "|    policy_gradient_loss | -0.0224     |\n",
      "|    value_loss           | 0.0645      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=345000, episode_reward=1.54 +/- 0.04\n",
      "Episode length: 15.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 15           |\n",
      "|    mean_reward          | 1.54         |\n",
      "| mean_reward             | 1.4862311    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 345000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0148176355 |\n",
      "|    clip_fraction        | 0.158        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.99        |\n",
      "|    explained_variance   | 0.061        |\n",
      "|    learning_rate        | 9.14e-05     |\n",
      "|    loss                 | -0.0511      |\n",
      "|    n_updates            | 13770        |\n",
      "|    policy_gradient_loss | -0.0257      |\n",
      "|    value_loss           | 0.0457       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15       |\n",
      "|    ep_rew_mean     | 1.44     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 460      |\n",
      "|    time_elapsed    | 23322    |\n",
      "|    total_timesteps | 345000   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4018204   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 461         |\n",
      "|    time_elapsed         | 23372       |\n",
      "|    total_timesteps      | 345750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018970553 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0.0458      |\n",
      "|    learning_rate        | 9.14e-05    |\n",
      "|    loss                 | -0.0397     |\n",
      "|    n_updates            | 13800       |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    value_loss           | 0.0804      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3778853   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 462         |\n",
      "|    time_elapsed         | 23422       |\n",
      "|    total_timesteps      | 346500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020515399 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.992      |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 9.14e-05    |\n",
      "|    loss                 | -0.0554     |\n",
      "|    n_updates            | 13830       |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    value_loss           | 0.0378      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.4505413  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15         |\n",
      "|    ep_rew_mean          | 1.41       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 463        |\n",
      "|    time_elapsed         | 23473      |\n",
      "|    total_timesteps      | 347250     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03235597 |\n",
      "|    clip_fraction        | 0.128      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.987     |\n",
      "|    explained_variance   | -0.00436   |\n",
      "|    learning_rate        | 9.13e-05   |\n",
      "|    loss                 | -0.0235    |\n",
      "|    n_updates            | 13860      |\n",
      "|    policy_gradient_loss | -0.0254    |\n",
      "|    value_loss           | 0.104      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4130847   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 464         |\n",
      "|    time_elapsed         | 23523       |\n",
      "|    total_timesteps      | 348000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016219776 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.999      |\n",
      "|    explained_variance   | 0.0191      |\n",
      "|    learning_rate        | 9.13e-05    |\n",
      "|    loss                 | -0.0369     |\n",
      "|    n_updates            | 13890       |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    value_loss           | 0.0763      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4154025   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 465         |\n",
      "|    time_elapsed         | 23573       |\n",
      "|    total_timesteps      | 348750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011396797 |\n",
      "|    clip_fraction        | 0.0965      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.986      |\n",
      "|    explained_variance   | -0.0152     |\n",
      "|    learning_rate        | 9.13e-05    |\n",
      "|    loss                 | -0.0386     |\n",
      "|    n_updates            | 13920       |\n",
      "|    policy_gradient_loss | -0.024      |\n",
      "|    value_loss           | 0.0689      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.2790802  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15         |\n",
      "|    ep_rew_mean          | 1.33       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 466        |\n",
      "|    time_elapsed         | 23623      |\n",
      "|    total_timesteps      | 349500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01746902 |\n",
      "|    clip_fraction        | 0.127      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.978     |\n",
      "|    explained_variance   | 0.00509    |\n",
      "|    learning_rate        | 9.13e-05   |\n",
      "|    loss                 | -0.0141    |\n",
      "|    n_updates            | 13950      |\n",
      "|    policy_gradient_loss | -0.0273    |\n",
      "|    value_loss           | 0.132      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=350000, episode_reward=1.59 +/- 0.03\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.59        |\n",
      "| mean_reward             | 1.2966552   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 350000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012628198 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0.000722    |\n",
      "|    learning_rate        | 9.13e-05    |\n",
      "|    loss                 | -0.0286     |\n",
      "|    n_updates            | 13980       |\n",
      "|    policy_gradient_loss | -0.0244     |\n",
      "|    value_loss           | 0.0903      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.37     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 467      |\n",
      "|    time_elapsed    | 23678    |\n",
      "|    total_timesteps | 350250   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4265709   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.36        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 468         |\n",
      "|    time_elapsed         | 23729       |\n",
      "|    total_timesteps      | 351000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017685782 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.989      |\n",
      "|    explained_variance   | -0.0603     |\n",
      "|    learning_rate        | 9.12e-05    |\n",
      "|    loss                 | -0.0358     |\n",
      "|    n_updates            | 14010       |\n",
      "|    policy_gradient_loss | -0.0325     |\n",
      "|    value_loss           | 0.0962      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2876353   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.38        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 469         |\n",
      "|    time_elapsed         | 23779       |\n",
      "|    total_timesteps      | 351750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012103593 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.974      |\n",
      "|    explained_variance   | 0.038       |\n",
      "|    learning_rate        | 9.12e-05    |\n",
      "|    loss                 | -0.0309     |\n",
      "|    n_updates            | 14040       |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    value_loss           | 0.0946      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3874974   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.38        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 470         |\n",
      "|    time_elapsed         | 23829       |\n",
      "|    total_timesteps      | 352500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013203352 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.954      |\n",
      "|    explained_variance   | 0.00646     |\n",
      "|    learning_rate        | 9.12e-05    |\n",
      "|    loss                 | -0.0226     |\n",
      "|    n_updates            | 14070       |\n",
      "|    policy_gradient_loss | -0.0252     |\n",
      "|    value_loss           | 0.102       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4464364   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 471         |\n",
      "|    time_elapsed         | 23879       |\n",
      "|    total_timesteps      | 353250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018056521 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.952      |\n",
      "|    explained_variance   | 0.0483      |\n",
      "|    learning_rate        | 9.12e-05    |\n",
      "|    loss                 | -0.0349     |\n",
      "|    n_updates            | 14100       |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    value_loss           | 0.0891      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4657538   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.46        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 472         |\n",
      "|    time_elapsed         | 23929       |\n",
      "|    total_timesteps      | 354000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011869336 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.93       |\n",
      "|    explained_variance   | -0.0498     |\n",
      "|    learning_rate        | 9.12e-05    |\n",
      "|    loss                 | -0.0495     |\n",
      "|    n_updates            | 14130       |\n",
      "|    policy_gradient_loss | -0.0198     |\n",
      "|    value_loss           | 0.0329      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.3861306  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15         |\n",
      "|    ep_rew_mean          | 1.39       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 473        |\n",
      "|    time_elapsed         | 23979      |\n",
      "|    total_timesteps      | 354750     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01613248 |\n",
      "|    clip_fraction        | 0.156      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.936     |\n",
      "|    explained_variance   | 0.0424     |\n",
      "|    learning_rate        | 9.11e-05   |\n",
      "|    loss                 | -0.0452    |\n",
      "|    n_updates            | 14160      |\n",
      "|    policy_gradient_loss | -0.027     |\n",
      "|    value_loss           | 0.0607     |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=355000, episode_reward=1.57 +/- 0.02\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.57        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 355000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012674956 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.943      |\n",
      "|    explained_variance   | 0.064       |\n",
      "|    learning_rate        | 9.11e-05    |\n",
      "|    loss                 | -0.0317     |\n",
      "|    n_updates            | 14190       |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    value_loss           | 0.0838      |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| mean_reward        | 1.3767554 |\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 15.1      |\n",
      "|    ep_rew_mean     | 1.39      |\n",
      "| time/              |           |\n",
      "|    fps             | 14        |\n",
      "|    iterations      | 474       |\n",
      "|    time_elapsed    | 24034     |\n",
      "|    total_timesteps | 355500    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.5066168   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 475         |\n",
      "|    time_elapsed         | 24085       |\n",
      "|    total_timesteps      | 356250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010900129 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.964      |\n",
      "|    explained_variance   | -0.00433    |\n",
      "|    learning_rate        | 9.11e-05    |\n",
      "|    loss                 | -0.0291     |\n",
      "|    n_updates            | 14220       |\n",
      "|    policy_gradient_loss | -0.0251     |\n",
      "|    value_loss           | 0.0897      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3648388   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 476         |\n",
      "|    time_elapsed         | 24135       |\n",
      "|    total_timesteps      | 357000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014913722 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.962      |\n",
      "|    explained_variance   | 0.0984      |\n",
      "|    learning_rate        | 9.11e-05    |\n",
      "|    loss                 | -0.0435     |\n",
      "|    n_updates            | 14250       |\n",
      "|    policy_gradient_loss | -0.0253     |\n",
      "|    value_loss           | 0.0537      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3544796   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.36        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 477         |\n",
      "|    time_elapsed         | 24185       |\n",
      "|    total_timesteps      | 357750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013279768 |\n",
      "|    clip_fraction        | 0.0984      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.937      |\n",
      "|    explained_variance   | 0.0255      |\n",
      "|    learning_rate        | 9.11e-05    |\n",
      "|    loss                 | -0.02       |\n",
      "|    n_updates            | 14280       |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    value_loss           | 0.117       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4506552   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.4         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 478         |\n",
      "|    time_elapsed         | 24235       |\n",
      "|    total_timesteps      | 358500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013178014 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.932      |\n",
      "|    explained_variance   | 0.128       |\n",
      "|    learning_rate        | 9.11e-05    |\n",
      "|    loss                 | -0.0432     |\n",
      "|    n_updates            | 14310       |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    value_loss           | 0.0682      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4066195   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 479         |\n",
      "|    time_elapsed         | 24285       |\n",
      "|    total_timesteps      | 359250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011913262 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.948      |\n",
      "|    explained_variance   | 0.0778      |\n",
      "|    learning_rate        | 9.1e-05     |\n",
      "|    loss                 | -0.0347     |\n",
      "|    n_updates            | 14340       |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    value_loss           | 0.086       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=360000, episode_reward=1.55 +/- 0.05\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.55        |\n",
      "| mean_reward             | 1.4532131   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 360000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009817301 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.91       |\n",
      "|    explained_variance   | -0.146      |\n",
      "|    learning_rate        | 9.1e-05     |\n",
      "|    loss                 | -0.0298     |\n",
      "|    n_updates            | 14370       |\n",
      "|    policy_gradient_loss | -0.0207     |\n",
      "|    value_loss           | 0.079       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15       |\n",
      "|    ep_rew_mean     | 1.45     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 480      |\n",
      "|    time_elapsed    | 24340    |\n",
      "|    total_timesteps | 360000   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4198605   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 481         |\n",
      "|    time_elapsed         | 24391       |\n",
      "|    total_timesteps      | 360750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012643856 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.945      |\n",
      "|    explained_variance   | 0.0104      |\n",
      "|    learning_rate        | 9.1e-05     |\n",
      "|    loss                 | -0.0457     |\n",
      "|    n_updates            | 14400       |\n",
      "|    policy_gradient_loss | -0.0221     |\n",
      "|    value_loss           | 0.046       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4346495   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 482         |\n",
      "|    time_elapsed         | 24441       |\n",
      "|    total_timesteps      | 361500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022068692 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.967      |\n",
      "|    explained_variance   | 0.0275      |\n",
      "|    learning_rate        | 9.1e-05     |\n",
      "|    loss                 | -0.0295     |\n",
      "|    n_updates            | 14430       |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    value_loss           | 0.092       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| mean_reward             | 1.4242444    |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15           |\n",
      "|    ep_rew_mean          | 1.43         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 483          |\n",
      "|    time_elapsed         | 24491        |\n",
      "|    total_timesteps      | 362250       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0148800025 |\n",
      "|    clip_fraction        | 0.103        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.94        |\n",
      "|    explained_variance   | 0.0444       |\n",
      "|    learning_rate        | 9.1e-05      |\n",
      "|    loss                 | -0.0452      |\n",
      "|    n_updates            | 14460        |\n",
      "|    policy_gradient_loss | -0.0232      |\n",
      "|    value_loss           | 0.051        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4490101   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 484         |\n",
      "|    time_elapsed         | 24541       |\n",
      "|    total_timesteps      | 363000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013084787 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.917      |\n",
      "|    explained_variance   | -0.0304     |\n",
      "|    learning_rate        | 9.09e-05    |\n",
      "|    loss                 | -0.0306     |\n",
      "|    n_updates            | 14490       |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    value_loss           | 0.0858      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4038211   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 485         |\n",
      "|    time_elapsed         | 24591       |\n",
      "|    total_timesteps      | 363750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012244321 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.909      |\n",
      "|    explained_variance   | 0.0515      |\n",
      "|    learning_rate        | 9.09e-05    |\n",
      "|    loss                 | -0.0292     |\n",
      "|    n_updates            | 14520       |\n",
      "|    policy_gradient_loss | -0.019      |\n",
      "|    value_loss           | 0.0617      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4315823   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 486         |\n",
      "|    time_elapsed         | 24642       |\n",
      "|    total_timesteps      | 364500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013415174 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.906      |\n",
      "|    explained_variance   | 0.0617      |\n",
      "|    learning_rate        | 9.09e-05    |\n",
      "|    loss                 | -0.0371     |\n",
      "|    n_updates            | 14550       |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    value_loss           | 0.0732      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=365000, episode_reward=1.59 +/- 0.02\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.59        |\n",
      "| mean_reward             | 1.5093747   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 365000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017112682 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.903      |\n",
      "|    explained_variance   | 0.0499      |\n",
      "|    learning_rate        | 9.09e-05    |\n",
      "|    loss                 | -0.045      |\n",
      "|    n_updates            | 14580       |\n",
      "|    policy_gradient_loss | -0.0216     |\n",
      "|    value_loss           | 0.0376      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.5      |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 487      |\n",
      "|    time_elapsed    | 24697    |\n",
      "|    total_timesteps | 365250   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.5328168   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.46        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 488         |\n",
      "|    time_elapsed         | 24747       |\n",
      "|    total_timesteps      | 366000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008121465 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.945      |\n",
      "|    explained_variance   | 0.132       |\n",
      "|    learning_rate        | 9.09e-05    |\n",
      "|    loss                 | -0.0481     |\n",
      "|    n_updates            | 14610       |\n",
      "|    policy_gradient_loss | -0.0166     |\n",
      "|    value_loss           | 0.0187      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3768756   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 489         |\n",
      "|    time_elapsed         | 24797       |\n",
      "|    total_timesteps      | 366750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011550384 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.945      |\n",
      "|    explained_variance   | 0.00739     |\n",
      "|    learning_rate        | 9.08e-05    |\n",
      "|    loss                 | -0.0176     |\n",
      "|    n_updates            | 14640       |\n",
      "|    policy_gradient_loss | -0.0227     |\n",
      "|    value_loss           | 0.0993      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4298956   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 490         |\n",
      "|    time_elapsed         | 24848       |\n",
      "|    total_timesteps      | 367500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011162111 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.949      |\n",
      "|    explained_variance   | 0.0589      |\n",
      "|    learning_rate        | 9.08e-05    |\n",
      "|    loss                 | -0.0435     |\n",
      "|    n_updates            | 14670       |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    value_loss           | 0.0544      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.5018401  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15         |\n",
      "|    ep_rew_mean          | 1.48       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 491        |\n",
      "|    time_elapsed         | 24898      |\n",
      "|    total_timesteps      | 368250     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01287183 |\n",
      "|    clip_fraction        | 0.118      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.96      |\n",
      "|    explained_variance   | 0.107      |\n",
      "|    learning_rate        | 9.08e-05   |\n",
      "|    loss                 | -0.0381    |\n",
      "|    n_updates            | 14700      |\n",
      "|    policy_gradient_loss | -0.0256    |\n",
      "|    value_loss           | 0.0677     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.5046563   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.51        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 492         |\n",
      "|    time_elapsed         | 24948       |\n",
      "|    total_timesteps      | 369000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013151823 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.961      |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 9.08e-05    |\n",
      "|    loss                 | -0.0566     |\n",
      "|    n_updates            | 14730       |\n",
      "|    policy_gradient_loss | -0.0227     |\n",
      "|    value_loss           | 0.021       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4685043   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.47        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 493         |\n",
      "|    time_elapsed         | 24998       |\n",
      "|    total_timesteps      | 369750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007544693 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.965      |\n",
      "|    explained_variance   | 0.105       |\n",
      "|    learning_rate        | 9.08e-05    |\n",
      "|    loss                 | -0.0445     |\n",
      "|    n_updates            | 14760       |\n",
      "|    policy_gradient_loss | -0.019      |\n",
      "|    value_loss           | 0.0352      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=370000, episode_reward=1.60 +/- 0.03\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.6         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 370000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014733718 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.968      |\n",
      "|    explained_variance   | 0.0562      |\n",
      "|    learning_rate        | 9.08e-05    |\n",
      "|    loss                 | -0.0353     |\n",
      "|    n_updates            | 14790       |\n",
      "|    policy_gradient_loss | -0.0245     |\n",
      "|    value_loss           | 0.0702      |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| mean_reward        | 1.3899502 |\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 15.1      |\n",
      "|    ep_rew_mean     | 1.41      |\n",
      "| time/              |           |\n",
      "|    fps             | 14        |\n",
      "|    iterations      | 494       |\n",
      "|    time_elapsed    | 25053     |\n",
      "|    total_timesteps | 370500    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3649029   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.36        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 495         |\n",
      "|    time_elapsed         | 25104       |\n",
      "|    total_timesteps      | 371250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012068184 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.965      |\n",
      "|    explained_variance   | 0.00376     |\n",
      "|    learning_rate        | 9.07e-05    |\n",
      "|    loss                 | -0.0348     |\n",
      "|    n_updates            | 14820       |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    value_loss           | 0.0887      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3982028   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 496         |\n",
      "|    time_elapsed         | 25154       |\n",
      "|    total_timesteps      | 372000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015346444 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.963      |\n",
      "|    explained_variance   | -0.038      |\n",
      "|    learning_rate        | 9.07e-05    |\n",
      "|    loss                 | -0.0306     |\n",
      "|    n_updates            | 14850       |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    value_loss           | 0.0937      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.464216    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 497         |\n",
      "|    time_elapsed         | 25204       |\n",
      "|    total_timesteps      | 372750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018020287 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.961      |\n",
      "|    explained_variance   | 0.0353      |\n",
      "|    learning_rate        | 9.07e-05    |\n",
      "|    loss                 | -0.0362     |\n",
      "|    n_updates            | 14880       |\n",
      "|    policy_gradient_loss | -0.0214     |\n",
      "|    value_loss           | 0.0597      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3621358   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.38        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 498         |\n",
      "|    time_elapsed         | 25254       |\n",
      "|    total_timesteps      | 373500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014353351 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.989      |\n",
      "|    explained_variance   | 0.0171      |\n",
      "|    learning_rate        | 9.07e-05    |\n",
      "|    loss                 | -0.0212     |\n",
      "|    n_updates            | 14910       |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    value_loss           | 0.104       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3289824   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.36        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 499         |\n",
      "|    time_elapsed         | 25305       |\n",
      "|    total_timesteps      | 374250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010185449 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.955      |\n",
      "|    explained_variance   | 0.012       |\n",
      "|    learning_rate        | 9.07e-05    |\n",
      "|    loss                 | -0.0321     |\n",
      "|    n_updates            | 14940       |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    value_loss           | 0.0806      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=375000, episode_reward=1.58 +/- 0.02\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.58        |\n",
      "| mean_reward             | 1.4360232   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 375000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014526937 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.984      |\n",
      "|    explained_variance   | 0.0407      |\n",
      "|    learning_rate        | 9.06e-05    |\n",
      "|    loss                 | -0.0211     |\n",
      "|    n_updates            | 14970       |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    value_loss           | 0.113       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15       |\n",
      "|    ep_rew_mean     | 1.36     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 500      |\n",
      "|    time_elapsed    | 25360    |\n",
      "|    total_timesteps | 375000   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3137581   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.38        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 501         |\n",
      "|    time_elapsed         | 25410       |\n",
      "|    total_timesteps      | 375750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013406352 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.0404      |\n",
      "|    learning_rate        | 9.06e-05    |\n",
      "|    loss                 | -0.0399     |\n",
      "|    n_updates            | 15000       |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    value_loss           | 0.0733      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4442391   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.4         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 502         |\n",
      "|    time_elapsed         | 25460       |\n",
      "|    total_timesteps      | 376500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022833727 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.989      |\n",
      "|    explained_variance   | -0.0638     |\n",
      "|    learning_rate        | 9.06e-05    |\n",
      "|    loss                 | -0.0156     |\n",
      "|    n_updates            | 15030       |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    value_loss           | 0.129       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4271588   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.4         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 503         |\n",
      "|    time_elapsed         | 25511       |\n",
      "|    total_timesteps      | 377250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016403452 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | -0.0128     |\n",
      "|    learning_rate        | 9.06e-05    |\n",
      "|    loss                 | -0.022      |\n",
      "|    n_updates            | 15060       |\n",
      "|    policy_gradient_loss | -0.0218     |\n",
      "|    value_loss           | 0.0985      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| mean_reward             | 1.2978423    |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15           |\n",
      "|    ep_rew_mean          | 1.38         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 504          |\n",
      "|    time_elapsed         | 25561        |\n",
      "|    total_timesteps      | 378000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0143538285 |\n",
      "|    clip_fraction        | 0.141        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1           |\n",
      "|    explained_variance   | -0.027       |\n",
      "|    learning_rate        | 9.06e-05     |\n",
      "|    loss                 | -0.0387      |\n",
      "|    n_updates            | 15090        |\n",
      "|    policy_gradient_loss | -0.0253      |\n",
      "|    value_loss           | 0.0712       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.403475    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.37        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 505         |\n",
      "|    time_elapsed         | 25611       |\n",
      "|    total_timesteps      | 378750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012871134 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | -0.000111   |\n",
      "|    learning_rate        | 9.06e-05    |\n",
      "|    loss                 | -0.0402     |\n",
      "|    n_updates            | 15120       |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    value_loss           | 0.0741      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3423414   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.37        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 506         |\n",
      "|    time_elapsed         | 25661       |\n",
      "|    total_timesteps      | 379500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013502853 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.99       |\n",
      "|    explained_variance   | 0.0335      |\n",
      "|    learning_rate        | 9.05e-05    |\n",
      "|    loss                 | -0.0273     |\n",
      "|    n_updates            | 15150       |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    value_loss           | 0.0959      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=380000, episode_reward=1.57 +/- 0.02\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.57        |\n",
      "| mean_reward             | 1.463818    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 380000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011277763 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.993      |\n",
      "|    explained_variance   | -0.00414    |\n",
      "|    learning_rate        | 9.05e-05    |\n",
      "|    loss                 | -0.0301     |\n",
      "|    n_updates            | 15180       |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    value_loss           | 0.0924      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.4      |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 507      |\n",
      "|    time_elapsed    | 25717    |\n",
      "|    total_timesteps | 380250   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4275122   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.47        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 508         |\n",
      "|    time_elapsed         | 25767       |\n",
      "|    total_timesteps      | 381000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014956909 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.977      |\n",
      "|    explained_variance   | 0.117       |\n",
      "|    learning_rate        | 9.05e-05    |\n",
      "|    loss                 | -0.0465     |\n",
      "|    n_updates            | 15210       |\n",
      "|    policy_gradient_loss | -0.024      |\n",
      "|    value_loss           | 0.0537      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4270519   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 509         |\n",
      "|    time_elapsed         | 25817       |\n",
      "|    total_timesteps      | 381750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013846809 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.0708      |\n",
      "|    learning_rate        | 9.05e-05    |\n",
      "|    loss                 | -0.053      |\n",
      "|    n_updates            | 15240       |\n",
      "|    policy_gradient_loss | -0.0222     |\n",
      "|    value_loss           | 0.0302      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4177868   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 510         |\n",
      "|    time_elapsed         | 25867       |\n",
      "|    total_timesteps      | 382500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013304643 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | -0.0311     |\n",
      "|    learning_rate        | 9.05e-05    |\n",
      "|    loss                 | -0.0257     |\n",
      "|    n_updates            | 15270       |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    value_loss           | 0.108       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3849051   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 511         |\n",
      "|    time_elapsed         | 25918       |\n",
      "|    total_timesteps      | 383250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013235472 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.99       |\n",
      "|    explained_variance   | 0.0536      |\n",
      "|    learning_rate        | 9.04e-05    |\n",
      "|    loss                 | -0.0576     |\n",
      "|    n_updates            | 15300       |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    value_loss           | 0.0448      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4858648   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.47        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 512         |\n",
      "|    time_elapsed         | 25968       |\n",
      "|    total_timesteps      | 384000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011334438 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.0672      |\n",
      "|    learning_rate        | 9.04e-05    |\n",
      "|    loss                 | -0.0443     |\n",
      "|    n_updates            | 15330       |\n",
      "|    policy_gradient_loss | -0.0225     |\n",
      "|    value_loss           | 0.0493      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.5234791   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.5         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 513         |\n",
      "|    time_elapsed         | 26018       |\n",
      "|    total_timesteps      | 384750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013184588 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.117       |\n",
      "|    learning_rate        | 9.04e-05    |\n",
      "|    loss                 | -0.0531     |\n",
      "|    n_updates            | 15360       |\n",
      "|    policy_gradient_loss | -0.0243     |\n",
      "|    value_loss           | 0.0366      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=385000, episode_reward=1.57 +/- 0.05\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.57        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 385000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015096169 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.0832      |\n",
      "|    learning_rate        | 9.04e-05    |\n",
      "|    loss                 | -0.0526     |\n",
      "|    n_updates            | 15390       |\n",
      "|    policy_gradient_loss | -0.019      |\n",
      "|    value_loss           | 0.0259      |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| mean_reward        | 1.4326088 |\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 15.1      |\n",
      "|    ep_rew_mean     | 1.43      |\n",
      "| time/              |           |\n",
      "|    fps             | 14        |\n",
      "|    iterations      | 514       |\n",
      "|    time_elapsed    | 26073     |\n",
      "|    total_timesteps | 385500    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.382279    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 515         |\n",
      "|    time_elapsed         | 26124       |\n",
      "|    total_timesteps      | 386250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016921125 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.0248      |\n",
      "|    learning_rate        | 9.04e-05    |\n",
      "|    loss                 | -0.0229     |\n",
      "|    n_updates            | 15420       |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    value_loss           | 0.12        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4296826   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.4         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 516         |\n",
      "|    time_elapsed         | 26174       |\n",
      "|    total_timesteps      | 387000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012107834 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.139       |\n",
      "|    learning_rate        | 9.03e-05    |\n",
      "|    loss                 | -0.0522     |\n",
      "|    n_updates            | 15450       |\n",
      "|    policy_gradient_loss | -0.0212     |\n",
      "|    value_loss           | 0.0336      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.3177987  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15         |\n",
      "|    ep_rew_mean          | 1.3        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 517        |\n",
      "|    time_elapsed         | 26224      |\n",
      "|    total_timesteps      | 387750     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01035763 |\n",
      "|    clip_fraction        | 0.152      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.05      |\n",
      "|    explained_variance   | -0.0586    |\n",
      "|    learning_rate        | 9.03e-05   |\n",
      "|    loss                 | -0.0399    |\n",
      "|    n_updates            | 15480      |\n",
      "|    policy_gradient_loss | -0.029     |\n",
      "|    value_loss           | 0.0895     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.385253    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 518         |\n",
      "|    time_elapsed         | 26274       |\n",
      "|    total_timesteps      | 388500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013777503 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.0157      |\n",
      "|    learning_rate        | 9.03e-05    |\n",
      "|    loss                 | -0.0372     |\n",
      "|    n_updates            | 15510       |\n",
      "|    policy_gradient_loss | -0.0318     |\n",
      "|    value_loss           | 0.103       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4721794   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.48        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 519         |\n",
      "|    time_elapsed         | 26324       |\n",
      "|    total_timesteps      | 389250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009711471 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.0836      |\n",
      "|    learning_rate        | 9.03e-05    |\n",
      "|    loss                 | -0.053      |\n",
      "|    n_updates            | 15540       |\n",
      "|    policy_gradient_loss | -0.0224     |\n",
      "|    value_loss           | 0.0414      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=390000, episode_reward=1.59 +/- 0.03\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.59        |\n",
      "| mean_reward             | 1.3897668   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 390000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014766335 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.083       |\n",
      "|    learning_rate        | 9.03e-05    |\n",
      "|    loss                 | -0.0407     |\n",
      "|    n_updates            | 15570       |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    value_loss           | 0.0583      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15       |\n",
      "|    ep_rew_mean     | 1.39     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 520      |\n",
      "|    time_elapsed    | 26380    |\n",
      "|    total_timesteps | 390000   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2503994   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.27        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 521         |\n",
      "|    time_elapsed         | 26430       |\n",
      "|    total_timesteps      | 390750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010844102 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | -0.0191     |\n",
      "|    learning_rate        | 9.02e-05    |\n",
      "|    loss                 | -0.00565    |\n",
      "|    n_updates            | 15600       |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    value_loss           | 0.149       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2877027   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.26        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 522         |\n",
      "|    time_elapsed         | 26480       |\n",
      "|    total_timesteps      | 391500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011270568 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.0243      |\n",
      "|    learning_rate        | 9.02e-05    |\n",
      "|    loss                 | 0.00434     |\n",
      "|    n_updates            | 15630       |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    value_loss           | 0.175       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.368634   |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15         |\n",
      "|    ep_rew_mean          | 1.36       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 523        |\n",
      "|    time_elapsed         | 26531      |\n",
      "|    total_timesteps      | 392250     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01986531 |\n",
      "|    clip_fraction        | 0.149      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.974     |\n",
      "|    explained_variance   | 0.0389     |\n",
      "|    learning_rate        | 9.02e-05   |\n",
      "|    loss                 | -0.0297    |\n",
      "|    n_updates            | 15660      |\n",
      "|    policy_gradient_loss | -0.0316    |\n",
      "|    value_loss           | 0.113      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4046115   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 524         |\n",
      "|    time_elapsed         | 26581       |\n",
      "|    total_timesteps      | 393000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014554253 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.954      |\n",
      "|    explained_variance   | 0.148       |\n",
      "|    learning_rate        | 9.02e-05    |\n",
      "|    loss                 | -0.0395     |\n",
      "|    n_updates            | 15690       |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    value_loss           | 0.0717      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.461048    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.46        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 525         |\n",
      "|    time_elapsed         | 26632       |\n",
      "|    total_timesteps      | 393750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044442087 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.969      |\n",
      "|    explained_variance   | 0.026       |\n",
      "|    learning_rate        | 9.02e-05    |\n",
      "|    loss                 | -0.037      |\n",
      "|    n_updates            | 15720       |\n",
      "|    policy_gradient_loss | -0.0216     |\n",
      "|    value_loss           | 0.0616      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| mean_reward             | 1.3681931 |\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 15        |\n",
      "|    ep_rew_mean          | 1.41      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 14        |\n",
      "|    iterations           | 526       |\n",
      "|    time_elapsed         | 26686     |\n",
      "|    total_timesteps      | 394500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.015229  |\n",
      "|    clip_fraction        | 0.206     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.991    |\n",
      "|    explained_variance   | 0.00211   |\n",
      "|    learning_rate        | 9.02e-05  |\n",
      "|    loss                 | -0.0543   |\n",
      "|    n_updates            | 15750     |\n",
      "|    policy_gradient_loss | -0.0292   |\n",
      "|    value_loss           | 0.0426    |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=395000, episode_reward=1.39 +/- 0.44\n",
      "Episode length: 15.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 15         |\n",
      "|    mean_reward          | 1.39       |\n",
      "| mean_reward             | 1.4817512  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 395000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01632958 |\n",
      "|    clip_fraction        | 0.151      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.979     |\n",
      "|    explained_variance   | 0.0209     |\n",
      "|    learning_rate        | 9.01e-05   |\n",
      "|    loss                 | -0.018     |\n",
      "|    n_updates            | 15780      |\n",
      "|    policy_gradient_loss | -0.0317    |\n",
      "|    value_loss           | 0.13       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.43     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 527      |\n",
      "|    time_elapsed    | 26748    |\n",
      "|    total_timesteps | 395250   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.494248    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.47        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 528         |\n",
      "|    time_elapsed         | 26802       |\n",
      "|    total_timesteps      | 396000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014352885 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.972      |\n",
      "|    explained_variance   | 0.00955     |\n",
      "|    learning_rate        | 9.01e-05    |\n",
      "|    loss                 | -0.0571     |\n",
      "|    n_updates            | 15810       |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    value_loss           | 0.0323      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3823353   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 529         |\n",
      "|    time_elapsed         | 26855       |\n",
      "|    total_timesteps      | 396750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012240135 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.979      |\n",
      "|    explained_variance   | 0.0529      |\n",
      "|    learning_rate        | 9.01e-05    |\n",
      "|    loss                 | -0.0502     |\n",
      "|    n_updates            | 15840       |\n",
      "|    policy_gradient_loss | -0.0315     |\n",
      "|    value_loss           | 0.0621      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4122975   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 530         |\n",
      "|    time_elapsed         | 26907       |\n",
      "|    total_timesteps      | 397500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017326733 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.995      |\n",
      "|    explained_variance   | 0.095       |\n",
      "|    learning_rate        | 9.01e-05    |\n",
      "|    loss                 | -0.0482     |\n",
      "|    n_updates            | 15870       |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    value_loss           | 0.0519      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.388639    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 531         |\n",
      "|    time_elapsed         | 26960       |\n",
      "|    total_timesteps      | 398250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012580991 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | -0.0328     |\n",
      "|    learning_rate        | 9.01e-05    |\n",
      "|    loss                 | -0.0307     |\n",
      "|    n_updates            | 15900       |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    value_loss           | 0.0924      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.5652531   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.48        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 532         |\n",
      "|    time_elapsed         | 27011       |\n",
      "|    total_timesteps      | 399000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012187578 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.973      |\n",
      "|    explained_variance   | 0.13        |\n",
      "|    learning_rate        | 9e-05       |\n",
      "|    loss                 | -0.0486     |\n",
      "|    n_updates            | 15930       |\n",
      "|    policy_gradient_loss | -0.0244     |\n",
      "|    value_loss           | 0.0451      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3222331   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 533         |\n",
      "|    time_elapsed         | 27062       |\n",
      "|    total_timesteps      | 399750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019611357 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.984      |\n",
      "|    explained_variance   | 0.0748      |\n",
      "|    learning_rate        | 9e-05       |\n",
      "|    loss                 | -0.045      |\n",
      "|    n_updates            | 15960       |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    value_loss           | 0.0478      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=400000, episode_reward=1.57 +/- 0.03\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.57        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 400000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012430288 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.97       |\n",
      "|    explained_variance   | 0.043       |\n",
      "|    learning_rate        | 9e-05       |\n",
      "|    loss                 | -0.0161     |\n",
      "|    n_updates            | 15990       |\n",
      "|    policy_gradient_loss | -0.0246     |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| mean_reward        | 1.3690066 |\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 15.1      |\n",
      "|    ep_rew_mean     | 1.34      |\n",
      "| time/              |           |\n",
      "|    fps             | 14        |\n",
      "|    iterations      | 534       |\n",
      "|    time_elapsed    | 27119     |\n",
      "|    total_timesteps | 400500    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4268718   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.38        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 535         |\n",
      "|    time_elapsed         | 27171       |\n",
      "|    total_timesteps      | 401250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012419078 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.956      |\n",
      "|    explained_variance   | -0.0163     |\n",
      "|    learning_rate        | 9e-05       |\n",
      "|    loss                 | -0.0164     |\n",
      "|    n_updates            | 16020       |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    value_loss           | 0.119       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3235571   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.36        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 536         |\n",
      "|    time_elapsed         | 27223       |\n",
      "|    total_timesteps      | 402000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013696076 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.974      |\n",
      "|    explained_variance   | -0.014      |\n",
      "|    learning_rate        | 9e-05       |\n",
      "|    loss                 | -0.0366     |\n",
      "|    n_updates            | 16050       |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    value_loss           | 0.0856      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.3764272  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15         |\n",
      "|    ep_rew_mean          | 1.37       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 537        |\n",
      "|    time_elapsed         | 27275      |\n",
      "|    total_timesteps      | 402750     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01315331 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.984     |\n",
      "|    explained_variance   | 0.0752     |\n",
      "|    learning_rate        | 8.99e-05   |\n",
      "|    loss                 | -0.0279    |\n",
      "|    n_updates            | 16080      |\n",
      "|    policy_gradient_loss | -0.0261    |\n",
      "|    value_loss           | 0.0999     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4292206   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 538         |\n",
      "|    time_elapsed         | 27326       |\n",
      "|    total_timesteps      | 403500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011994062 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.971      |\n",
      "|    explained_variance   | -0.0697     |\n",
      "|    learning_rate        | 8.99e-05    |\n",
      "|    loss                 | -0.0518     |\n",
      "|    n_updates            | 16110       |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    value_loss           | 0.0588      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2724422   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 539         |\n",
      "|    time_elapsed         | 27377       |\n",
      "|    total_timesteps      | 404250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012518887 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.977      |\n",
      "|    explained_variance   | 0.0109      |\n",
      "|    learning_rate        | 8.99e-05    |\n",
      "|    loss                 | -0.0424     |\n",
      "|    n_updates            | 16140       |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    value_loss           | 0.0468      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=405000, episode_reward=1.58 +/- 0.03\n",
      "Episode length: 15.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 15         |\n",
      "|    mean_reward          | 1.58       |\n",
      "| mean_reward             | 1.3839029  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 405000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01215406 |\n",
      "|    clip_fraction        | 0.0959     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.989     |\n",
      "|    explained_variance   | -0.0113    |\n",
      "|    learning_rate        | 8.99e-05   |\n",
      "|    loss                 | 0.0144     |\n",
      "|    n_updates            | 16170      |\n",
      "|    policy_gradient_loss | -0.0248    |\n",
      "|    value_loss           | 0.177      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15       |\n",
      "|    ep_rew_mean     | 1.31     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 540      |\n",
      "|    time_elapsed    | 27433    |\n",
      "|    total_timesteps | 405000   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3015759   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.31        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 541         |\n",
      "|    time_elapsed         | 27486       |\n",
      "|    total_timesteps      | 405750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010306792 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.0401      |\n",
      "|    learning_rate        | 8.99e-05    |\n",
      "|    loss                 | -0.0297     |\n",
      "|    n_updates            | 16200       |\n",
      "|    policy_gradient_loss | -0.0252     |\n",
      "|    value_loss           | 0.0913      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.2373481   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1.27        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 542         |\n",
      "|    time_elapsed         | 27542       |\n",
      "|    total_timesteps      | 406500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015241679 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.0624      |\n",
      "|    learning_rate        | 8.99e-05    |\n",
      "|    loss                 | -0.00882    |\n",
      "|    n_updates            | 16230       |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    value_loss           | 0.159       |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| mean_reward             | 1.3778753 |\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 15        |\n",
      "|    ep_rew_mean          | 1.36      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 14        |\n",
      "|    iterations           | 543       |\n",
      "|    time_elapsed         | 27595     |\n",
      "|    total_timesteps      | 407250    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0153116 |\n",
      "|    clip_fraction        | 0.134     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.995    |\n",
      "|    explained_variance   | 0.0204    |\n",
      "|    learning_rate        | 8.98e-05  |\n",
      "|    loss                 | -0.031    |\n",
      "|    n_updates            | 16260     |\n",
      "|    policy_gradient_loss | -0.0309   |\n",
      "|    value_loss           | 0.119     |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4170674   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 544         |\n",
      "|    time_elapsed         | 27647       |\n",
      "|    total_timesteps      | 408000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014474397 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.981      |\n",
      "|    explained_variance   | -0.0368     |\n",
      "|    learning_rate        | 8.98e-05    |\n",
      "|    loss                 | -0.0417     |\n",
      "|    n_updates            | 16290       |\n",
      "|    policy_gradient_loss | -0.0251     |\n",
      "|    value_loss           | 0.0678      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.3928773  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15         |\n",
      "|    ep_rew_mean          | 1.42       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 545        |\n",
      "|    time_elapsed         | 27700      |\n",
      "|    total_timesteps      | 408750     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00978589 |\n",
      "|    clip_fraction        | 0.122      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.974     |\n",
      "|    explained_variance   | -0.0609    |\n",
      "|    learning_rate        | 8.98e-05   |\n",
      "|    loss                 | -0.0218    |\n",
      "|    n_updates            | 16320      |\n",
      "|    policy_gradient_loss | -0.0279    |\n",
      "|    value_loss           | 0.117      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.5022681   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 546         |\n",
      "|    time_elapsed         | 27754       |\n",
      "|    total_timesteps      | 409500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010869878 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.979      |\n",
      "|    explained_variance   | 0.0521      |\n",
      "|    learning_rate        | 8.98e-05    |\n",
      "|    loss                 | -0.057      |\n",
      "|    n_updates            | 16350       |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    value_loss           | 0.034       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=410000, episode_reward=1.58 +/- 0.03\n",
      "Episode length: 15.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 15         |\n",
      "|    mean_reward          | 1.58       |\n",
      "| mean_reward             | 1.408915   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 410000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01507441 |\n",
      "|    clip_fraction        | 0.106      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.983     |\n",
      "|    explained_variance   | 0.0136     |\n",
      "|    learning_rate        | 8.98e-05   |\n",
      "|    loss                 | -0.0199    |\n",
      "|    n_updates            | 16380      |\n",
      "|    policy_gradient_loss | -0.0249    |\n",
      "|    value_loss           | 0.11       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | 1.44     |\n",
      "| time/              |          |\n",
      "|    fps             | 14       |\n",
      "|    iterations      | 547      |\n",
      "|    time_elapsed    | 27813    |\n",
      "|    total_timesteps | 410250   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| mean_reward             | 1.4748863  |\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15.1       |\n",
      "|    ep_rew_mean          | 1.45       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 548        |\n",
      "|    time_elapsed         | 27866      |\n",
      "|    total_timesteps      | 411000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01622619 |\n",
      "|    clip_fraction        | 0.176      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.03      |\n",
      "|    explained_variance   | 0.173      |\n",
      "|    learning_rate        | 8.97e-05   |\n",
      "|    loss                 | -0.0638    |\n",
      "|    n_updates            | 16410      |\n",
      "|    policy_gradient_loss | -0.0256    |\n",
      "|    value_loss           | 0.0201     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4146007   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 549         |\n",
      "|    time_elapsed         | 27917       |\n",
      "|    total_timesteps      | 411750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011653291 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.0363      |\n",
      "|    learning_rate        | 8.97e-05    |\n",
      "|    loss                 | -0.044      |\n",
      "|    n_updates            | 16440       |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    value_loss           | 0.0564      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4363723   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 550         |\n",
      "|    time_elapsed         | 27968       |\n",
      "|    total_timesteps      | 412500      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012671279 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | -0.00266    |\n",
      "|    learning_rate        | 8.97e-05    |\n",
      "|    loss                 | -0.0503     |\n",
      "|    n_updates            | 16470       |\n",
      "|    policy_gradient_loss | -0.024      |\n",
      "|    value_loss           | 0.0404      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4870644   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 551         |\n",
      "|    time_elapsed         | 28019       |\n",
      "|    total_timesteps      | 413250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011087881 |\n",
      "|    clip_fraction        | 0.0988      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.0514      |\n",
      "|    learning_rate        | 8.97e-05    |\n",
      "|    loss                 | -0.0339     |\n",
      "|    n_updates            | 16500       |\n",
      "|    policy_gradient_loss | -0.0244     |\n",
      "|    value_loss           | 0.0781      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4538149   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 552         |\n",
      "|    time_elapsed         | 28070       |\n",
      "|    total_timesteps      | 414000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011957881 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.0742      |\n",
      "|    learning_rate        | 8.97e-05    |\n",
      "|    loss                 | -0.0591     |\n",
      "|    n_updates            | 16530       |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    value_loss           | 0.0307      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.3756331   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 553         |\n",
      "|    time_elapsed         | 28121       |\n",
      "|    total_timesteps      | 414750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011332313 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.0115      |\n",
      "|    learning_rate        | 8.97e-05    |\n",
      "|    loss                 | -0.0396     |\n",
      "|    n_updates            | 16560       |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    value_loss           | 0.0623      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=415000, episode_reward=1.60 +/- 0.03\n",
      "Episode length: 15.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 1.6         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 415000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012866376 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.0747      |\n",
      "|    learning_rate        | 8.96e-05    |\n",
      "|    loss                 | -0.0499     |\n",
      "|    n_updates            | 16590       |\n",
      "|    policy_gradient_loss | -0.0257     |\n",
      "|    value_loss           | 0.0477      |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| mean_reward        | 1.4070166 |\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 15.1      |\n",
      "|    ep_rew_mean     | 1.42      |\n",
      "| time/              |           |\n",
      "|    fps             | 14        |\n",
      "|    iterations      | 554       |\n",
      "|    time_elapsed    | 28178     |\n",
      "|    total_timesteps | 415500    |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| mean_reward             | 1.4619429    |\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15.1         |\n",
      "|    ep_rew_mean          | 1.42         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 555          |\n",
      "|    time_elapsed         | 28229        |\n",
      "|    total_timesteps      | 416250       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0101788975 |\n",
      "|    clip_fraction        | 0.121        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.01        |\n",
      "|    explained_variance   | 0.0146       |\n",
      "|    learning_rate        | 8.96e-05     |\n",
      "|    loss                 | -0.0299      |\n",
      "|    n_updates            | 16620        |\n",
      "|    policy_gradient_loss | -0.0237      |\n",
      "|    value_loss           | 0.0843       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.4152901   |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 556         |\n",
      "|    time_elapsed         | 28283       |\n",
      "|    total_timesteps      | 417000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011709764 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.0892      |\n",
      "|    learning_rate        | 8.96e-05    |\n",
      "|    loss                 | -0.0449     |\n",
      "|    n_updates            | 16650       |\n",
      "|    policy_gradient_loss | -0.0233     |\n",
      "|    value_loss           | 0.0503      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| mean_reward             | 1.418432    |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15          |\n",
      "|    ep_rew_mean          | 1.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 557         |\n",
      "|    time_elapsed         | 28336       |\n",
      "|    total_timesteps      | 417750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012367277 |\n",
      "|    clip_fraction        | 0.091       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | -0.00327    |\n",
      "|    learning_rate        | 8.96e-05    |\n",
      "|    loss                 | -0.0495     |\n",
      "|    n_updates            | 16680       |\n",
      "|    policy_gradient_loss | -0.0248     |\n",
      "|    value_loss           | 0.0486      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/st/_8t9rcwd49jf2y161kkqlch40000gn/T/ipykernel_5618/292290626.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseCallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCheckpointCallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEvalCallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mtb_log_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"PPO\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     ) -> SelfPPO:\n\u001b[0;32m--> 315\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_interval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    191\u001b[0m                     \u001b[0;31m# Otherwise, clip the actions to avoid out of bound error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                     \u001b[0;31m# as we are sampling from an unbounded Gaussian distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                     \u001b[0mclipped_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0mnew_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minformation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \"\"\"\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mVecEnvStepReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Avoid circular imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             obs, self.buf_rews[env_idx], terminated, truncated, self.buf_infos[env_idx] = self.envs[env_idx].step(\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             )\n\u001b[1;32m     61\u001b[0m             \u001b[0;31m# convert to SB3 VecEnv api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/stable_baselines3/common/monitor.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minformation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \"\"\"\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tried to step environment that needs reset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mterminated\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_reset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/files/fun_projects/fantasy_football_2024/rl_env.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincrement_turn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m         \u001b[0;31m# -- run other managers -- #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbetween_turns_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;31m# -- calculate reward -- #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/files/fun_projects/fantasy_football_2024/rl_env_sarl.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbetween_turns_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_other_mgrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/files/fun_projects/fantasy_football_2024/rl_env_sarl.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_other_mgrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;34m'''runs the draft for all managers except the RL agent'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_turn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraft\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cur_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrl_mgr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mchoice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstochastic_choice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstochastic_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_player\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cur_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchoice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sleeper_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincrement_turn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/files/fun_projects/fantasy_football_2024/rl_env.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, mgr_num, temperature)\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0mneeded_positions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"RB\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"WR\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TE\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mexclude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"K\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DEF\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mteam_comp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mSTARTER_COMPOSITION\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"K\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"DEF\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneeded_positions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                 \u001b[0;31m# if anything is needed besides K and DEF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m                 \u001b[0mplayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample_player\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneeded_positions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneeded_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0mplayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample_player\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/files/fun_projects/fantasy_football_2024/rl_env.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, temperature, needed_positions, exclude_pos, samp_size)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mneeded_positions\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mplayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mplayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_players\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexclude_pos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0mplayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mplayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"position\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexclude_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mplayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msamp_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"position\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"DEF\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m25\u001b[0m  \u001b[0;31m# Downweighting defense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_deprecated_callable_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaybe_callable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1413\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getbool_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1414\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m             \u001b[0;31m# an iterable multi-selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1207\u001b[0m         \u001b[0;31m# caller is responsible for ensuring non-None axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m         \u001b[0minds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   4149\u001b[0m         \u001b[0mFor\u001b[0m \u001b[0mSeries\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mdoes\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpublic\u001b[0m \u001b[0mtake\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m \u001b[0mnever\u001b[0m \u001b[0msets\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0m_is_copy\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4151\u001b[0m         \u001b[0mSee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocstring\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mexplanation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4152\u001b[0m         \"\"\"\n\u001b[0;32m-> 4153\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4154\u001b[0m         \u001b[0;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4156\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[1;32m   4134\u001b[0m             \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4135\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4136\u001b[0m             \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4137\u001b[0m         )\n\u001b[0;32m-> 4138\u001b[0;31m         return self._constructor_from_mgr(new_data, axes=new_data.axes).__finalize__(\n\u001b[0m\u001b[1;32m   4139\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"take\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4140\u001b[0m         )\n",
      "\u001b[0;32m~/files/fun_projects/fantasy_football_2024/venv/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, other, method, **kwargs)\u001b[0m\n\u001b[1;32m   6251\u001b[0m                \u001b[0mThe\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcurrently\u001b[0m \u001b[0mconsidered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6252\u001b[0m                \u001b[0mstable\u001b[0m \u001b[0macross\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0mreleases\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6253\u001b[0m         \"\"\"\n\u001b[1;32m   6254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNDFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6255\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6256\u001b[0m                 \u001b[0;31m# We want attrs propagation to have minimal performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6257\u001b[0m                 \u001b[0;31m# impact if attrs are not used; i.e. attrs is an empty dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6258\u001b[0m                 \u001b[0;31m# One could make the deepcopy unconditionally, but a deepcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback, EvalCallback\n",
    "from stable_baselines3.common.logger import configure\n",
    "from datetime import datetime\n",
    "\n",
    "notes = \"\"\"\n",
    "- starting from model logs/PPO_20240828-000621/best_model.zip\n",
    "- penalizing >1 DEF and >1 K\n",
    "- half rewards for RB and WR past starters\n",
    "- half rewards for first QB past starter\n",
    "- quarter rewards for TE past starters\n",
    "^^^^^ everything above this is when things got worse according to simulations\n",
    "- added rules to stochastic choice for opponents so that they'll pick def and k later\n",
    "- added penalty for picking K before round 12\n",
    "- reduced bye week penalty to 1/(17*2) for each non completed week\n",
    "\"\"\"\n",
    "\n",
    "# Define your environment\n",
    "env = SARLDraftEnv(stochastic_temp=.5)\n",
    "\n",
    "# Generate a unique run ID\n",
    "run_id = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Choose significant parameters to include in the log directory name\n",
    "lr_str = ppo_params['learning_rate'].__name__ if callable(ppo_params['learning_rate']) else ppo_params['learning_rate']\n",
    "gamma_str = ppo_params['gamma'].__name__ if callable(ppo_params['gamma']) else ppo_params['gamma']\n",
    "param_str = f\"PPO\"\n",
    "# param_str = f\"PPO_lr_{lr_str}_gamma_{gamma_str}\"\n",
    "log_dir = f\"./logs/{param_str}_{run_id}/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Save ppo_params to a JSON file\n",
    "params_save_path = os.path.join(log_dir, f'ppo_params_{run_id}.json')\n",
    "with open(params_save_path, 'w') as f:\n",
    "    # if the param is not json serializable (e.g. function), then write callable name\n",
    "    json_params = {key: value if not callable(value) else value.__name__ for key, value in ppo_params.items()}\n",
    "    json_params['stochastic_temp'] = env.stochastic_temp\n",
    "    json_params['notes'] = notes\n",
    "    json.dump(json_params, f, indent=2)\n",
    "\n",
    "# Configure the logger\n",
    "new_logger = configure(log_dir, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "\n",
    "# Define the PPO model and attach the logger\n",
    "# model = PPO(**ppo_params, env=env, tensorboard_log=log_dir)\n",
    "# model = PPO.load(\"logs/PPO_20240827-143658/best_model.zip\", env=env) # load model from previous run\n",
    "# model = PPO.load(\"logs/PPO_20240829-001139/best_model.zip\", env=env) # load model from previous run\n",
    "# model = PPO.load(\"logs/PPO_20240829-022850/best_model.zip\", env=env) # load model from previous run\n",
    "model = PPO.load(\"logs/PPO_20240828-000621/best_model.zip\", env=env) # load model from previous run\n",
    "model.set_logger(new_logger)\n",
    "\n",
    "\n",
    "# Log ppo_params to TensorBoard/CSV\n",
    "for key, value in ppo_params.items():\n",
    "    model.logger.record(f'params/{key}', value)\n",
    "\n",
    "\n",
    "# Custom callback for monitoring and saving during training\n",
    "class TrainingMonitorCallback(BaseCallback):\n",
    "    def __init__(self, log_interval: int, draft_save_interval: int, log_dir: str, run_id: str, verbose=1):\n",
    "        super(TrainingMonitorCallback, self).__init__(verbose)\n",
    "        self.log_interval = log_interval\n",
    "        self.draft_save_interval = draft_save_interval\n",
    "        self.log_dir = log_dir\n",
    "        self.run_id = run_id\n",
    "        self.best_mean_reward = -float(\"inf\")\n",
    "        self.rewards_history = []\n",
    "        self.losses_history = []\n",
    "        self.episode_counter = 0\n",
    "        self.draft_history = []  # List to store draft dicts\n",
    "\n",
    "        self.model_save_path = os.path.join(log_dir, f'best_model_{run_id}')\n",
    "        self.rewards_losses_path = os.path.join(log_dir, f'rewards_losses_{run_id}.json')\n",
    "        self.drafts_save_path = os.path.join(log_dir, f'draft_history_{run_id}.json')\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "\n",
    "        # Check if the episode has terminated\n",
    "        if self.locals['dones'][0]:\n",
    "            \n",
    "            # Log losses if they are available (PPO does not use TD errors)\n",
    "            if 'loss' in self.locals:\n",
    "                current_loss = self.locals['loss']\n",
    "                self.losses_history.append(current_loss)\n",
    "                \n",
    "            # Log rewards\n",
    "            current_reward = self.locals['rewards']\n",
    "            self.rewards_history.append(current_reward)\n",
    "            \n",
    "            # Periodic logging and saving\n",
    "            if self.episode_counter % self.log_interval == 0:\n",
    "                mean_reward = np.mean(self.rewards_history[-self.log_interval:])\n",
    "                self.logger.record('mean_reward', mean_reward)\n",
    "\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    self.model.save(self.model_save_path)\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"New best mean reward: {self.best_mean_reward}. Model saved to {self.model_save_path}\")\n",
    "                \n",
    "                # Convert rewards and losses to lists before saving\n",
    "                rewards_list = [float(reward.item() if isinstance(reward, np.ndarray) else reward) for reward in self.rewards_history]\n",
    "                losses_list = [float(loss.item() if isinstance(loss, np.ndarray) else loss) for loss in self.losses_history]\n",
    "\n",
    "                # Save rewards and losses to a file\n",
    "                with open(self.rewards_losses_path, 'w') as f:\n",
    "                    json.dump({'rewards': rewards_list, 'losses': losses_list}, f)\n",
    "            \n",
    "            if self.episode_counter % self.draft_save_interval == 0:\n",
    "                draft = self.locals['infos'][0]['draft']\n",
    "                self.draft_history.append(draft)\n",
    "                with open(self.drafts_save_path, 'w') as f:\n",
    "                    json.dump(self.draft_history, f, indent=4)\n",
    "                \n",
    "            self.episode_counter += 1\n",
    "\n",
    "        return True\n",
    "\n",
    "# Initialize and use the custom callback\n",
    "monitor_callback = TrainingMonitorCallback(log_interval=50, # each 50 episodes, save rewards, losses, best model\n",
    "                                           draft_save_interval=50,  # Every 250 episodes save draft\n",
    "                                           log_dir=log_dir, \n",
    "                                           run_id=run_id)\n",
    "\n",
    "# Create a checkpoint callback to save the model periodically\n",
    "checkpoint_callback = CheckpointCallback(save_freq=5000, save_path=log_dir, name_prefix='ppo_model')\n",
    "\n",
    "# Gets called when there is new best model\n",
    "eval_callback = EvalCallback(env, best_model_save_path=log_dir, log_path=log_dir, eval_freq=5000, deterministic=True, render=False)\n",
    "\n",
    "# Combine all callbacks\n",
    "callback = [checkpoint_callback, eval_callback, monitor_callback]\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=2e6, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
